{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#IMDB-Movie-Review-Sentiment-Classification\" data-toc-modified-id=\"IMDB-Movie-Review-Sentiment-Classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>IMDB Movie Review Sentiment Classification</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Process\" data-toc-modified-id=\"Process-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Process</a></span></li><li><span><a href=\"#Configure-notebook,-import-libraries,-and-import-dataset\" data-toc-modified-id=\"Configure-notebook,-import-libraries,-and-import-dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Configure notebook, import libraries, and import dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries\" data-toc-modified-id=\"Import-libraries-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Import libraries</a></span></li><li><span><a href=\"#Define-global-variables\" data-toc-modified-id=\"Define-global-variables-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Define global variables</a></span></li><li><span><a href=\"#Import-labeled-data\" data-toc-modified-id=\"Import-labeled-data-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Import labeled data</a></span></li></ul></li><li><span><a href=\"#Examine-the-data\" data-toc-modified-id=\"Examine-the-data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Examine the data</a></span></li><li><span><a href=\"#Cleaning-and-preprocessing\" data-toc-modified-id=\"Cleaning-and-preprocessing-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Cleaning and preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-training-data\" data-toc-modified-id=\"Load-training-data-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Load training data</a></span></li><li><span><a href=\"#Write-helper-functions\" data-toc-modified-id=\"Write-helper-functions-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Write helper functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentence-cleaner\" data-toc-modified-id=\"Sentence-cleaner-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Sentence cleaner</a></span></li></ul></li><li><span><a href=\"#Create-list-of-Doc2Vec-TaggedDocument-objects\" data-toc-modified-id=\"Create-list-of-Doc2Vec-TaggedDocument-objects-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Create list of Doc2Vec TaggedDocument objects</a></span></li></ul></li><li><span><a href=\"#Train-Doc2Vec-model\" data-toc-modified-id=\"Train-Doc2Vec-model-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Train Doc2Vec model</a></span></li><li><span><a href=\"#Classification-model-training-and-evaluation\" data-toc-modified-id=\"Classification-model-training-and-evaluation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Classification model training and evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Kaggle-model\" data-toc-modified-id=\"Kaggle-model-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Kaggle model</a></span></li><li><span><a href=\"#Standard-write-up-models\" data-toc-modified-id=\"Standard-write-up-models-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Standard write-up models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-model-comments\" data-toc-modified-id=\"Standard-model-comments-8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;</span>Standard model comments</a></span></li></ul></li></ul></li><li><span><a href=\"#Tuning-Doc3Vec\" data-toc-modified-id=\"Tuning-Doc3Vec-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Tuning Doc3Vec</a></span></li><li><span><a href=\"#Combing-models\" data-toc-modified-id=\"Combing-models-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Combing models</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>IMDB Movie Review Sentiment Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px; width: 30%; height: 30%;\" src=\"images/imdb.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The overall goal of this set of write-ups is to explore a number of machine learning algorithms utilizing natural language processing (NLP) to classify sentiment IMDB movie reviews.\n",
    "\n",
    "The specific goals of this write-up include:\n",
    "1. Create a set of word embeddings from the IMDb movie review text utilizing [Word2vec](https://en.wikipedia.org/wiki/Word2vec)\n",
    "2. Cluster the embeddings utilizing a K-nearest neighbors algorithm into a set of centroids\n",
    "2. Run the models from the [last write-up](./Model-06.ipynb) against the centroid feature set\n",
    "3. Determine if the centroid feature set improves our ability to correctly classify movie review sentiment\n",
    "\n",
    "References:\n",
    "* This series of write-ups is inspired by the Kaggle [\n",
    "Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial) competition.\n",
    "* [Gensim Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "* [smart_open](https://pypi.org/project/smart_open/)\n",
    "\n",
    "Dataset source:  [IMDB Movie Reviews](https://www.kaggle.com/c/word2vec-nlp-tutorial/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "\n",
    "Previously covered [here](./Model-06.ipynb#Process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure notebook, import libraries, and import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T15:16:22.416553Z",
     "start_time": "2018-10-16T15:16:22.413553Z"
    }
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:39:21.837900Z",
     "start_time": "2018-11-05T21:39:21.398908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import smart_open\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import set_option\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#from sklearn.metrics import classification_report\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# http://www.nltk.org/index.html\n",
    "# pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Creating function implementing punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "\n",
    "# Only need this the first time...\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "# pip install BeautifulSoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# https://pypi.org/project/gensim/\n",
    "# pip install gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:19:34.359393Z",
     "start_time": "2018-11-05T21:19:33.909402Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Opens a GUI that allows us to download the NLTK data\n",
    "# nltk.download()\n",
    "\n",
    "dataPath = os.path.join('.', 'datasets', 'imdb_movie_reviews')\n",
    "labeledTrainData = os.path.join(dataPath, 'labeledTrainData.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:19:35.260375Z",
     "start_time": "2018-11-05T21:19:34.419391Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(labeledTrainData, sep = '\\t', header = 0, quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the data\n",
    "\n",
    "Previously covered [here](./Model-06.ipynb#Examine-the-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Previous process justification and methodology also previously covered [here](./Model-06.ipynb#Cleaning-and-preprocessing).)\n",
    "\n",
    "First, read in the labeled training data (which we've done before) as well as the unlabeled training data (which is new to this write-up).  The more data we can feed to Word2Vec the better, and this will help the algorithm associate related words more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:19:38.652310Z",
     "start_time": "2018-11-05T21:19:37.869325Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pull in the labeled data\n",
    "labeledTrainData = os.path.join(dataPath, 'labeledTrainData.tsv')\n",
    "df = pd.read_csv(labeledTrainData, sep = '\\t', header = 0, quoting = 3)\n",
    "\n",
    "# Pull in the unlabeled data since it can also be utilized by Word2Vec\n",
    "# unlabeledTrainData = os.path.join(dataPath, 'unlabeledTrainData.tsv')\n",
    "# dfUn = pd.read_csv(unlabeledTrainData, sep = '\\t', header = 0, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:19:48.467120Z",
     "start_time": "2018-11-05T21:19:47.989129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape : (25000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "print('df.shape :', df.shape)\n",
    "#print('dfUn.shape :', dfUn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write helper functions\n",
    "\n",
    "Word2Vec expects single sentences as inputs, and each sentence formated as a list of words (i.e. a list of lists).  Let's write two functions to achieve this next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence cleaner\n",
    "\n",
    "Take a given sentence and process/clean it (i.e. remove HTML and other cruft, lower case the text, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:19:54.058011Z",
     "start_time": "2018-11-05T21:19:53.588020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Update stop word helper function to output a list of words\n",
    "\n",
    "# Clean IMDB review text\n",
    "def cleanReview(review, removeStopWords = False):\n",
    "    # Convert the stop words to a set\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Remove HTML\n",
    "    clean = BeautifulSoup(review)\n",
    "    \n",
    "    # Remove non-alpha chars\n",
    "    clean = re.sub(\"[^a-zA-Z]\", ' ', clean.get_text())\n",
    "    \n",
    "    # Convert to lower case and \"tokenize\"\n",
    "    clean = clean.lower().split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    if removeStopWords:\n",
    "        clean = [x for x in clean if not x in stopWords]\n",
    "    \n",
    "    # Return results\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick examination of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:19:58.248929Z",
     "start_time": "2018-11-05T21:19:57.758939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looking',\n",
       " 'for',\n",
       " 'quo',\n",
       " 'vadis',\n",
       " 'at',\n",
       " 'my',\n",
       " 'local',\n",
       " 'video',\n",
       " 'store',\n",
       " 'i',\n",
       " 'found',\n",
       " 'this']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine\n",
    "cleanReview(df.iloc[25,2])[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:31:12.153854Z",
     "start_time": "2018-11-05T21:31:11.722863Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examine\n",
    "#cleanReview(dfUn.iloc[0,1])[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create list of Doc2Vec TaggedDocument objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:48:45.068401Z",
     "start_time": "2018-11-05T21:48:07.536072Z"
    }
   },
   "outputs": [],
   "source": [
    "taggedDocs = []\n",
    "\n",
    "for i, s in enumerate(df.iloc[:,2]):\n",
    "    clean = cleanReview(s)\n",
    "    taggedDocs.append(TaggedDocument(clean, [i]))\n",
    "    #if (i > 20):\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:48:45.575391Z",
     "start_time": "2018-11-05T21:48:45.070401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(taggedDocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:48:55.131221Z",
     "start_time": "2018-11-05T21:48:54.710228Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-05 14:48:55,121 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "doc2vecModel = Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:48:57.760174Z",
     "start_time": "2018-11-05T21:48:55.223219Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-05 14:48:55,646 : INFO : collecting all words and their counts\n",
      "2018-11-05 14:48:55,646 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-11-05 14:48:56,076 : INFO : PROGRESS: at example #10000, processed 2385574 words (5568249/s), 51527 word types, 10000 tags\n",
      "2018-11-05 14:48:56,464 : INFO : PROGRESS: at example #20000, processed 4747503 words (6060061/s), 67813 word types, 20000 tags\n",
      "2018-11-05 14:48:56,657 : INFO : collected 74218 word types and 25000 unique tags from a corpus of 25000 examples and 5920713 words\n",
      "2018-11-05 14:48:56,657 : INFO : Loading a fresh vocabulary\n",
      "2018-11-05 14:48:56,737 : INFO : effective_min_count=2 retains 46350 unique words (62% of original 74218, drops 27868)\n",
      "2018-11-05 14:48:56,737 : INFO : effective_min_count=2 leaves 5892845 word corpus (99% of original 5920713, drops 27868)\n",
      "2018-11-05 14:48:56,862 : INFO : deleting the raw counts dictionary of 74218 items\n",
      "2018-11-05 14:48:56,862 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-11-05 14:48:56,862 : INFO : downsampling leaves estimated 4416048 word corpus (74.9% of prior 5892845)\n",
      "2018-11-05 14:48:56,995 : INFO : estimated required memory for 46350 words and 50 dimensions: 46715000 bytes\n",
      "2018-11-05 14:48:56,995 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "doc2vecModel.build_vocab(taggedDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:51:49.396158Z",
     "start_time": "2018-11-05T21:48:57.760174Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-05 14:48:58,183 : INFO : training model with 3 workers on 46350 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-05 14:48:59,189 : INFO : EPOCH 1 - PROGRESS: at 22.71% examples, 1022680 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:00,202 : INFO : EPOCH 1 - PROGRESS: at 46.82% examples, 1041462 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:01,208 : INFO : EPOCH 1 - PROGRESS: at 69.91% examples, 1034194 words/s, in_qsize 4, out_qsize 1\n",
      "2018-11-05 14:49:02,231 : INFO : EPOCH 1 - PROGRESS: at 93.68% examples, 1031504 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:02,495 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:02,495 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:02,505 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:02,505 : INFO : EPOCH - 1 : training on 5920713 raw words (4441232 effective words) took 4.3s, 1030368 effective words/s\n",
      "2018-11-05 14:49:03,515 : INFO : EPOCH 2 - PROGRESS: at 22.90% examples, 1019008 words/s, in_qsize 4, out_qsize 1\n",
      "2018-11-05 14:49:04,525 : INFO : EPOCH 2 - PROGRESS: at 45.97% examples, 1019033 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:05,526 : INFO : EPOCH 2 - PROGRESS: at 69.14% examples, 1022826 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:06,521 : INFO : EPOCH 2 - PROGRESS: at 93.31% examples, 1032213 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:06,814 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:06,814 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:06,814 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:06,814 : INFO : EPOCH - 2 : training on 5920713 raw words (4440008 effective words) took 4.3s, 1030077 effective words/s\n",
      "2018-11-05 14:49:07,820 : INFO : EPOCH 3 - PROGRESS: at 23.40% examples, 1051935 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:08,833 : INFO : EPOCH 3 - PROGRESS: at 46.16% examples, 1028249 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:09,826 : INFO : EPOCH 3 - PROGRESS: at 69.73% examples, 1036161 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:10,829 : INFO : EPOCH 3 - PROGRESS: at 93.31% examples, 1034472 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:11,097 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:11,107 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:11,107 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:11,107 : INFO : EPOCH - 3 : training on 5920713 raw words (4440741 effective words) took 4.3s, 1036170 effective words/s\n",
      "2018-11-05 14:49:12,122 : INFO : EPOCH 4 - PROGRESS: at 23.02% examples, 1031235 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:13,119 : INFO : EPOCH 4 - PROGRESS: at 45.80% examples, 1020820 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:14,129 : INFO : EPOCH 4 - PROGRESS: at 69.90% examples, 1035624 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:49:15,145 : INFO : EPOCH 4 - PROGRESS: at 93.67% examples, 1032790 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:15,395 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:15,395 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:15,395 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:15,395 : INFO : EPOCH - 4 : training on 5920713 raw words (4440971 effective words) took 4.3s, 1036178 effective words/s\n",
      "2018-11-05 14:49:16,405 : INFO : EPOCH 5 - PROGRESS: at 23.21% examples, 1045575 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:17,402 : INFO : EPOCH 5 - PROGRESS: at 46.82% examples, 1045854 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:49:18,408 : INFO : EPOCH 5 - PROGRESS: at 70.40% examples, 1045179 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:49:19,415 : INFO : EPOCH 5 - PROGRESS: at 94.01% examples, 1041815 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:19,655 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:19,655 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:19,665 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:19,665 : INFO : EPOCH - 5 : training on 5920713 raw words (4441238 effective words) took 4.3s, 1041583 effective words/s\n",
      "2018-11-05 14:49:20,671 : INFO : EPOCH 6 - PROGRESS: at 23.21% examples, 1043489 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:21,675 : INFO : EPOCH 6 - PROGRESS: at 46.50% examples, 1037281 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:49:22,687 : INFO : EPOCH 6 - PROGRESS: at 69.44% examples, 1030117 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:23,691 : INFO : EPOCH 6 - PROGRESS: at 93.33% examples, 1033302 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:23,971 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:23,981 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:23,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:23,981 : INFO : EPOCH - 6 : training on 5920713 raw words (4442341 effective words) took 4.3s, 1029440 effective words/s\n",
      "2018-11-05 14:49:25,000 : INFO : EPOCH 7 - PROGRESS: at 23.82% examples, 1059504 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:26,001 : INFO : EPOCH 7 - PROGRESS: at 46.34% examples, 1029940 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:49:27,003 : INFO : EPOCH 7 - PROGRESS: at 67.16% examples, 995371 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:49:28,004 : INFO : EPOCH 7 - PROGRESS: at 89.84% examples, 997212 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:28,509 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:28,519 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:28,529 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:28,529 : INFO : EPOCH - 7 : training on 5920713 raw words (4441600 effective words) took 4.5s, 978469 effective words/s\n",
      "2018-11-05 14:49:29,547 : INFO : EPOCH 8 - PROGRESS: at 23.02% examples, 1031516 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:30,546 : INFO : EPOCH 8 - PROGRESS: at 46.34% examples, 1029719 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:31,554 : INFO : EPOCH 8 - PROGRESS: at 69.73% examples, 1032825 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:49:32,558 : INFO : EPOCH 8 - PROGRESS: at 93.67% examples, 1036370 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:49:32,810 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:32,820 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:32,820 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:32,820 : INFO : EPOCH - 8 : training on 5920713 raw words (4441109 effective words) took 4.3s, 1036392 effective words/s\n",
      "2018-11-05 14:49:33,836 : INFO : EPOCH 9 - PROGRESS: at 23.02% examples, 1027375 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:34,842 : INFO : EPOCH 9 - PROGRESS: at 45.80% examples, 1015525 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:35,854 : INFO : EPOCH 9 - PROGRESS: at 69.44% examples, 1026002 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:36,850 : INFO : EPOCH 9 - PROGRESS: at 92.58% examples, 1023712 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:37,152 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:37,152 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:37,152 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:37,152 : INFO : EPOCH - 9 : training on 5920713 raw words (4440859 effective words) took 4.3s, 1026052 effective words/s\n",
      "2018-11-05 14:49:38,159 : INFO : EPOCH 10 - PROGRESS: at 22.33% examples, 1006676 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:49:39,172 : INFO : EPOCH 10 - PROGRESS: at 44.32% examples, 986693 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:49:40,169 : INFO : EPOCH 10 - PROGRESS: at 67.33% examples, 998331 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-05 14:49:41,175 : INFO : EPOCH 10 - PROGRESS: at 89.84% examples, 996346 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:49:41,574 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:41,574 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:41,584 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:41,584 : INFO : EPOCH - 10 : training on 5920713 raw words (4441137 effective words) took 4.4s, 1003604 effective words/s\n",
      "2018-11-05 14:49:42,596 : INFO : EPOCH 11 - PROGRESS: at 20.04% examples, 897535 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:43,598 : INFO : EPOCH 11 - PROGRESS: at 43.41% examples, 966700 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:44,618 : INFO : EPOCH 11 - PROGRESS: at 66.48% examples, 980438 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:45,627 : INFO : EPOCH 11 - PROGRESS: at 90.02% examples, 993870 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:46,025 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:46,025 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:46,025 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:46,025 : INFO : EPOCH - 11 : training on 5920713 raw words (4441233 effective words) took 4.4s, 1000362 effective words/s\n",
      "2018-11-05 14:49:47,045 : INFO : EPOCH 12 - PROGRESS: at 23.99% examples, 1066983 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:48,051 : INFO : EPOCH 12 - PROGRESS: at 47.80% examples, 1059824 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:49,051 : INFO : EPOCH 12 - PROGRESS: at 70.78% examples, 1047257 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:50,058 : INFO : EPOCH 12 - PROGRESS: at 94.47% examples, 1043672 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:50,288 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:50,288 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:50,298 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:50,298 : INFO : EPOCH - 12 : training on 5920713 raw words (4440338 effective words) took 4.3s, 1040381 effective words/s\n",
      "2018-11-05 14:49:51,314 : INFO : EPOCH 13 - PROGRESS: at 24.17% examples, 1076475 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:52,321 : INFO : EPOCH 13 - PROGRESS: at 47.50% examples, 1053948 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:49:53,331 : INFO : EPOCH 13 - PROGRESS: at 70.96% examples, 1047080 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:54,341 : INFO : EPOCH 13 - PROGRESS: at 95.29% examples, 1049889 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:54,539 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:54,549 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:54,549 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:54,549 : INFO : EPOCH - 13 : training on 5920713 raw words (4441161 effective words) took 4.2s, 1046793 effective words/s\n",
      "2018-11-05 14:49:55,564 : INFO : EPOCH 14 - PROGRESS: at 22.33% examples, 1001373 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:49:56,576 : INFO : EPOCH 14 - PROGRESS: at 43.75% examples, 972090 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:57,590 : INFO : EPOCH 14 - PROGRESS: at 65.38% examples, 962251 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:58,597 : INFO : EPOCH 14 - PROGRESS: at 87.50% examples, 967109 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:49:59,084 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:49:59,084 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:49:59,094 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:49:59,094 : INFO : EPOCH - 14 : training on 5920713 raw words (4442031 effective words) took 4.5s, 978422 effective words/s\n",
      "2018-11-05 14:50:00,110 : INFO : EPOCH 15 - PROGRESS: at 23.40% examples, 1041051 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:01,101 : INFO : EPOCH 15 - PROGRESS: at 47.00% examples, 1042205 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:02,108 : INFO : EPOCH 15 - PROGRESS: at 70.92% examples, 1049577 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:03,123 : INFO : EPOCH 15 - PROGRESS: at 94.64% examples, 1044865 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:03,340 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:03,340 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:03,340 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:50:03,340 : INFO : EPOCH - 15 : training on 5920713 raw words (4441186 effective words) took 4.2s, 1046653 effective words/s\n",
      "2018-11-05 14:50:04,353 : INFO : EPOCH 16 - PROGRESS: at 23.60% examples, 1051556 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:05,356 : INFO : EPOCH 16 - PROGRESS: at 47.66% examples, 1058497 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:06,362 : INFO : EPOCH 16 - PROGRESS: at 71.11% examples, 1054094 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:07,361 : INFO : EPOCH 16 - PROGRESS: at 94.64% examples, 1047859 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:07,566 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:07,576 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:07,576 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:50:07,576 : INFO : EPOCH - 16 : training on 5920713 raw words (4441577 effective words) took 4.2s, 1049041 effective words/s\n",
      "2018-11-05 14:50:08,582 : INFO : EPOCH 17 - PROGRESS: at 23.99% examples, 1074812 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:09,589 : INFO : EPOCH 17 - PROGRESS: at 47.48% examples, 1059826 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:10,590 : INFO : EPOCH 17 - PROGRESS: at 70.96% examples, 1053665 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:11,603 : INFO : EPOCH 17 - PROGRESS: at 95.13% examples, 1053452 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:11,805 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:11,805 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:11,815 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:50:11,815 : INFO : EPOCH - 17 : training on 5920713 raw words (4441941 effective words) took 4.2s, 1049595 effective words/s\n",
      "2018-11-05 14:50:12,817 : INFO : EPOCH 18 - PROGRESS: at 24.17% examples, 1081401 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:13,822 : INFO : EPOCH 18 - PROGRESS: at 47.48% examples, 1057575 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:14,832 : INFO : EPOCH 18 - PROGRESS: at 71.41% examples, 1057588 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:15,849 : INFO : EPOCH 18 - PROGRESS: at 95.64% examples, 1056139 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:16,012 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:16,012 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:16,012 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:50:16,012 : INFO : EPOCH - 18 : training on 5920713 raw words (4441295 effective words) took 4.2s, 1058487 effective words/s\n",
      "2018-11-05 14:50:17,017 : INFO : EPOCH 19 - PROGRESS: at 23.60% examples, 1049384 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:18,020 : INFO : EPOCH 19 - PROGRESS: at 47.31% examples, 1051842 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:19,039 : INFO : EPOCH 19 - PROGRESS: at 70.92% examples, 1050887 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:20,039 : INFO : EPOCH 19 - PROGRESS: at 94.82% examples, 1048678 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:20,245 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:20,245 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:20,245 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-05 14:50:20,245 : INFO : EPOCH - 19 : training on 5920713 raw words (4441247 effective words) took 4.2s, 1051191 effective words/s\n",
      "2018-11-05 14:50:21,252 : INFO : EPOCH 20 - PROGRESS: at 23.60% examples, 1055377 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:22,259 : INFO : EPOCH 20 - PROGRESS: at 47.95% examples, 1069179 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:23,252 : INFO : EPOCH 20 - PROGRESS: at 71.29% examples, 1058021 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:24,262 : INFO : EPOCH 20 - PROGRESS: at 95.13% examples, 1053102 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:24,454 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:24,454 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:24,464 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:50:24,464 : INFO : EPOCH - 20 : training on 5920713 raw words (4441016 effective words) took 4.2s, 1053983 effective words/s\n",
      "2018-11-05 14:50:25,478 : INFO : EPOCH 21 - PROGRESS: at 24.17% examples, 1068843 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:26,484 : INFO : EPOCH 21 - PROGRESS: at 48.11% examples, 1068654 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:27,480 : INFO : EPOCH 21 - PROGRESS: at 70.09% examples, 1038900 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:28,486 : INFO : EPOCH 21 - PROGRESS: at 94.34% examples, 1042955 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:28,721 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:28,721 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:28,731 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:50:28,731 : INFO : EPOCH - 21 : training on 5920713 raw words (4440974 effective words) took 4.3s, 1042049 effective words/s\n",
      "2018-11-05 14:50:29,738 : INFO : EPOCH 22 - PROGRESS: at 24.17% examples, 1075328 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:30,748 : INFO : EPOCH 22 - PROGRESS: at 47.95% examples, 1063987 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:31,755 : INFO : EPOCH 22 - PROGRESS: at 71.88% examples, 1061526 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:32,766 : INFO : EPOCH 22 - PROGRESS: at 96.08% examples, 1060436 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:32,906 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:32,907 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:32,907 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:50:32,907 : INFO : EPOCH - 22 : training on 5920713 raw words (4441193 effective words) took 4.2s, 1061501 effective words/s\n",
      "2018-11-05 14:50:33,914 : INFO : EPOCH 23 - PROGRESS: at 23.40% examples, 1046766 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:34,932 : INFO : EPOCH 23 - PROGRESS: at 47.31% examples, 1052499 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:35,938 : INFO : EPOCH 23 - PROGRESS: at 71.41% examples, 1056771 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:36,945 : INFO : EPOCH 23 - PROGRESS: at 95.29% examples, 1052521 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:37,128 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:37,138 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:37,138 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:50:37,138 : INFO : EPOCH - 23 : training on 5920713 raw words (4440949 effective words) took 4.2s, 1053649 effective words/s\n",
      "2018-11-05 14:50:38,135 : INFO : EPOCH 24 - PROGRESS: at 23.02% examples, 1038718 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:39,138 : INFO : EPOCH 24 - PROGRESS: at 47.50% examples, 1060147 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:40,145 : INFO : EPOCH 24 - PROGRESS: at 70.96% examples, 1052882 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:41,148 : INFO : EPOCH 24 - PROGRESS: at 94.82% examples, 1050893 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:41,342 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:41,344 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:41,350 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:50:41,351 : INFO : EPOCH - 24 : training on 5920713 raw words (4441262 effective words) took 4.2s, 1051181 effective words/s\n",
      "2018-11-05 14:50:42,372 : INFO : EPOCH 25 - PROGRESS: at 23.99% examples, 1073864 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:43,382 : INFO : EPOCH 25 - PROGRESS: at 44.68% examples, 992203 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:44,388 : INFO : EPOCH 25 - PROGRESS: at 68.33% examples, 1010443 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:45,388 : INFO : EPOCH 25 - PROGRESS: at 92.58% examples, 1024725 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:45,704 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:45,704 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:45,705 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:50:45,705 : INFO : EPOCH - 25 : training on 5920713 raw words (4441897 effective words) took 4.3s, 1022463 effective words/s\n",
      "2018-11-05 14:50:46,716 : INFO : EPOCH 26 - PROGRESS: at 24.17% examples, 1083139 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:47,726 : INFO : EPOCH 26 - PROGRESS: at 47.80% examples, 1065485 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:48,724 : INFO : EPOCH 26 - PROGRESS: at 71.57% examples, 1063023 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:49,734 : INFO : EPOCH 26 - PROGRESS: at 95.79% examples, 1061734 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:49,881 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:49,891 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:49,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:50:49,901 : INFO : EPOCH - 26 : training on 5920713 raw words (4441978 effective words) took 4.2s, 1062985 effective words/s\n",
      "2018-11-05 14:50:50,905 : INFO : EPOCH 27 - PROGRESS: at 23.40% examples, 1048819 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:50:51,911 : INFO : EPOCH 27 - PROGRESS: at 47.00% examples, 1044960 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:52,910 : INFO : EPOCH 27 - PROGRESS: at 71.41% examples, 1055464 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:53,920 : INFO : EPOCH 27 - PROGRESS: at 95.13% examples, 1050534 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:54,118 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:54,118 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:54,118 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:50:54,118 : INFO : EPOCH - 27 : training on 5920713 raw words (4440967 effective words) took 4.2s, 1052937 effective words/s\n",
      "2018-11-05 14:50:55,121 : INFO : EPOCH 28 - PROGRESS: at 23.60% examples, 1059884 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:56,131 : INFO : EPOCH 28 - PROGRESS: at 47.95% examples, 1070004 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:57,138 : INFO : EPOCH 28 - PROGRESS: at 71.41% examples, 1059074 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:50:58,148 : INFO : EPOCH 28 - PROGRESS: at 93.68% examples, 1035343 words/s, in_qsize 5, out_qsize 1\n",
      "2018-11-05 14:50:58,398 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:50:58,398 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:50:58,398 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:50:58,398 : INFO : EPOCH - 28 : training on 5920713 raw words (4441606 effective words) took 4.3s, 1038666 effective words/s\n",
      "2018-11-05 14:50:59,404 : INFO : EPOCH 29 - PROGRESS: at 23.82% examples, 1068799 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:00,396 : INFO : EPOCH 29 - PROGRESS: at 47.48% examples, 1059736 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-05 14:51:01,411 : INFO : EPOCH 29 - PROGRESS: at 70.92% examples, 1052542 words/s, in_qsize 5, out_qsize 1\n",
      "2018-11-05 14:51:02,411 : INFO : EPOCH 29 - PROGRESS: at 95.48% examples, 1057964 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:02,613 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:51:02,613 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:51:02,623 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:51:02,623 : INFO : EPOCH - 29 : training on 5920713 raw words (4441697 effective words) took 4.2s, 1053429 effective words/s\n",
      "2018-11-05 14:51:03,627 : INFO : EPOCH 30 - PROGRESS: at 23.99% examples, 1073326 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:04,625 : INFO : EPOCH 30 - PROGRESS: at 47.66% examples, 1058391 words/s, in_qsize 5, out_qsize 1\n",
      "2018-11-05 14:51:05,625 : INFO : EPOCH 30 - PROGRESS: at 71.41% examples, 1058448 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:06,641 : INFO : EPOCH 30 - PROGRESS: at 95.48% examples, 1057534 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:06,810 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:51:06,820 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:51:06,820 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:51:06,820 : INFO : EPOCH - 30 : training on 5920713 raw words (4439937 effective words) took 4.2s, 1057928 effective words/s\n",
      "2018-11-05 14:51:07,833 : INFO : EPOCH 31 - PROGRESS: at 23.40% examples, 1045847 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:51:08,847 : INFO : EPOCH 31 - PROGRESS: at 47.15% examples, 1044511 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:09,850 : INFO : EPOCH 31 - PROGRESS: at 71.29% examples, 1052948 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:10,853 : INFO : EPOCH 31 - PROGRESS: at 94.97% examples, 1049312 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:51:11,047 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:51:11,047 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:51:11,057 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:51:11,057 : INFO : EPOCH - 31 : training on 5920713 raw words (4441675 effective words) took 4.2s, 1049857 effective words/s\n",
      "2018-11-05 14:51:12,063 : INFO : EPOCH 32 - PROGRESS: at 23.62% examples, 1057084 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:13,070 : INFO : EPOCH 32 - PROGRESS: at 48.11% examples, 1071650 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:14,073 : INFO : EPOCH 32 - PROGRESS: at 71.41% examples, 1060262 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:15,076 : INFO : EPOCH 32 - PROGRESS: at 95.29% examples, 1055914 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:15,263 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:51:15,263 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:51:15,273 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:51:15,273 : INFO : EPOCH - 32 : training on 5920713 raw words (4441006 effective words) took 4.2s, 1055438 effective words/s\n",
      "2018-11-05 14:51:16,280 : INFO : EPOCH 33 - PROGRESS: at 24.17% examples, 1079477 words/s, in_qsize 5, out_qsize 1\n",
      "2018-11-05 14:51:17,287 : INFO : EPOCH 33 - PROGRESS: at 47.95% examples, 1065217 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:51:18,293 : INFO : EPOCH 33 - PROGRESS: at 71.41% examples, 1055775 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:19,293 : INFO : EPOCH 33 - PROGRESS: at 95.94% examples, 1060569 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:19,463 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:51:19,463 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:51:19,473 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:51:19,473 : INFO : EPOCH - 33 : training on 5920713 raw words (4440960 effective words) took 4.2s, 1057952 effective words/s\n",
      "2018-11-05 14:51:20,478 : INFO : EPOCH 34 - PROGRESS: at 22.17% examples, 996549 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:51:21,484 : INFO : EPOCH 34 - PROGRESS: at 45.66% examples, 1016301 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:51:22,490 : INFO : EPOCH 34 - PROGRESS: at 69.73% examples, 1035281 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:51:23,488 : INFO : EPOCH 34 - PROGRESS: at 93.33% examples, 1034705 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:51:23,743 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:51:23,743 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:51:23,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:51:23,753 : INFO : EPOCH - 34 : training on 5920713 raw words (4440985 effective words) took 4.3s, 1037608 effective words/s\n",
      "2018-11-05 14:51:24,766 : INFO : EPOCH 35 - PROGRESS: at 23.78% examples, 1057191 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:25,759 : INFO : EPOCH 35 - PROGRESS: at 47.66% examples, 1058584 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:26,763 : INFO : EPOCH 35 - PROGRESS: at 71.71% examples, 1062503 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:27,772 : INFO : EPOCH 35 - PROGRESS: at 92.06% examples, 1017806 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:28,095 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:51:28,095 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:51:28,095 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:51:28,095 : INFO : EPOCH - 35 : training on 5920713 raw words (4441505 effective words) took 4.3s, 1023720 effective words/s\n",
      "2018-11-05 14:51:29,106 : INFO : EPOCH 36 - PROGRESS: at 21.95% examples, 992596 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:30,103 : INFO : EPOCH 36 - PROGRESS: at 46.16% examples, 1030168 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:31,110 : INFO : EPOCH 36 - PROGRESS: at 69.44% examples, 1032366 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:32,109 : INFO : EPOCH 36 - PROGRESS: at 93.50% examples, 1038000 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:51:32,376 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:51:32,376 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:51:32,376 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:51:32,386 : INFO : EPOCH - 36 : training on 5920713 raw words (4441741 effective words) took 4.3s, 1038078 effective words/s\n",
      "2018-11-05 14:51:33,387 : INFO : EPOCH 37 - PROGRESS: at 24.17% examples, 1082240 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:34,394 : INFO : EPOCH 37 - PROGRESS: at 47.95% examples, 1069736 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:35,403 : INFO : EPOCH 37 - PROGRESS: at 71.29% examples, 1056101 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:36,399 : INFO : EPOCH 37 - PROGRESS: at 95.79% examples, 1061170 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:36,563 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:51:36,568 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:51:36,570 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:51:36,571 : INFO : EPOCH - 37 : training on 5920713 raw words (4442190 effective words) took 4.2s, 1057861 effective words/s\n",
      "2018-11-05 14:51:37,576 : INFO : EPOCH 38 - PROGRESS: at 23.99% examples, 1073965 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:51:38,579 : INFO : EPOCH 38 - PROGRESS: at 46.82% examples, 1044310 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:39,604 : INFO : EPOCH 38 - PROGRESS: at 70.96% examples, 1051593 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:40,596 : INFO : EPOCH 38 - PROGRESS: at 94.82% examples, 1048569 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:40,806 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:51:40,816 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-05 14:51:40,816 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:51:40,816 : INFO : EPOCH - 38 : training on 5920713 raw words (4441428 effective words) took 4.2s, 1049617 effective words/s\n",
      "2018-11-05 14:51:41,822 : INFO : EPOCH 39 - PROGRESS: at 23.40% examples, 1053166 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 14:51:42,829 : INFO : EPOCH 39 - PROGRESS: at 46.82% examples, 1044148 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:43,836 : INFO : EPOCH 39 - PROGRESS: at 68.52% examples, 1016614 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:44,822 : INFO : EPOCH 39 - PROGRESS: at 91.35% examples, 1014179 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:45,176 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:51:45,176 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:51:45,186 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:51:45,186 : INFO : EPOCH - 39 : training on 5920713 raw words (4442148 effective words) took 4.4s, 1018334 effective words/s\n",
      "2018-11-05 14:51:46,189 : INFO : EPOCH 40 - PROGRESS: at 23.60% examples, 1057948 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:47,189 : INFO : EPOCH 40 - PROGRESS: at 47.95% examples, 1070466 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:48,189 : INFO : EPOCH 40 - PROGRESS: at 71.57% examples, 1064356 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:49,196 : INFO : EPOCH 40 - PROGRESS: at 95.13% examples, 1056254 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 14:51:49,386 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 14:51:49,386 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 14:51:49,386 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 14:51:49,386 : INFO : EPOCH - 40 : training on 5920713 raw words (4439911 effective words) took 4.2s, 1057525 effective words/s\n",
      "2018-11-05 14:51:49,386 : INFO : training on a 236828520 raw words (177650270 effective words) took 171.2s, 1037674 effective words/s\n"
     ]
    }
   ],
   "source": [
    "doc2vecModel.train(taggedDocs, total_examples = doc2vecModel.corpus_count, epochs = doc2vecModel.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:51:49.824150Z",
     "start_time": "2018-11-05T21:51:49.396158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83336973, -0.5986345 , -1.0499012 ,  0.705882  , -2.0995665 ,\n",
       "        0.23506959, -0.7789486 ,  1.5394003 , -0.7136909 , -0.82254475,\n",
       "       -0.5283045 , -0.55548066,  1.2660635 ,  1.0626009 , -1.4789108 ,\n",
       "       -0.3345526 ,  0.35198566,  0.94045407, -0.623906  , -0.2252483 ,\n",
       "       -3.1190784 ,  0.9477625 , -0.7620193 ,  1.9794213 ,  0.72246337,\n",
       "        0.4865187 , -0.47181603, -1.1284602 , -1.2948472 , -2.414973  ,\n",
       "        0.87563956, -1.2747077 , -0.40591377,  0.54443544,  0.15972735,\n",
       "        2.3147593 , -1.3057712 ,  0.15407787, -2.2205367 , -0.4058556 ,\n",
       "       -1.3378377 , -0.2862275 , -4.262958  , -0.5495442 , -0.16821636,\n",
       "       -1.6936331 ,  1.4303458 ,  2.1376326 , -0.36495203,  0.3570954 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vecModel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:51:50.252142Z",
     "start_time": "2018-11-05T21:51:49.824150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc2vecModel.docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:51:50.733134Z",
     "start_time": "2018-11-05T21:51:50.252142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83336973, -0.5986345 , -1.0499012 ,  0.705882  , -2.0995665 ,\n",
       "        0.23506959, -0.7789486 ,  1.5394003 , -0.7136909 , -0.82254475,\n",
       "       -0.5283045 , -0.55548066,  1.2660635 ,  1.0626009 , -1.4789108 ,\n",
       "       -0.3345526 ,  0.35198566,  0.94045407, -0.623906  , -0.2252483 ,\n",
       "       -3.1190784 ,  0.9477625 , -0.7620193 ,  1.9794213 ,  0.72246337,\n",
       "        0.4865187 , -0.47181603, -1.1284602 , -1.2948472 , -2.414973  ,\n",
       "        0.87563956, -1.2747077 , -0.40591377,  0.54443544,  0.15972735,\n",
       "        2.3147593 , -1.3057712 ,  0.15407787, -2.2205367 , -0.4058556 ,\n",
       "       -1.3378377 , -0.2862275 , -4.262958  , -0.5495442 , -0.16821636,\n",
       "       -1.6936331 ,  1.4303458 ,  2.1376326 , -0.36495203,  0.3570954 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vecModel.docvecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle model\n",
    "\n",
    "First we'll evalute the Kaggle model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T21:53:56.733909Z",
     "start_time": "2018-11-05T21:51:50.733134Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>StdDev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.008925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Accuracy    StdDev\n",
       "0  RandomForestClassifier     0.818  0.008925"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Init vars and params\n",
    "eFolds = 10\n",
    "eSeed = 10\n",
    "\n",
    "# Use accuracy since this is a classification problem\n",
    "eScore = 'accuracy'\n",
    "\n",
    "modelName = 'RandomForestClassifier'\n",
    "RandomForestClassifier(n_estimators = 100)\n",
    "xTrain = doc2vecModel.docvecs\n",
    "yTrain = df.iloc[:, 1]\n",
    "\n",
    "_DF = pd.DataFrame(columns = ['Model', 'Accuracy', 'StdDev'])\n",
    "_Results = {}\n",
    "_model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "kFold = KFold(n_splits = eFolds, random_state = eSeed)\n",
    "_Results[modelName] = cross_val_score(_model, xTrain, yTrain, cv = kFold, scoring = eScore)\n",
    "\n",
    "_DF.loc[len(_DF)] = list(['RandomForestClassifier', _Results[modelName].mean(), _Results[modelName].std()])\n",
    "display(_DF.sort_values(by = ['Accuracy', 'StdDev', 'Model'], ascending = [False, True, True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard write-up models\n",
    "\n",
    "Next we'll train the standard set of models (LR, LDA, etc.) we use in the majority of our write-ups for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T22:31:00.780037Z",
     "start_time": "2018-11-05T22:22:06.540521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LR ....\n",
      "Training LDA ....\n",
      "Training KNN ....\n",
      "Training CART ....\n",
      "Training NB ....\n",
      "Training SVM ....\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>StdDev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.84480</td>\n",
       "      <td>0.007198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA</td>\n",
       "      <td>0.84168</td>\n",
       "      <td>0.009679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.84136</td>\n",
       "      <td>0.010189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.79420</td>\n",
       "      <td>0.009315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.78476</td>\n",
       "      <td>0.008591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CART</td>\n",
       "      <td>0.68780</td>\n",
       "      <td>0.007391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  Accuracy    StdDev\n",
       "5   SVM   0.84480  0.007198\n",
       "1   LDA   0.84168  0.009679\n",
       "0    LR   0.84136  0.010189\n",
       "4    NB   0.79420  0.009315\n",
       "2   KNN   0.78476  0.008591\n",
       "3  CART   0.68780  0.007391"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init vars\n",
    "folds = 10\n",
    "seed = 10\n",
    "models = []\n",
    "results = {}\n",
    "\n",
    "# Use accuracy since this is a classification\n",
    "score = 'accuracy'\n",
    "\n",
    "# Assign training features and labels\n",
    "xTrain = doc2vecModel.docvecs\n",
    "yTrain = df.iloc[:, 1]\n",
    "\n",
    "# Instantiate model objects\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# Create a Pandas DF to hold all our spiffy results\n",
    "_df = pd.DataFrame(columns = ['Model', 'Accuracy', 'StdDev'])\n",
    "\n",
    "# Run the models\n",
    "for modelName, model in models:\n",
    "    print(\"Training\", modelName, \"....\")\n",
    "    # Implement K-fold cross validation where K = 10\n",
    "    kFold = KFold(n_splits = folds, random_state = seed)\n",
    "    results[modelName] = cross_val_score(model, xTrain, yTrain, cv = kFold, scoring = score)\n",
    "    _df.loc[len(_df)] = list([modelName, results[modelName].mean(), results[modelName].std()])\n",
    "\n",
    "# Print results sorted by Mean desc, StdDev asc, Model asc\n",
    "_df.sort_values(by = ['Accuracy', 'StdDev', 'Model'], ascending = [False, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T22:31:01.494037Z",
     "start_time": "2018-11-05T22:31:00.782037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize = (8,6))\n",
    "figure.suptitle(\"Model Results\")\n",
    "axis = figure.add_subplot(111)\n",
    "plt.boxplot(results.values())\n",
    "axis.set_xticklabels(results.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard model comments\n",
    "\n",
    "The first thing I noticed right away is how much faster training the set of models was.  Training wrapped up in about 9 mins compared to the sometimes hours required in previous write-ups.  Accuracy was also high being only two percentage points less then the baseline model:\n",
    "\n",
    "|Model|Accuracy|Best Params                                      |\n",
    "|-------------------|--------|-----------------------------------|\n",
    "|LR (baseline)      |86.35%  |{'LR__C': 0.1, 'LR__penalty': 'l1'}|\n",
    "|SVM centroid       |86.36%  |Scikit-learn defaults              |\n",
    "|SVM Doc2Vec        |84.48%  |Scikit-learn defaults              |\n",
    "\n",
    "\n",
    "Clearly for very large data sets the small drop in accuracy might be more than offset by the greatly reduced training time required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100 \n",
    "dm = 0 #0 = dbow; 1 = dmpv\n",
    "worker_count = 1 #number of parallel processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:24:06.098389Z",
     "start_time": "2018-11-05T23:24:05.668389Z"
    }
   },
   "outputs": [],
   "source": [
    "doc2vecModel = Doc2Vec(\n",
    "    vector_size = 300,\n",
    "    window_size = 15,\n",
    "    min_count = 4,\n",
    "    sampling_threshold = 1e-5,\n",
    "    negative_size = 5,\n",
    "    train_epoch = 100 ,\n",
    "    dm = 0, #0 = dbow; 1 = dmpv\n",
    "    #worker_count = 1 #number of parallel processes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:28:11.673301Z",
     "start_time": "2018-11-05T23:25:08.509167Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-05 16:25:08,957 : INFO : collecting all words and their counts\n",
      "2018-11-05 16:25:08,957 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-11-05 16:25:09,349 : INFO : PROGRESS: at example #10000, processed 2385574 words (6097466/s), 51527 word types, 10000 tags\n",
      "2018-11-05 16:25:09,719 : INFO : PROGRESS: at example #20000, processed 4747503 words (6346052/s), 67813 word types, 20000 tags\n",
      "2018-11-05 16:25:09,899 : INFO : collected 74218 word types and 25000 unique tags from a corpus of 25000 examples and 5920713 words\n",
      "2018-11-05 16:25:09,899 : INFO : Loading a fresh vocabulary\n",
      "2018-11-05 16:25:10,088 : INFO : effective_min_count=5 retains 28757 unique words (38% of original 74218, drops 45461)\n",
      "2018-11-05 16:25:10,088 : INFO : effective_min_count=5 leaves 5845494 word corpus (98% of original 5920713, drops 75219)\n",
      "2018-11-05 16:25:10,159 : INFO : deleting the raw counts dictionary of 74218 items\n",
      "2018-11-05 16:25:10,169 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-11-05 16:25:10,169 : INFO : downsampling leaves estimated 4363319 word corpus (74.6% of prior 5845494)\n",
      "2018-11-05 16:25:10,259 : INFO : estimated required memory for 28757 words and 100 dimensions: 47384100 bytes\n",
      "2018-11-05 16:25:10,259 : INFO : resetting layer weights\n",
      "2018-11-05 16:25:10,817 : INFO : training model with 3 workers on 28757 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-05 16:25:11,816 : INFO : EPOCH 1 - PROGRESS: at 22.50% examples, 1003417 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:12,824 : INFO : EPOCH 1 - PROGRESS: at 45.49% examples, 1000096 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:13,843 : INFO : EPOCH 1 - PROGRESS: at 68.82% examples, 1003538 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:14,844 : INFO : EPOCH 1 - PROGRESS: at 92.58% examples, 1009826 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:15,139 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:25:15,139 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:25:15,139 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:25:15,149 : INFO : EPOCH - 1 : training on 5920713 raw words (4388395 effective words) took 4.3s, 1013974 effective words/s\n",
      "2018-11-05 16:25:16,150 : INFO : EPOCH 2 - PROGRESS: at 20.36% examples, 906838 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:17,151 : INFO : EPOCH 2 - PROGRESS: at 43.94% examples, 968784 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:18,160 : INFO : EPOCH 2 - PROGRESS: at 68.01% examples, 994891 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:19,170 : INFO : EPOCH 2 - PROGRESS: at 91.35% examples, 1000312 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:19,501 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:25:19,511 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:25:19,511 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:25:19,511 : INFO : EPOCH - 2 : training on 5920713 raw words (4387377 effective words) took 4.4s, 1005211 effective words/s\n",
      "2018-11-05 16:25:20,531 : INFO : EPOCH 3 - PROGRESS: at 22.54% examples, 991631 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:21,550 : INFO : EPOCH 3 - PROGRESS: at 46.34% examples, 1008123 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:22,551 : INFO : EPOCH 3 - PROGRESS: at 69.44% examples, 1011117 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:23,554 : INFO : EPOCH 3 - PROGRESS: at 92.75% examples, 1010802 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:23,828 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:25:23,838 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:25:23,838 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:25:23,838 : INFO : EPOCH - 3 : training on 5920713 raw words (4387271 effective words) took 4.3s, 1015221 effective words/s\n",
      "2018-11-05 16:25:24,845 : INFO : EPOCH 4 - PROGRESS: at 23.02% examples, 1025740 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:25,851 : INFO : EPOCH 4 - PROGRESS: at 46.50% examples, 1024779 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:26,855 : INFO : EPOCH 4 - PROGRESS: at 69.44% examples, 1019502 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:27,851 : INFO : EPOCH 4 - PROGRESS: at 93.14% examples, 1021317 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:28,154 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:25:28,154 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:25:28,164 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:25:28,164 : INFO : EPOCH - 4 : training on 5920713 raw words (4388147 effective words) took 4.3s, 1015086 effective words/s\n",
      "2018-11-05 16:25:29,169 : INFO : EPOCH 5 - PROGRESS: at 22.71% examples, 1010964 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:30,189 : INFO : EPOCH 5 - PROGRESS: at 43.12% examples, 945359 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:31,188 : INFO : EPOCH 5 - PROGRESS: at 66.18% examples, 967593 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:32,189 : INFO : EPOCH 5 - PROGRESS: at 90.50% examples, 990397 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:32,599 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:25:32,599 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:25:32,619 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:25:32,619 : INFO : EPOCH - 5 : training on 5920713 raw words (4389378 effective words) took 4.4s, 987220 effective words/s\n",
      "2018-11-05 16:25:33,635 : INFO : EPOCH 6 - PROGRESS: at 24.17% examples, 1057415 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:34,624 : INFO : EPOCH 6 - PROGRESS: at 45.31% examples, 995118 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:35,633 : INFO : EPOCH 6 - PROGRESS: at 68.47% examples, 1001645 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:36,632 : INFO : EPOCH 6 - PROGRESS: at 91.01% examples, 995172 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:37,028 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:25:37,039 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:25:37,042 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:25:37,043 : INFO : EPOCH - 6 : training on 5920713 raw words (4388859 effective words) took 4.4s, 989396 effective words/s\n",
      "2018-11-05 16:25:38,057 : INFO : EPOCH 7 - PROGRESS: at 22.03% examples, 973459 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:39,063 : INFO : EPOCH 7 - PROGRESS: at 45.04% examples, 986677 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:40,064 : INFO : EPOCH 7 - PROGRESS: at 68.80% examples, 1007174 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:41,064 : INFO : EPOCH 7 - PROGRESS: at 91.01% examples, 997742 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:41,445 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:25:41,450 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:25:41,452 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:25:41,452 : INFO : EPOCH - 7 : training on 5920713 raw words (4388758 effective words) took 4.4s, 996625 effective words/s\n",
      "2018-11-05 16:25:42,477 : INFO : EPOCH 8 - PROGRESS: at 21.18% examples, 943176 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:43,465 : INFO : EPOCH 8 - PROGRESS: at 44.16% examples, 971760 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:44,481 : INFO : EPOCH 8 - PROGRESS: at 67.33% examples, 986927 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:45,482 : INFO : EPOCH 8 - PROGRESS: at 89.84% examples, 985827 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:45,887 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:25:45,887 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:25:45,897 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-05 16:25:45,897 : INFO : EPOCH - 8 : training on 5920713 raw words (4387846 effective words) took 4.4s, 990545 effective words/s\n",
      "2018-11-05 16:25:46,914 : INFO : EPOCH 9 - PROGRESS: at 22.54% examples, 999266 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:47,909 : INFO : EPOCH 9 - PROGRESS: at 45.31% examples, 998078 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:48,917 : INFO : EPOCH 9 - PROGRESS: at 68.01% examples, 997035 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:49,920 : INFO : EPOCH 9 - PROGRESS: at 91.53% examples, 1002397 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:50,264 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:25:50,264 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:25:50,264 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:25:50,274 : INFO : EPOCH - 9 : training on 5920713 raw words (4388447 effective words) took 4.4s, 1005406 effective words/s\n",
      "2018-11-05 16:25:51,261 : INFO : EPOCH 10 - PROGRESS: at 22.54% examples, 1003871 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:52,279 : INFO : EPOCH 10 - PROGRESS: at 45.97% examples, 1013599 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:53,280 : INFO : EPOCH 10 - PROGRESS: at 68.33% examples, 1003069 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:54,278 : INFO : EPOCH 10 - PROGRESS: at 91.01% examples, 999372 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:54,681 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:25:54,691 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:25:54,701 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:25:54,701 : INFO : EPOCH - 10 : training on 5920713 raw words (4388469 effective words) took 4.4s, 991734 effective words/s\n",
      "2018-11-05 16:25:55,708 : INFO : EPOCH 11 - PROGRESS: at 23.60% examples, 1047429 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:56,708 : INFO : EPOCH 11 - PROGRESS: at 45.31% examples, 998344 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:57,715 : INFO : EPOCH 11 - PROGRESS: at 68.66% examples, 1006071 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:25:58,711 : INFO : EPOCH 11 - PROGRESS: at 90.02% examples, 987520 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:25:59,141 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:25:59,141 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:25:59,142 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:25:59,142 : INFO : EPOCH - 11 : training on 5920713 raw words (4388971 effective words) took 4.4s, 986654 effective words/s\n",
      "2018-11-05 16:26:00,161 : INFO : EPOCH 12 - PROGRESS: at 23.02% examples, 1019297 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:01,157 : INFO : EPOCH 12 - PROGRESS: at 45.49% examples, 1001637 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:02,164 : INFO : EPOCH 12 - PROGRESS: at 68.80% examples, 1009353 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:03,167 : INFO : EPOCH 12 - PROGRESS: at 92.41% examples, 1012871 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:03,460 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:26:03,460 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:26:03,470 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:26:03,470 : INFO : EPOCH - 12 : training on 5920713 raw words (4388227 effective words) took 4.3s, 1017318 effective words/s\n",
      "2018-11-05 16:26:04,484 : INFO : EPOCH 13 - PROGRESS: at 23.02% examples, 1017027 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:05,496 : INFO : EPOCH 13 - PROGRESS: at 46.50% examples, 1017573 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:06,500 : INFO : EPOCH 13 - PROGRESS: at 69.45% examples, 1010749 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:07,503 : INFO : EPOCH 13 - PROGRESS: at 92.94% examples, 1012898 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:07,786 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:26:07,786 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:26:07,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:26:07,786 : INFO : EPOCH - 13 : training on 5920713 raw words (4388547 effective words) took 4.3s, 1016238 effective words/s\n",
      "2018-11-05 16:26:08,803 : INFO : EPOCH 14 - PROGRESS: at 23.21% examples, 1024219 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:09,813 : INFO : EPOCH 14 - PROGRESS: at 47.31% examples, 1036275 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:10,819 : INFO : EPOCH 14 - PROGRESS: at 70.40% examples, 1029242 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:11,827 : INFO : EPOCH 14 - PROGRESS: at 93.67% examples, 1021575 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:12,068 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:26:12,078 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:26:12,078 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:26:12,078 : INFO : EPOCH - 14 : training on 5920713 raw words (4387943 effective words) took 4.3s, 1023662 effective words/s\n",
      "2018-11-05 16:26:13,075 : INFO : EPOCH 15 - PROGRESS: at 23.40% examples, 1037754 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:14,080 : INFO : EPOCH 15 - PROGRESS: at 47.00% examples, 1033982 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:15,081 : INFO : EPOCH 15 - PROGRESS: at 69.91% examples, 1026379 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:16,116 : INFO : EPOCH 15 - PROGRESS: at 93.14% examples, 1016418 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:16,482 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:26:16,492 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:26:16,492 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:26:16,492 : INFO : EPOCH - 15 : training on 5920713 raw words (4388459 effective words) took 4.4s, 995099 effective words/s\n",
      "2018-11-05 16:26:17,503 : INFO : EPOCH 16 - PROGRESS: at 23.82% examples, 1049730 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:18,508 : INFO : EPOCH 16 - PROGRESS: at 47.15% examples, 1037614 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:19,515 : INFO : EPOCH 16 - PROGRESS: at 70.58% examples, 1035391 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:20,521 : INFO : EPOCH 16 - PROGRESS: at 94.82% examples, 1037770 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:20,741 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:26:20,741 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:26:20,751 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:26:20,751 : INFO : EPOCH - 16 : training on 5920713 raw words (4388752 effective words) took 4.3s, 1032641 effective words/s\n",
      "2018-11-05 16:26:21,758 : INFO : EPOCH 17 - PROGRESS: at 23.21% examples, 1032734 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:22,754 : INFO : EPOCH 17 - PROGRESS: at 46.50% examples, 1025104 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:23,762 : INFO : EPOCH 17 - PROGRESS: at 70.40% examples, 1034135 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:24,782 : INFO : EPOCH 17 - PROGRESS: at 94.14% examples, 1028480 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:25,005 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:26:25,005 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:26:25,015 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:26:25,015 : INFO : EPOCH - 17 : training on 5920713 raw words (4388023 effective words) took 4.3s, 1030760 effective words/s\n",
      "2018-11-05 16:26:26,025 : INFO : EPOCH 18 - PROGRESS: at 23.02% examples, 1011963 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:27,032 : INFO : EPOCH 18 - PROGRESS: at 45.97% examples, 1007995 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-05 16:26:28,029 : INFO : EPOCH 18 - PROGRESS: at 68.96% examples, 1007053 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:29,036 : INFO : EPOCH 18 - PROGRESS: at 92.06% examples, 1003520 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:29,364 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:26:29,364 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:26:29,364 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:26:29,364 : INFO : EPOCH - 18 : training on 5920713 raw words (4386755 effective words) took 4.3s, 1009325 effective words/s\n",
      "2018-11-05 16:26:30,374 : INFO : EPOCH 19 - PROGRESS: at 22.54% examples, 994728 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:31,385 : INFO : EPOCH 19 - PROGRESS: at 46.50% examples, 1020241 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:32,384 : INFO : EPOCH 19 - PROGRESS: at 69.73% examples, 1021483 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:33,380 : INFO : EPOCH 19 - PROGRESS: at 93.14% examples, 1019407 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:33,664 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:26:33,664 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:26:33,674 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:26:33,674 : INFO : EPOCH - 19 : training on 5920713 raw words (4389186 effective words) took 4.3s, 1019825 effective words/s\n",
      "2018-11-05 16:26:34,677 : INFO : EPOCH 20 - PROGRESS: at 22.71% examples, 1009446 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:35,681 : INFO : EPOCH 20 - PROGRESS: at 45.97% examples, 1012841 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:36,678 : INFO : EPOCH 20 - PROGRESS: at 68.47% examples, 1004777 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:37,694 : INFO : EPOCH 20 - PROGRESS: at 92.06% examples, 1008395 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:38,041 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:26:38,041 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:26:38,042 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:26:38,042 : INFO : EPOCH - 20 : training on 5920713 raw words (4388787 effective words) took 4.4s, 1003087 effective words/s\n",
      "2018-11-05 16:26:39,042 : INFO : EPOCH 21 - PROGRESS: at 23.78% examples, 1054393 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:40,044 : INFO : EPOCH 21 - PROGRESS: at 46.82% examples, 1032865 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:41,051 : INFO : EPOCH 21 - PROGRESS: at 70.25% examples, 1030767 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:42,055 : INFO : EPOCH 21 - PROGRESS: at 94.16% examples, 1031026 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:42,300 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:26:42,300 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:26:42,310 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:26:42,310 : INFO : EPOCH - 21 : training on 5920713 raw words (4388794 effective words) took 4.3s, 1030986 effective words/s\n",
      "2018-11-05 16:26:43,323 : INFO : EPOCH 22 - PROGRESS: at 23.02% examples, 1017315 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:44,326 : INFO : EPOCH 22 - PROGRESS: at 46.50% examples, 1019647 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:45,339 : INFO : EPOCH 22 - PROGRESS: at 70.40% examples, 1029534 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:46,348 : INFO : EPOCH 22 - PROGRESS: at 93.68% examples, 1019696 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:46,606 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:26:46,606 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:26:46,606 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:26:46,606 : INFO : EPOCH - 22 : training on 5920713 raw words (4389060 effective words) took 4.3s, 1022283 effective words/s\n",
      "2018-11-05 16:26:47,627 : INFO : EPOCH 23 - PROGRESS: at 22.54% examples, 991917 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:48,618 : INFO : EPOCH 23 - PROGRESS: at 44.16% examples, 966853 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:49,622 : INFO : EPOCH 23 - PROGRESS: at 66.01% examples, 963837 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:50,626 : INFO : EPOCH 23 - PROGRESS: at 81.47% examples, 889889 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:51,517 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:26:51,528 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:26:51,529 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:26:51,530 : INFO : EPOCH - 23 : training on 5920713 raw words (4389226 effective words) took 4.9s, 890324 effective words/s\n",
      "2018-11-05 16:26:52,536 : INFO : EPOCH 24 - PROGRESS: at 15.52% examples, 690578 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:53,547 : INFO : EPOCH 24 - PROGRESS: at 29.06% examples, 640793 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:54,556 : INFO : EPOCH 24 - PROGRESS: at 41.66% examples, 609796 words/s, in_qsize 6, out_qsize 1\n",
      "2018-11-05 16:26:55,556 : INFO : EPOCH 24 - PROGRESS: at 53.74% examples, 591827 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:56,563 : INFO : EPOCH 24 - PROGRESS: at 71.29% examples, 625411 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:57,564 : INFO : EPOCH 24 - PROGRESS: at 92.23% examples, 672689 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:26:57,956 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:26:57,961 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:26:57,962 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:26:57,963 : INFO : EPOCH - 24 : training on 5920713 raw words (4387728 effective words) took 6.4s, 682620 effective words/s\n",
      "2018-11-05 16:26:58,969 : INFO : EPOCH 25 - PROGRESS: at 21.51% examples, 959448 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:26:59,981 : INFO : EPOCH 25 - PROGRESS: at 43.98% examples, 966005 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:00,987 : INFO : EPOCH 25 - PROGRESS: at 66.82% examples, 976772 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:01,993 : INFO : EPOCH 25 - PROGRESS: at 89.13% examples, 975184 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:02,449 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:27:02,455 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:27:02,460 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:27:02,461 : INFO : EPOCH - 25 : training on 5920713 raw words (4388448 effective words) took 4.5s, 976689 effective words/s\n",
      "2018-11-05 16:27:03,470 : INFO : EPOCH 26 - PROGRESS: at 22.54% examples, 999637 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:04,478 : INFO : EPOCH 26 - PROGRESS: at 42.18% examples, 925973 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:05,481 : INFO : EPOCH 26 - PROGRESS: at 63.16% examples, 924502 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:06,483 : INFO : EPOCH 26 - PROGRESS: at 82.98% examples, 909723 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:07,196 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:27:07,197 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:27:07,206 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:27:07,206 : INFO : EPOCH - 26 : training on 5920713 raw words (4388633 effective words) took 4.7s, 925542 effective words/s\n",
      "2018-11-05 16:27:08,216 : INFO : EPOCH 27 - PROGRESS: at 22.71% examples, 1006219 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:09,218 : INFO : EPOCH 27 - PROGRESS: at 45.31% examples, 998091 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:10,235 : INFO : EPOCH 27 - PROGRESS: at 66.81% examples, 975095 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:11,243 : INFO : EPOCH 27 - PROGRESS: at 88.56% examples, 968221 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-05 16:27:11,702 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:27:11,705 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:27:11,707 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:27:11,707 : INFO : EPOCH - 27 : training on 5920713 raw words (4388450 effective words) took 4.5s, 975936 effective words/s\n",
      "2018-11-05 16:27:12,725 : INFO : EPOCH 28 - PROGRESS: at 21.95% examples, 969438 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:13,741 : INFO : EPOCH 28 - PROGRESS: at 42.62% examples, 929598 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:14,748 : INFO : EPOCH 28 - PROGRESS: at 65.84% examples, 956873 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:15,749 : INFO : EPOCH 28 - PROGRESS: at 88.18% examples, 963058 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:16,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:27:16,252 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:27:16,253 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:27:16,253 : INFO : EPOCH - 28 : training on 5920713 raw words (4388529 effective words) took 4.5s, 966204 effective words/s\n",
      "2018-11-05 16:27:17,271 : INFO : EPOCH 29 - PROGRESS: at 15.83% examples, 697629 words/s, in_qsize 4, out_qsize 1\n",
      "2018-11-05 16:27:18,289 : INFO : EPOCH 29 - PROGRESS: at 37.42% examples, 821963 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:19,294 : INFO : EPOCH 29 - PROGRESS: at 60.43% examples, 884072 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:20,309 : INFO : EPOCH 29 - PROGRESS: at 83.46% examples, 910830 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:21,062 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:27:21,065 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:27:21,069 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:27:21,070 : INFO : EPOCH - 29 : training on 5920713 raw words (4389470 effective words) took 4.8s, 912324 effective words/s\n",
      "2018-11-05 16:27:22,081 : INFO : EPOCH 30 - PROGRESS: at 21.66% examples, 960817 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:23,093 : INFO : EPOCH 30 - PROGRESS: at 44.16% examples, 966575 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:24,108 : INFO : EPOCH 30 - PROGRESS: at 65.54% examples, 953032 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:25,119 : INFO : EPOCH 30 - PROGRESS: at 86.56% examples, 943474 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:25,742 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:27:25,746 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:27:25,755 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:27:25,756 : INFO : EPOCH - 30 : training on 5920713 raw words (4388093 effective words) took 4.7s, 937185 effective words/s\n",
      "2018-11-05 16:27:26,764 : INFO : EPOCH 31 - PROGRESS: at 21.18% examples, 941370 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:27,769 : INFO : EPOCH 31 - PROGRESS: at 44.32% examples, 974765 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:28,777 : INFO : EPOCH 31 - PROGRESS: at 65.54% examples, 958024 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:29,781 : INFO : EPOCH 31 - PROGRESS: at 85.62% examples, 937936 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:30,392 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:27:30,397 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:27:30,404 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:27:30,406 : INFO : EPOCH - 31 : training on 5920713 raw words (4388033 effective words) took 4.6s, 944391 effective words/s\n",
      "2018-11-05 16:27:31,413 : INFO : EPOCH 32 - PROGRESS: at 21.80% examples, 971838 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:32,413 : INFO : EPOCH 32 - PROGRESS: at 44.88% examples, 988969 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:33,414 : INFO : EPOCH 32 - PROGRESS: at 67.33% examples, 989062 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:34,418 : INFO : EPOCH 32 - PROGRESS: at 89.13% examples, 979431 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:34,915 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:27:34,917 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:27:34,922 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:27:34,922 : INFO : EPOCH - 32 : training on 5920713 raw words (4388399 effective words) took 4.5s, 972454 effective words/s\n",
      "2018-11-05 16:27:35,928 : INFO : EPOCH 33 - PROGRESS: at 22.71% examples, 1010015 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:36,932 : INFO : EPOCH 33 - PROGRESS: at 45.49% examples, 1002802 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:37,944 : INFO : EPOCH 33 - PROGRESS: at 68.01% examples, 994684 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:38,951 : INFO : EPOCH 33 - PROGRESS: at 91.18% examples, 997296 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:39,334 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:27:39,338 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:27:39,346 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:27:39,346 : INFO : EPOCH - 33 : training on 5920713 raw words (4388985 effective words) took 4.4s, 993040 effective words/s\n",
      "2018-11-05 16:27:40,356 : INFO : EPOCH 34 - PROGRESS: at 23.02% examples, 1020810 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:41,358 : INFO : EPOCH 34 - PROGRESS: at 45.66% examples, 1005273 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:42,360 : INFO : EPOCH 34 - PROGRESS: at 68.17% examples, 999703 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:43,372 : INFO : EPOCH 34 - PROGRESS: at 91.53% examples, 1001722 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:43,733 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:27:43,740 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:27:43,745 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:27:43,745 : INFO : EPOCH - 34 : training on 5920713 raw words (4388955 effective words) took 4.4s, 998584 effective words/s\n",
      "2018-11-05 16:27:44,752 : INFO : EPOCH 35 - PROGRESS: at 22.90% examples, 1017192 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:45,759 : INFO : EPOCH 35 - PROGRESS: at 42.18% examples, 928298 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:46,759 : INFO : EPOCH 35 - PROGRESS: at 62.78% examples, 921918 words/s, in_qsize 4, out_qsize 0\n",
      "2018-11-05 16:27:47,764 : INFO : EPOCH 35 - PROGRESS: at 85.45% examples, 937949 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:48,384 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:27:48,392 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:27:48,397 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:27:48,398 : INFO : EPOCH - 35 : training on 5920713 raw words (4387585 effective words) took 4.6s, 944144 effective words/s\n",
      "2018-11-05 16:27:49,405 : INFO : EPOCH 36 - PROGRESS: at 22.50% examples, 1002060 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:50,418 : INFO : EPOCH 36 - PROGRESS: at 45.31% examples, 993681 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:51,422 : INFO : EPOCH 36 - PROGRESS: at 68.01% examples, 993771 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:52,433 : INFO : EPOCH 36 - PROGRESS: at 90.50% examples, 988396 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:52,819 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:27:52,820 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:27:52,825 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:27:52,826 : INFO : EPOCH - 36 : training on 5920713 raw words (4387671 effective words) took 4.4s, 991956 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-05 16:27:53,834 : INFO : EPOCH 37 - PROGRESS: at 22.33% examples, 993358 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:54,844 : INFO : EPOCH 37 - PROGRESS: at 44.68% examples, 980057 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:55,846 : INFO : EPOCH 37 - PROGRESS: at 67.68% examples, 990074 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:27:56,850 : INFO : EPOCH 37 - PROGRESS: at 90.16% examples, 987406 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:57,250 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:27:57,257 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:27:57,261 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:27:57,261 : INFO : EPOCH - 37 : training on 5920713 raw words (4388840 effective words) took 4.4s, 990454 effective words/s\n",
      "2018-11-05 16:27:58,276 : INFO : EPOCH 38 - PROGRESS: at 22.50% examples, 993752 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:27:59,279 : INFO : EPOCH 38 - PROGRESS: at 45.66% examples, 1002345 words/s, in_qsize 4, out_qsize 1\n",
      "2018-11-05 16:28:00,283 : INFO : EPOCH 38 - PROGRESS: at 68.01% examples, 994424 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:28:01,284 : INFO : EPOCH 38 - PROGRESS: at 90.16% examples, 987592 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:28:01,685 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:28:01,692 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:28:01,696 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:28:01,696 : INFO : EPOCH - 38 : training on 5920713 raw words (4387349 effective words) took 4.4s, 990206 effective words/s\n",
      "2018-11-05 16:28:02,703 : INFO : EPOCH 39 - PROGRESS: at 22.03% examples, 980040 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:28:03,705 : INFO : EPOCH 39 - PROGRESS: at 45.04% examples, 991775 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:28:04,705 : INFO : EPOCH 39 - PROGRESS: at 67.16% examples, 985979 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:28:05,724 : INFO : EPOCH 39 - PROGRESS: at 88.37% examples, 968090 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:28:06,282 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:28:06,285 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:28:06,290 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:28:06,291 : INFO : EPOCH - 39 : training on 5920713 raw words (4386755 effective words) took 4.6s, 955736 effective words/s\n",
      "2018-11-05 16:28:07,303 : INFO : EPOCH 40 - PROGRESS: at 21.51% examples, 953925 words/s, in_qsize 3, out_qsize 1\n",
      "2018-11-05 16:28:08,325 : INFO : EPOCH 40 - PROGRESS: at 38.26% examples, 835843 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-05 16:28:09,348 : INFO : EPOCH 40 - PROGRESS: at 50.73% examples, 734896 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:28:10,353 : INFO : EPOCH 40 - PROGRESS: at 71.29% examples, 775241 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:28:11,355 : INFO : EPOCH 40 - PROGRESS: at 92.41% examples, 803230 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-05 16:28:11,655 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-05 16:28:11,664 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-05 16:28:11,668 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-05 16:28:11,669 : INFO : EPOCH - 40 : training on 5920713 raw words (4388589 effective words) took 5.4s, 816750 effective words/s\n",
      "2018-11-05 16:28:11,669 : INFO : training on a 236828520 raw words (175534189 effective words) took 180.9s, 970512 effective words/s\n"
     ]
    }
   ],
   "source": [
    "doc2vecModel = Doc2Vec(vector_size = 100, min_count = 5, epochs = 40)\n",
    "doc2vecModel.build_vocab(taggedDocs)\n",
    "doc2vecModel.train(taggedDocs, total_examples = doc2vecModel.corpus_count, epochs = doc2vecModel.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:28:17.030301Z",
     "start_time": "2018-11-05T23:28:11.675301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LR ....\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>StdDev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.85208</td>\n",
       "      <td>0.009182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  Accuracy    StdDev\n",
       "0    LR   0.85208  0.009182"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init vars\n",
    "folds = 10\n",
    "seed = 10\n",
    "models = []\n",
    "results = {}\n",
    "\n",
    "# Use accuracy since this is a classification\n",
    "score = 'accuracy'\n",
    "\n",
    "# Assign training features and labels\n",
    "xTrain = doc2vecModel.docvecs\n",
    "yTrain = df.iloc[:, 1]\n",
    "\n",
    "# Instantiate model objects\n",
    "models.append(('LR', LogisticRegression()))\n",
    "#models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "#models.append(('KNN', KNeighborsClassifier()))\n",
    "#models.append(('CART', DecisionTreeClassifier()))\n",
    "#models.append(('NB', GaussianNB()))\n",
    "#models.append(('SVM', SVC()))\n",
    "\n",
    "# Create a Pandas DF to hold all our spiffy results\n",
    "_df = pd.DataFrame(columns = ['Model', 'Accuracy', 'StdDev'])\n",
    "\n",
    "# Run the models\n",
    "for modelName, model in models:\n",
    "    print(\"Training\", modelName, \"....\")\n",
    "    # Implement K-fold cross validation where K = 10\n",
    "    kFold = KFold(n_splits = folds, random_state = seed)\n",
    "    results[modelName] = cross_val_score(model, xTrain, yTrain, cv = kFold, scoring = score)\n",
    "    _df.loc[len(_df)] = list([modelName, results[modelName].mean(), results[modelName].std()])\n",
    "\n",
    "# Print results sorted by Mean desc, StdDev asc, Model asc\n",
    "_df.sort_values(by = ['Accuracy', 'StdDev', 'Model'], ascending = [False, True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combing models\n",
    "\n",
    "https://markroxor.github.io/gensim/static/notebooks/doc2vec-IMDB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T23:16:47.079482Z",
     "start_time": "2018-11-05T23:16:46.609444Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-e2c69c190e25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m simple_models = [\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdm_concat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# PV-DBOW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cores' is not defined"
     ]
    }
   ],
   "source": [
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T22:53:37.685121Z",
     "start_time": "2018-11-05T22:53:37.194129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this write-up we accomplished the following:\n",
    "\n",
    "1. Created a set of word embeddings from the IMDb movie review text utilizing Word2vec\n",
    "2. Clustered the embeddings utilizing a K-nearest neighbors algorithm into a set of centroids\n",
    "3. Trained and evaluated the models from the last write-up against the centroid feature set\n",
    "\n",
    "And finally, here is the baseline model's performance vs. the 'centroid' model we developed in this write-up:\n",
    "\n",
    "|Model|Accuracy|Best Params                           |\n",
    "|-------------------|--------|-----------------------------------|\n",
    "|LR (baseline)      |86.35%  |{'LR__C': 0.1, 'LR__penalty': 'l1'}|\n",
    "|Kaggle centroid    |84.68%  |Estimators = 100                   |\n",
    "|SVM centroid       |86.36%  |Scikit-learn defaults              |\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "Similar to the last [last write-up](./Model-06.p2.ipynb) the work in this notebook was an interesting idea to explore, but ultimately didn't result in an overall performance increase versus the baseline model.  As such this line of exploration will be rejected in favor of keeping the current base line model and accuracy rating as benchmarks moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
