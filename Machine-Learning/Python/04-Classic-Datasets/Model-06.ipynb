{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#IMDB-Movie-Review-Sentiment-Classification\" data-toc-modified-id=\"IMDB-Movie-Review-Sentiment-Classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>IMDB Movie Review Sentiment Classification</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Process\" data-toc-modified-id=\"Process-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Process</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries\" data-toc-modified-id=\"Import-libraries-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Import libraries</a></span></li></ul></li><li><span><a href=\"#Examine-data\" data-toc-modified-id=\"Examine-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Examine data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Descriptive-statistics\" data-toc-modified-id=\"Descriptive-statistics-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Descriptive statistics</a></span></li></ul></li><li><span><a href=\"#Baseline-model-development\" data-toc-modified-id=\"Baseline-model-development-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Baseline model development</a></span></li><li><span><a href=\"#Init-vars\" data-toc-modified-id=\"Init-vars-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Init vars</a></span></li><li><span><a href=\"#Build-the-computational-graph\" data-toc-modified-id=\"Build-the-computational-graph-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Build the computational graph</a></span><ul class=\"toc-item\"><li><span><a href=\"#Static-vs-Dynamic-TensorFlow-RNNs\" data-toc-modified-id=\"Static-vs-Dynamic-TensorFlow-RNNs-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Static vs Dynamic TensorFlow RNNs</a></span></li><li><span><a href=\"#LSTM-v1\" data-toc-modified-id=\"LSTM-v1-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>LSTM v1</a></span></li><li><span><a href=\"#LSTM-v2\" data-toc-modified-id=\"LSTM-v2-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>LSTM v2</a></span></li><li><span><a href=\"#LSTM-v3\" data-toc-modified-id=\"LSTM-v3-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>LSTM v3</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>IMDB Movie Review Sentiment Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px; width: 30%; height: 30%;\" src=\"images/imdb.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The purpose of this write-up is create a predictive classification model utilizing natural language processing (NLP) to process IMDB movie review sentiments.  The write-up is inspired by the Kaggle (\n",
    "Bag of Words Meets Bags of Popcorn)[https://www.kaggle.com/c/word2vec-nlp-tutorial] competition.    \n",
    "\n",
    "Goals include:\n",
    "* TODO\n",
    "* TODO\n",
    "* TODO\n",
    "\n",
    "Dataset source:  [IMDB Movie Reviews](https://www.kaggle.com/c/word2vec-nlp-tutorial/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "\n",
    "We'll utilize the following process to guide us through this and the following write-ups on the IDMB movie review dataset:\n",
    "\n",
    "1. Problem definition\n",
    "2. Evaluation Strategy\n",
    "3. Baseline model(s)\n",
    "4. Data validation\n",
    "5. Model development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure notebook and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:08:44.796666Z",
     "start_time": "2018-09-18T21:08:44.793666Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import set_option\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# http://www.nltk.org/index.html\n",
    "# pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "# pip install BeautifulSoup4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataPath = os.path.join('.', 'datasets', 'imdb_movie_reviews')\n",
    "labeledTrainData = os.path.join(dataPath, 'labeledTrainData.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine data\n",
    "\n",
    "If we open the training data file in a text editor we can see that:\n",
    "* A header row exists with the values 'id\tsentiment\treview'\n",
    "* The values appear to be separated by tabs\n",
    "* There are double quotes around the review text as well as within the contents of the review text\n",
    "\n",
    "Based on the last point we'll tell Pandas to avoid quoting with the parameter `quoting = 3`.\n",
    "\n",
    "Let's go ahead and read the test data file into a Pandas DataFrame and then explore the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(labeledTrainData, sep = '\\t', header = 0, quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shape and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           object\n",
       "sentiment     int64\n",
       "review       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Inspect a few rows of raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"319_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"A friend of mine bought this film for £1, and even then it was grossly overpriced. Despite featuring big names such as Adam Sandler, Billy Bob Thornton and the incredibly talented Burt Young, this film was about as funny as taking a chisel and hammering it straight through your earhole. It uses tired, bottom of the barrel comedic techniques - consistently breaking the fourth wall as Sandler talks to the audience, and seemingly pointless montages of 'hot girls'.&lt;br /&gt;&lt;br /&gt;Adam Sandler plays a waiter on a cruise ship who wants to make it as a successful comedian in order to become successful with women. When the ship's resident comedian - the shamelessly named 'Dickie' due to his unfathomable success with the opposite gender - is presumed lost at sea, Sandler's character Shecker gets his big break. Dickie is not dead, he's rather locked in the bathroom, presumably sea sick.&lt;br /&gt;&lt;br /&gt;Perhaps from his mouth he just vomited the worst film of all time.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"8713_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"&lt;br /&gt;&lt;br /&gt;This movie is full of references. Like \\\"Mad Max II\\\", \\\"The wild one\\\" and many others. The ladybug´s face it´s a clear reference (or tribute) to Peter Lorre. This movie is a masterpiece. We´ll talk much more about in the future.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"2486_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What happens when an army of wetbacks, towelheads, and Godless Eastern European commies gather their forces south of the border? Gary Busey kicks their butts, of course. Another laughable example of Reagan-era cultural fallout, Bulletproof wastes a decent supporting cast headed by L Q Jones and Thalmus Rasulala.\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment  \\\n",
       "8   \"319_1\"    0           \n",
       "9   \"8713_10\"  1           \n",
       "10  \"2486_3\"   0           \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   review  \n",
       "8   \"A friend of mine bought this film for £1, and even then it was grossly overpriced. Despite featuring big names such as Adam Sandler, Billy Bob Thornton and the incredibly talented Burt Young, this film was about as funny as taking a chisel and hammering it straight through your earhole. It uses tired, bottom of the barrel comedic techniques - consistently breaking the fourth wall as Sandler talks to the audience, and seemingly pointless montages of 'hot girls'.<br /><br />Adam Sandler plays a waiter on a cruise ship who wants to make it as a successful comedian in order to become successful with women. When the ship's resident comedian - the shamelessly named 'Dickie' due to his unfathomable success with the opposite gender - is presumed lost at sea, Sandler's character Shecker gets his big break. Dickie is not dead, he's rather locked in the bathroom, presumably sea sick.<br /><br />Perhaps from his mouth he just vomited the worst film of all time.\"  \n",
       "9   \"<br /><br />This movie is full of references. Like \\\"Mad Max II\\\", \\\"The wild one\\\" and many others. The ladybug´s face it´s a clear reference (or tribute) to Peter Lorre. This movie is a masterpiece. We´ll talk much more about in the future.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "10  \"What happens when an army of wetbacks, towelheads, and Godless Eastern European commies gather their forces south of the border? Gary Busey kicks their butts, of course. Another laughable example of Reagan-era cultural fallout, Bulletproof wastes a decent supporting cast headed by L Q Jones and Thalmus Rasulala.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't truncate\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df[8:11].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears there is a lot of noise in the `review` column we are going to have to deal with:  punctuation, html, escaped double quotes, currency symbols, and so forth.  \n",
    "\n",
    "Two of the reviews seem to have a clear sentiment, which will hopefully allow the model to train and learn well against:\n",
    "* Row 8 :: \"This movie is a masterpiece.\" --> Clearly positive\n",
    "* Row 10 :: \"... the worst film of all time.\" --> Clearly negative\n",
    "\n",
    "And then we have Row[10] which even as a human I wouldn't be 100% sure if they were being negative and/or sarcastic but in a positive or snarky way.  I would assume this type of review is going to give our learning algorithm some issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    12500\n",
       "1    12500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('sentiment').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an even split of likes and dislikes; no one classification has a skewed representation in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ID distribution\n",
    "\n",
    "Kaggle's site has this to say about the ID column:\n",
    "* id - Unique ID of each review\n",
    "\n",
    "It isn't clear; however, if each review is from a unique author, or we have potentially multiple reviews written by the same person.\n",
    "\n",
    "It appears that perhaps the first part of the ID before the underscore might identify the author, and the second part of the ID after the underscore might be the Nth review from that author.\n",
    "\n",
    "We can explore this theory using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for dupes against the raw ID values\n",
    "df['id'].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the ID on the underscore\n",
    "split = df['id'].str.replace('\"', '').str.split('_')\n",
    "split.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      " (12500,) \n",
      "\n",
      "First five:\n",
      " 3981     2\n",
      "7046     2\n",
      "8771     2\n",
      "10085    2\n",
      "10021    2\n",
      "dtype: int64 \n",
      "\n",
      "Last five:\n",
      " 6437     2\n",
      "2014     2\n",
      "12019    2\n",
      "10907    2\n",
      "9770     2\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pull out the first part of the ID values using a list comprehension, and place results into a Pandas Series object\n",
    "ids = pd.Series([x[0] for x in split])\n",
    "\n",
    "# Let's see if the number of records has changed\n",
    "print(\"Shape:\\n\", ids.value_counts(ascending = False).shape, \"\\n\")\n",
    "print(\"First five:\\n\", ids.value_counts(ascending = False).head(5), \"\\n\")\n",
    "print(\"Last five:\\n\", ids.value_counts(ascending = False).tail(5), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our theory is correct--which it may not be--then each review author has exactly two entries present in the training observations.  This still provides us with a wide range (12500 in fact) of writing styles, word compositions, and so forth.  It also mitigates the possibility that we might have a few authors with a large number of reviews that would skew the algorithm's ability to generalize to unseen observations.\n",
    "\n",
    "If our theory is incorrect then we simply have 25,000 unique reviews each written by a different author, and this can only help the model to generalize.\n",
    "\n",
    "Just for fun; however, let's pick out two reviews by the same author, and see if the writing styles are similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10936</th>\n",
       "      <td>\"12486_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Rich ditzy Joan Winfield (a woefully miscast Bette Davis) is engaged to be married to stupid egotistical Allen Brice (Jack Carson looking lost). Her father (Eugene Palette) is determined to stop the marriage and has her kidnapped by pilot Steve Collins (James Cagney. Seriously). They crash land in the desert and hate each other but (sigh) start falling in love.&lt;br /&gt;&lt;br /&gt;This seems to be getting a high rating from reviewers here only because Cagney and Davis are in it. They were both brilliant actors but they were known for dramas NOT comedy and this movie shows why! The script is just horrible--there's not one genuine laugh in the entire movie. The running joke in this has Cagney and Davis falling rump first in a cactus (this is done THREE TIMES!). Only their considerable talents save them from being completely humiliated. As it is they both do their best with the lousy material. Cagney tries his best with his lines and Davis screeches every line full force but it doesn't work. Carson has this \\\"what the hell\\\" look on his face throughout the entire movie (probably because his characters emotions change in seconds). Only Palette with his distinctive voice and over the top readings manges to elicit a few smiles. But, all in all, this was dull and laughless--a real chore to sit through. This gets two stars only for Cagney and Davis' acting and some beautiful cinematography but really--it's not worth seeing. Cagney and Davis hated this film in later years and you can see why.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11427</th>\n",
       "      <td>\"12486_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Well, this film is a difficult one really. To be straight with you, this film doesn't contain much of a riveting story, nore does it make u 'want' to know how it'll end...but I'll tell you something now...never have I been as tense and jumped up before in my life! This film sure does deliver the jumps and thrills! To be fair, I did watch it at almost midnight so I was kinda sleepy anyway, so maybe that explains why I was jumpy...or maybe it's because this film does deliver in that aspect! It's basically about a couple who lose their child in a tragic event. They decide to move away and rent a cabin looking thing in the mountains...all looks peaceful and calm until they have their first visitors (i think it's it's the sister of the main character, and she brings along her husband)...during the night, the husband hears noises...checks it out, and thats when things start to go really really wrong...they don't stay for another day and tell the couple they should leave asap as something isn't right...to cut a long story short...eventually they find out what has happened in that house in the past few years and decide it needs to be taken care of.&lt;br /&gt;&lt;br /&gt;It's not a Hollywood blockbuster, nore does it have a huge budget, but please don't let that put you off. It's creepy, tense and very very jumpy! Just give it a try :)\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  sentiment  \\\n",
       "10936  \"12486_2\"  0           \n",
       "11427  \"12486_7\"  1           \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              review  \n",
       "10936  \"Rich ditzy Joan Winfield (a woefully miscast Bette Davis) is engaged to be married to stupid egotistical Allen Brice (Jack Carson looking lost). Her father (Eugene Palette) is determined to stop the marriage and has her kidnapped by pilot Steve Collins (James Cagney. Seriously). They crash land in the desert and hate each other but (sigh) start falling in love.<br /><br />This seems to be getting a high rating from reviewers here only because Cagney and Davis are in it. They were both brilliant actors but they were known for dramas NOT comedy and this movie shows why! The script is just horrible--there's not one genuine laugh in the entire movie. The running joke in this has Cagney and Davis falling rump first in a cactus (this is done THREE TIMES!). Only their considerable talents save them from being completely humiliated. As it is they both do their best with the lousy material. Cagney tries his best with his lines and Davis screeches every line full force but it doesn't work. Carson has this \\\"what the hell\\\" look on his face throughout the entire movie (probably because his characters emotions change in seconds). Only Palette with his distinctive voice and over the top readings manges to elicit a few smiles. But, all in all, this was dull and laughless--a real chore to sit through. This gets two stars only for Cagney and Davis' acting and some beautiful cinematography but really--it's not worth seeing. Cagney and Davis hated this film in later years and you can see why.\"  \n",
       "11427  \"Well, this film is a difficult one really. To be straight with you, this film doesn't contain much of a riveting story, nore does it make u 'want' to know how it'll end...but I'll tell you something now...never have I been as tense and jumped up before in my life! This film sure does deliver the jumps and thrills! To be fair, I did watch it at almost midnight so I was kinda sleepy anyway, so maybe that explains why I was jumpy...or maybe it's because this film does deliver in that aspect! It's basically about a couple who lose their child in a tragic event. They decide to move away and rent a cabin looking thing in the mountains...all looks peaceful and calm until they have their first visitors (i think it's it's the sister of the main character, and she brings along her husband)...during the night, the husband hears noises...checks it out, and thats when things start to go really really wrong...they don't stay for another day and tell the couple they should leave asap as something isn't right...to cut a long story short...eventually they find out what has happened in that house in the past few years and decide it needs to be taken care of.<br /><br />It's not a Hollywood blockbuster, nore does it have a huge budget, but please don't let that put you off. It's creepy, tense and very very jumpy! Just give it a try :)\"                                                                                                                                                                    "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = df[ df['id'].str.contains('12486_') ]\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the writing styles present in the two samples above the theory that these were written by the same author seems to be weakened.  For example, the second sample utilizes '...' a number of times, but we don't see that present in the first sample.  Likewise the first sample uses all uppercase characters for emphasis, but there are none present in the second sample.  And finally, the use (or misuse) of grammar does not match between the two entries either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           object\n",
       "sentiment     int64\n",
       "review       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'stats'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c8e180dbbaf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3612\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3613\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3614\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3616\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'stats'"
     ]
    }
   ],
   "source": [
    "df.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of               id  sentiment                                             review\n",
       "0       \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1       \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2       \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3       \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4       \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...\n",
       "5       \"8196_8\"          1  \"I dont know why people think this is such a b...\n",
       "6       \"7166_2\"          0  \"This movie could have been very good, but com...\n",
       "7      \"10633_1\"          0  \"I watched this video at a friend's house. I'm...\n",
       "8        \"319_1\"          0  \"A friend of mine bought this film for £1, and...\n",
       "9      \"8713_10\"          1  \"<br /><br />This movie is full of references....\n",
       "10      \"2486_3\"          0  \"What happens when an army of wetbacks, towelh...\n",
       "11     \"6811_10\"          1  \"Although I generally do not like remakes beli...\n",
       "12     \"11744_9\"          1  \"\\\"Mr. Harvey Lights a Candle\\\" is anchored by...\n",
       "13      \"7369_1\"          0  \"I had a feeling that after \\\"Submerged\\\", thi...\n",
       "14     \"12081_1\"          0  \"note to George Litman, and others: the Myster...\n",
       "15      \"3561_4\"          0  \"Stephen King adaptation (scripted by King him...\n",
       "16      \"4489_1\"          0  \"`The Matrix' was an exciting summer blockbust...\n",
       "17      \"3951_2\"          0  \"Ulli Lommel's 1980 film 'The Boogey Man' is n...\n",
       "18     \"3304_10\"          1  \"This movie is one among the very few Indian m...\n",
       "19     \"9352_10\"          1  \"Most people, especially young people, may not...\n",
       "20      \"3374_7\"          1  \"\\\"Soylent Green\\\" is one of the best and most...\n",
       "21     \"10782_7\"          1  \"Michael Stearns plays Mike, a sexually frustr...\n",
       "22     \"5414_10\"          1  \"This happy-go-luck 1939 military swashbuckler...\n",
       "23     \"10492_1\"          0  \"I would love to have that two hours of my lif...\n",
       "24      \"3350_3\"          0  \"The script for this movie was probably found ...\n",
       "25      \"6581_7\"          1  \"Looking for Quo Vadis at my local video store...\n",
       "26      \"2203_3\"          0  \"Note to all mad scientists everywhere: if you...\n",
       "27       \"689_1\"          0  \"What the ........... is this ? This must, wit...\n",
       "28      \"9152_1\"          0  \"Intrigued by the synopsis (every gay video th...\n",
       "29      \"6077_1\"          0  \"Would anyone really watch this RUBBISH if it ...\n",
       "...          ...        ...                                                ...\n",
       "24970   \"9389_7\"          1  \"Red Rock West (1993)<br /><br />Nicolas Cage ...\n",
       "24971   \"9251_9\"          1  \"what can i say?, ms Erika Eleniak is my favor...\n",
       "24972  \"1422_10\"          1  \"The spoiler warning is for those people who w...\n",
       "24973   \"7415_2\"          0  \"What do you call a horror story without horro...\n",
       "24974   \"7492_7\"          1  \"Though not a horror film in the traditional s...\n",
       "24975  \"7689_10\"          1  \"This was what black society was like before t...\n",
       "24976  \"12370_4\"          0  \"They probably should have called this movie T...\n",
       "24977   \"5625_8\"          1  \"Attractive Marjorie(Farrah Fawcett)lives in f...\n",
       "24978   \"9397_9\"          1  \"Vaguely reminiscent of great 1940's westerns,...\n",
       "24979   \"5992_7\"          1  \"I admit I had no idea what to expect before v...\n",
       "24980  \"2488_10\"          1  \"To me, the final scene, in which Harris respo...\n",
       "24981  \"9627_10\"          1  \"This is by far the funniest short made by the...\n",
       "24982   \"3822_2\"          0  \"To be a Buster Keaton fan is to have your hea...\n",
       "24983   \"5983_4\"          0  \"I was one of those \\\"few Americans\\\" that gre...\n",
       "24984   \"8021_2\"          0  \"Visually disjointed and full of itself, the d...\n",
       "24985   \"3471_3\"          0  \"this movie had more holes than a piece of swi...\n",
       "24986  \"6034_10\"          1  \"Last November, I had a chance to see this fil...\n",
       "24987   \"1988_9\"          1  \"First off, I'd like to make a correction on a...\n",
       "24988   \"7623_9\"          1  \"While originally reluctant to jump on the ban...\n",
       "24989   \"5974_7\"          1  \"I heard about this movie when watching VH1's ...\n",
       "24990   \"2034_9\"          1  \"I've never been huge on IMAX films. They're c...\n",
       "24991   \"9416_3\"          0  \"Steve McQueen has certainly a lot of loyal fa...\n",
       "24992  \"10994_1\"          0  \"Sometimes you wonder how some people get fund...\n",
       "24993  \"10957_3\"          0  \"I am a student of film, and have been for sev...\n",
       "24994   \"2372_1\"          0  \"Unimaginably stupid, redundant and humiliatin...\n",
       "24995   \"3453_3\"          0  \"It seems like more consideration has gone int...\n",
       "24996   \"5064_1\"          0  \"I don't believe they made this film. Complete...\n",
       "24997  \"10905_3\"          0  \"Guy is a loser. Can't get girls, needs to bui...\n",
       "24998  \"10194_3\"          0  \"This 30 minute documentary Buñuel made in the...\n",
       "24999   \"8478_8\"          1  \"I saw this movie as a child and it broke my h...\n",
       "\n",
       "[25000 rows x 3 columns]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'sentiment', 'review'], dtype='object')\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(labeledTrainData, sep = '\\t', header = 0, quoting = 3)\n",
    "\n",
    "print(df.columns)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next well create a number of baseline models in order to obtain a benchmark to compare further efforts against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init vars\n",
    "\n",
    "The LSTM wants inputs of shape `[samples, timeSteps, features]`, and we have several thousand MNIST images of size 28 x 28 pixels.  \n",
    "\n",
    "One way to think of this is a complete image is comprised of 28 rows of 28 pixels each.  If we were to step through the rows one by one and stack them up then the image would be more and more complete as time went by.  So our units of \"time\" will be the rows stacking together to create a complete image, and the number of features will be the number of pixels in the image row at that step in time (i.e. 28).  This gives us:\n",
    "\n",
    "* samples     = number of observations (i.e. number of images in the mini batch)\n",
    "* timeSteps   = number of rows we need to step through/stack up to make a complete image\n",
    "* features    = the number of features in each row we are stepping through (i.e. also 28)\n",
    "\n",
    "Additionally, we only care about the final output of the LSTM network which should give us the prediction of which numeral the image represents.  Other LSTM networks do care about the outputs of each LSTM cell (translating each word in a sentence for example), but that doesn't apply in our case.\n",
    "\n",
    "Having said this we can continue with initializing the various variables we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:10:06.564666Z",
     "start_time": "2018-09-18T21:10:06.098666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "\n",
      " Example label:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Setup vars for the MINST data set\n",
    "timeSteps = 28\n",
    "features = 28\n",
    "\n",
    "lstmUnits = 128\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "samples = 50\n",
    "\n",
    "classes = 10\n",
    "\n",
    "# Allow results to be reproduced\n",
    "seed = 10\n",
    "\n",
    "# Notice we are pulling in the labels as one hot encodings!\n",
    "mninst = input_data.read_data_sets(\"./datasets/mnist\", one_hot = True)\n",
    "\n",
    "# For use when we create the LSTM network below\n",
    "testShape = mninst.test.images.shape\n",
    "\n",
    "# Note the one hot encoding on the label:\n",
    "print(\"\\n\", \"Example label: \", mninst.test.labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-07T20:47:13.613722Z",
     "start_time": "2018-09-07T20:47:13.609737Z"
    }
   },
   "source": [
    "# Build the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static vs Dynamic TensorFlow RNNs\n",
    "\n",
    "> Dynamic RNN's allow for variable sequence lengths. You might have an input shape (batch_size, max_sequence_length), but this will allow you to run the RNN for the correct number of time steps on those sequences that are shorter than max_sequence_length.\n",
    " \n",
    "> In contrast, there are static RNNs, which expect to run the entire fixed RNN length. There are cases where you might prefer to do this, such as if you are padding your inputs to max_sequence_length anyway.\n",
    "\n",
    ">In short, dynamic_rnn is usually what you want for variable length sequential data. It has a sequence_length parameter, and it is your friend.  [Source](https://stackoverflow.com/questions/43100981/what-is-a-dynamic-rnn-in-tensorflow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM v1\n",
    "\n",
    "* Utilize f.contrib.rnn.BasicLSTMCell\n",
    "* Utilize tf.contrib.rnn.static_rnn\n",
    "* Manual weight and bias definitions with tf.random_normal for initialization\n",
    "* Track training and validiation loss and accuracy in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:15:16.094666Z",
     "start_time": "2018-09-18T21:15:14.061666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the seed\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/mnistLSTM/runOne/train'\n",
    "logDirValidation = './logs/mnistLSTM/runOne/validation'\n",
    "\n",
    "\n",
    "# Create place holders\n",
    "x = tf.placeholder(tf.float32, shape = [None, timeSteps, features], name = 'x')\n",
    "# Give 2nd dimension arg to shape since we are using one hot encodings\n",
    "y = tf.placeholder(tf.int64, shape = [None, classes], name = 'y')\n",
    "\n",
    "# Create weights and bias tensors\n",
    "with tf.name_scope(\"weightBias\"):\n",
    "    w = tf.Variable(tf.random_normal([lstmUnits, classes]))\n",
    "    b = tf.Variable(tf.random_normal([classes]))\n",
    "\n",
    "\n",
    "# Add the LSTM cells\n",
    "with tf.name_scope(\"LSTM\"):\n",
    "    \n",
    "    # Later in the code we'll make a call to tf.contrib.rnn.static_rnn\n",
    "    # tf.contrib.rnn.static_rnn expects a length T list of inputs, each a Tensor of shape [batch_size, input_size]\n",
    "    # So we need to convert our inputs of shape [batchSize, timeSteps, numberOfInputs] to [batch_size, input_size]\n",
    "    #\n",
    "    # https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/static_rnn\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/unstack\n",
    "    inputs = tf.unstack(x, num = timeSteps, axis = 1)\n",
    "    \n",
    "    # Create the basic LSTM cell\n",
    "    # It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline.\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "    \n",
    "    # Add the cell to the RNN\n",
    "    # https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/static_rnn\n",
    "    output, state = tf.contrib.rnn.static_rnn(cell, inputs, dtype = tf.float32)\n",
    "    \n",
    "    # We only care about the final output which should be the model's prediction\n",
    "    yH = tf.matmul(output[-1], w) + b\n",
    "    \n",
    "# Add loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # We don't use \"tf.nn.sparse_softmax_cross_entropy_with_logits\" here since we have one hot encodings\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits = yH, labels = y)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    # We don't use \"tf.nn.in_top_k(yH, y, 1)\" here since are aren't using \"tf.nn.sparse_softmax_cross_entropy_with_logits\"\n",
    "    correct = tf.equal(tf.argmax(yH, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:32:47.556966Z",
     "start_time": "2018-09-18T21:28:10.774141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.96 Validation Acc:  0.9686\n",
      "10 Train Acc:  1.0 Validation Acc:  0.9898\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Validation Acc:  0.9898 Test Acc:  0.9882\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    validationWriter = tf.summary.FileWriter(logDirValidation)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs + 1):\n",
    "        for i in range(mninst.train.num_examples // samples):      \n",
    "            counter += 1     \n",
    "            \n",
    "            # Grab the next minibatch\n",
    "            xBatch, yBatch = mninst.train.next_batch(samples)\n",
    "            \n",
    "            # Reshape x to [samples, timeSteps, features] for the LSTM:\n",
    "            #   The image is given to us as a single vector of dimensionality 784\n",
    "            #   So to use them we need to gather the number of rows together to be the timeSteps\n",
    "            xBatch = xBatch.reshape(samples, timeSteps, features)\n",
    "            \n",
    "            # Train the model\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 10 == 0:\n",
    "                # Manually add to the train accuracy summary value\n",
    "                summary, accTrain = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter) \n",
    "                \n",
    "                # Manually add to the test accuracy summary value\n",
    "                \n",
    "                # If test accuracy calcs are causing speed issues you can reduce the number tested via the following:\n",
    "                #summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                #    x: mninst.validation.images[:450].reshape(-1, timeSteps, features), \n",
    "                #    y: mninst.validation.labels[:450]})\n",
    "                summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.validation.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.validation.labels})\n",
    "                validationWriter.add_summary(summary, counter)\n",
    "                \n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation)\n",
    "        \n",
    "    print(\" \")\n",
    "    # Compute test set accuracy rating\n",
    "    summary, accTest = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.test.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.test.labels})\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px;\" src=\"images/mnist-run-one.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a little slower than some other models we've looked at the LSTM has exellent accuracy on this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM v2\n",
    "\n",
    "* Enclose the CG architecture in a graph object; pass to the training session\n",
    "* Utilize f.contrib.rnn.BasicLSTMCell\n",
    "* Utilize tf.contrib.rnn.dynamic_rnn, so we don't need to unstack the 'x' tensor\n",
    "* Remove manual weight and bias definitions and relace with a dense layer\n",
    "* Utilize He initialization\n",
    "* Track training and validiation loss and accuracy in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:15:16.094666Z",
     "start_time": "2018-09-18T21:15:14.061666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the seed\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/mnistLSTM/runTwo/train'\n",
    "logDirValidation = './logs/mnistLSTM/runTwo/validation'\n",
    "\n",
    "# Create the graph object and populate it\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Create place holders\n",
    "    x = tf.placeholder(tf.float32, shape = [None, timeSteps, features], name = 'x')\n",
    "    # Give 2nd dimension arg to shape since we are using one hot encodings\n",
    "    y = tf.placeholder(tf.int64, shape = [None, classes], name = 'y')\n",
    "\n",
    "    # Add the LSTM cells with He initialization (we'll let TF worry about the \"w\" and \"b\" values)\n",
    "    # Notice to do this we switch from \"tf.name_scope\" to \"tf.variable_scope\" and add the \"initializer\" param\n",
    "    with tf.variable_scope(\"LSTM\", initializer = tf.variance_scaling_initializer()):\n",
    "\n",
    "        # Create the basic LSTM cell\n",
    "        # It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "\n",
    "        # Add the cell to the RNN\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "        # Notice we don't have to unstack x as in the previous model\n",
    "        output, state = tf.nn.dynamic_rnn(cell, x, dtype = tf.float32)\n",
    "\n",
    "        # We only care about the final output which should be the model's prediction\n",
    "        yH = tf.layers.dense(state[-1], classes)\n",
    "\n",
    "    # Add loss function\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # We don't use \"tf.nn.sparse_softmax_cross_entropy_with_logits\" here since we have one hot encodings\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(logits = yH, labels = y)\n",
    "        loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "        # Capture loss\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "\n",
    "    # Eval the model's accuracy\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        # We don't use \"tf.nn.in_top_k(yH, y, 1)\" here since are aren't using \"tf.nn.sparse_softmax_cross_entropy_with_logits\"\n",
    "        correct = tf.equal(tf.argmax(yH, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        # Capture accuracy\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:32:47.556966Z",
     "start_time": "2018-09-18T21:28:10.774141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Validation Acc:  0.9606\n",
      "10 Train Acc:  1.0 Validation Acc:  0.9884\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Validation Acc:  0.9884 Test Acc:  0.9877\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    validationWriter = tf.summary.FileWriter(logDirValidation)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs + 1):\n",
    "        for i in range(mninst.train.num_examples // samples):      \n",
    "            counter += 1     \n",
    "            \n",
    "            # Grab the next minibatch\n",
    "            xBatch, yBatch = mninst.train.next_batch(samples)\n",
    "            \n",
    "            # Reshape x to [samples, timeSteps, features] for the LSTM:\n",
    "            #   The image is given to us as a single vector of dimensionality 784\n",
    "            #   So to use them we need to gather the number of rows together to be the timeSteps\n",
    "            xBatch = xBatch.reshape(samples, timeSteps, features)\n",
    "            \n",
    "            # Train the model\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 10 == 0:\n",
    "                # Manually add to the train accuracy summary value\n",
    "                summary, accTrain = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter) \n",
    "                \n",
    "                # Manually add to the test accuracy summary value\n",
    "                \n",
    "                # If test accuracy calcs are causing speed issues you can reduce the number tested via the following:\n",
    "                #summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                #    x: mninst.validation.images[:450].reshape(-1, timeSteps, features), \n",
    "                #    y: mninst.validation.labels[:450]})\n",
    "                summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.validation.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.validation.labels})\n",
    "                validationWriter.add_summary(summary, counter)\n",
    "                \n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation)\n",
    "        \n",
    "    print(\" \")\n",
    "    # Compute test set accuracy rating\n",
    "    summary, accTest = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.test.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.test.labels})\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px;\" src=\"images/mnist-run-two.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM v3\n",
    "\n",
    "* Enclose the CG architecture in a graph object; pass to the training session\n",
    "* Utilize tf.contrib.rnn.LSTMBlockCell\n",
    "* Utilize tf.contrib.rnn.dynamic_rnn, so we don't need to unstack the 'x' tensor\n",
    "* Apply [Batch normalization](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization)\n",
    "* Dense layer with He initialization for weights and biases\n",
    "* Using [tf.nn.softmax_cross_entropy_with_logits_v2](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2) in the 'loss' calculations\n",
    "* Perform gradient clipping via [tf.clip_by_global_norm](https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/clip_by_global_norm) during optimization\n",
    "* Track training and validiation loss and accuracy in TensorBoard\n",
    "\n",
    "LSTM types and benchmarks:  https://returnn.readthedocs.io/en/latest/tf_lstm_benchmark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:15:16.094666Z",
     "start_time": "2018-09-18T21:15:14.061666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the seed\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/mnistLSTM/runThree/train'\n",
    "logDirValidation = './logs/mnistLSTM/runThree/validation'\n",
    "\n",
    "# Create the graph object and populate it\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # We need a way to track if we are training or not for the gradient clipping\n",
    "    isTraining = tf.placeholder_with_default(False, shape = (), name = 'isTraining')\n",
    "    \n",
    "    # Create place holders\n",
    "    x = tf.placeholder(tf.float32, shape = [None, timeSteps, features], name = 'x')\n",
    "    # Give 2nd dimension arg to shape since we are using one hot encodings\n",
    "    y = tf.placeholder(tf.int64, shape = [None, classes], name = 'y')\n",
    "\n",
    "    # Add the LSTM cells with He initialization (we'll let TF worry about the \"w\" and \"b\" values)\n",
    "    # Notice to do this we switch from \"tf.name_scope\" to \"tf.variable_scope\" and add the \"initializer\" param\n",
    "    with tf.variable_scope(\"LSTM\", initializer = tf.variance_scaling_initializer()):\n",
    "\n",
    "        # Create LSTMBlockCell which should be faster\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMBlockCell\n",
    "        # LSTM types and benchmarks:  https://returnn.readthedocs.io/en/latest/tf_lstm_benchmark.html\n",
    "        cell = tf.contrib.rnn.LSTMBlockCell(lstmUnits)\n",
    "\n",
    "        # Add the LSTMBlockCell to the RNN\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "        # Notice we don't have to unstack x as in the previous model\n",
    "        output, state = tf.nn.dynamic_rnn(cell, x, dtype = tf.float32)\n",
    "        \n",
    "        # Return the last output for each sample and apply batch normalization\n",
    "        # Ex: \n",
    "        #   x = np.arange(24)\n",
    "        #   x = x.reshape((2,3,4))\n",
    "        #   x\n",
    "        #   >>>\n",
    "        #   array([[[ 0,  1,  2,  3],\n",
    "        #           [ 4,  5,  6,  7],\n",
    "        #           [ 8,  9, 10, 11]],\n",
    "        #   \n",
    "        #          [[12, 13, 14, 15],\n",
    "        #           [16, 17, 18, 19],\n",
    "        #           [20, 21, 22, 23]]])\n",
    "        #\n",
    "        #   x[:,-1,:]\n",
    "        #   >>>\n",
    "        #   array([[ 8,  9, 10, 11],\n",
    "        #          [20, 21, 22, 23]])\n",
    "        #\n",
    "        # Don't forget to enable/disable training!\n",
    "        bnormOutput = tf.layers.batch_normalization(output[:, -1, :], training = isTraining)\n",
    "        \n",
    "        # Apply the dense layer to output prediction probabilities\n",
    "        yH = tf.layers.dense(bnormOutput, classes)\n",
    "\n",
    "    # Add loss function\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # We don't use \"tf.nn.sparse_softmax_cross_entropy_with_logits\" here since we have one hot encodings\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = yH, labels = y)\n",
    "        loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "        # Capture loss\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        # Since we want to apply gradient clipping we need to compute the gradients,\n",
    "        # process them, and then update the model's parameters by hand\n",
    "        # https://stackoverflow.com/questions/36498127/how-to-apply-gradient-clipping-in-tensorflow\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/clip_by_global_norm\n",
    "        _opt = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "        gvs = _opt.compute_gradients(loss)\n",
    "        cappedGvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "        opt = _opt.apply_gradients(cappedGvs)       \n",
    "\n",
    "    # Eval the model's accuracy\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        # We don't use \"tf.nn.in_top_k(yH, y, 1)\" here since are aren't using \"tf.nn.sparse_softmax_cross_entropy_with_logits\"\n",
    "        correct = tf.equal(tf.argmax(yH, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        # Capture accuracy\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:32:47.556966Z",
     "start_time": "2018-09-18T21:28:10.774141Z"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    validationWriter = tf.summary.FileWriter(logDirValidation)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs + 1):\n",
    "        for i in range(mninst.train.num_examples // samples):      \n",
    "            counter += 1     \n",
    "            \n",
    "            # Grab the next minibatch\n",
    "            xBatch, yBatch = mninst.train.next_batch(samples)\n",
    "            \n",
    "            # Reshape x to [samples, timeSteps, features] for the LSTM:\n",
    "            #   The image is given to us as a single vector of dimensionality 784\n",
    "            #   So to use them we need to gather the number of rows together to be the timeSteps\n",
    "            xBatch = xBatch.reshape(samples, timeSteps, features)\n",
    "            \n",
    "            # Train the model\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 10 == 0:\n",
    "                # Manually add to the train accuracy summary value\n",
    "                summary, accTrain = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter) \n",
    "                \n",
    "                # Manually add to the test accuracy summary value\n",
    "                \n",
    "                # If test accuracy calcs are causing speed issues you can reduce the number tested via the following:\n",
    "                #summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                #    x: mninst.validation.images[:450].reshape(-1, timeSteps, features), \n",
    "                #    y: mninst.validation.labels[:450]})\n",
    "                summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.validation.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.validation.labels})\n",
    "                validationWriter.add_summary(summary, counter)\n",
    "                \n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation)\n",
    "        \n",
    "    print(\" \")\n",
    "    # Compute test set accuracy rating\n",
    "    summary, accTest = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.test.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.test.labels})\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation, \"Test Acc: \", accTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
