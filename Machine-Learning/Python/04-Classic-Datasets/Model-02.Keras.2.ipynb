{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Boston-Housing-Prices-Regression-Modeling-with-Keras\" data-toc-modified-id=\"Boston-Housing-Prices-Regression-Modeling-with-Keras-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Boston Housing Prices Regression Modeling with Keras</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Load-libraries-and-data\" data-toc-modified-id=\"Load-libraries-and-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load libraries and data</a></span></li><li><span><a href=\"#Helper-functions\" data-toc-modified-id=\"Helper-functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Helper functions</a></span></li><li><span><a href=\"#Inspect-and-visualize-the-data\" data-toc-modified-id=\"Inspect-and-visualize-the-data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Inspect and visualize the data</a></span></li><li><span><a href=\"#Model-the-data\" data-toc-modified-id=\"Model-the-data-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-validation-data-set\" data-toc-modified-id=\"Create-validation-data-set-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Create validation data set</a></span></li><li><span><a href=\"#Build-models\" data-toc-modified-id=\"Build-models-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Build models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-model-function\" data-toc-modified-id=\"Build-model-function-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Build model function</a></span></li><li><span><a href=\"#Initial-pass\" data-toc-modified-id=\"Initial-pass-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Initial pass</a></span></li><li><span><a href=\"#Grid-search-hyperparameter-tuning\" data-toc-modified-id=\"Grid-search-hyperparameter-tuning-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Grid search hyperparameter tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Atler-tuneModel-for-RandomizedSearchCV-support\" data-toc-modified-id=\"Atler-tuneModel-for-RandomizedSearchCV-support-6.2.3.1\"><span class=\"toc-item-num\">6.2.3.1&nbsp;&nbsp;</span>Atler tuneModel for RandomizedSearchCV support</a></span></li><li><span><a href=\"#First-execution\" data-toc-modified-id=\"First-execution-6.2.3.2\"><span class=\"toc-item-num\">6.2.3.2&nbsp;&nbsp;</span>First execution</a></span></li><li><span><a href=\"#Second-execution\" data-toc-modified-id=\"Second-execution-6.2.3.3\"><span class=\"toc-item-num\">6.2.3.3&nbsp;&nbsp;</span>Second execution</a></span></li><li><span><a href=\"#Third-execution\" data-toc-modified-id=\"Third-execution-6.2.3.4\"><span class=\"toc-item-num\">6.2.3.4&nbsp;&nbsp;</span>Third execution</a></span></li><li><span><a href=\"#Comments\" data-toc-modified-id=\"Comments-6.2.3.5\"><span class=\"toc-item-num\">6.2.3.5&nbsp;&nbsp;</span>Comments</a></span></li></ul></li><li><span><a href=\"#Predictions\" data-toc-modified-id=\"Predictions-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;</span>Predictions</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Boston Housing Prices Regression Modeling with Keras</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px; width: 40%; height: 40%; \" src=\"images/boston.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source:  [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The purpose of this write-up is to build upon the [first](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.ipynb) and [second](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.Keras.1.ipynb) write-ups involving the Boston housing prices dataset.  \n",
    "\n",
    "Goals include:\n",
    "* Utilize RandomizedSearchCV for hyperparameter tuning\n",
    "* Feature selection with SelectKBest\n",
    "* Examine algorithm performance visually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T03:16:21.340154Z",
     "start_time": "2018-08-27T03:16:21.126123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T03:16:21.644153Z",
     "start_time": "2018-08-27T03:16:21.343155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "\n",
    "#import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T03:16:21.964189Z",
     "start_time": "2018-08-27T03:16:21.650173Z"
    }
   },
   "outputs": [],
   "source": [
    "dataFile = os.path.join(\".\", \"datasets\", \"housing.csv\")\n",
    "data = read_csv(dataFile, header = 0, delim_whitespace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T03:16:28.244707Z",
     "start_time": "2018-08-27T03:16:28.053702Z"
    }
   },
   "outputs": [],
   "source": [
    "def corrTableColors(value):\n",
    "    color = 'black'\n",
    "\n",
    "    if value == 1:\n",
    "        color = 'white'\n",
    "    elif value < -0.7:\n",
    "        color = 'red'\n",
    "    elif value > 0.7:\n",
    "        color = 'green'\n",
    "\n",
    "    return 'color: %s' % color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T03:16:28.580727Z",
     "start_time": "2018-08-27T03:16:28.331695Z"
    }
   },
   "outputs": [],
   "source": [
    "def makeRange(start, stop, step = 1, multi = 1, dec = 1):\n",
    "    vals = []\n",
    "    for i in range(start, stop, step):\n",
    "        vals.append(np.round(multi * i, decimals = dec))\n",
    "        \n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect and visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please the [first Boston housing data's write-up](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.ipynb#Inspect-and-visualize-the-data) details on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T03:16:30.654049Z",
     "start_time": "2018-08-27T03:16:30.404016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =  (506, 13)\n",
      "y.shape =  (506,)\n",
      "--------\n",
      "xTrain.shape =  (404, 13)\n",
      "yTrain.shape =  (404,)\n",
      "xVal.shape =  (102, 13)\n",
      "yVal.shape =  (102,)\n"
     ]
    }
   ],
   "source": [
    "# Seperate X and Y values\n",
    "x = data.values[:, 0:len(data.columns) - 1]\n",
    "y = data.values[:, len(data.columns) - 1]\n",
    "\n",
    "print(\"x.shape = \", x.shape)\n",
    "print(\"y.shape = \", y.shape)\n",
    "\n",
    "# Split out validation set -- 80/20 split\n",
    "seed = 10\n",
    "valSize = 0.2\n",
    "\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(x, y, test_size = valSize, random_state = seed)\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"xTrain.shape = \", xTrain.shape)\n",
    "print(\"yTrain.shape = \", yTrain.shape)\n",
    "print(\"xVal.shape = \", xVal.shape)\n",
    "print(\"yVal.shape = \", yVal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info on the `kernal_initializer`:  https://keras.io/initializers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T03:16:32.333363Z",
     "start_time": "2018-08-27T03:16:32.132334Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildModel(optimizer = 'Adam', lr = 0.001, decay = 0.0, epsilon = None):\n",
    "    opt = None\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # kernel_initializer='normal' -> Initializer capable of adapting its scale to the shape of weights\n",
    "    # bias_initializer -> 'zeros' (default per the docs)\n",
    "    \n",
    "    #model.add(Dense(20, input_dim = xTrain.shape[1], kernel_initializer='normal', activation = 'relu'))\n",
    "    #model.add(Dense(20, input_dim = 18, kernel_initializer='normal', activation = 'relu'))\n",
    "    model.add(Dense(20, kernel_initializer='normal', activation = 'relu'))\n",
    "    model.add(Dense(10, kernel_initializer='normal', activation = 'relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    if optimizer.lower() == 'adam':\n",
    "        opt = Adam(lr = lr, decay = decay, epsilon = epsilon)\n",
    "    else:\n",
    "        # Please don't ever use eval where you're recieving input from non-trusted sources!\n",
    "        # A Jupyter notebook is OK; a public facing service is certainly not\n",
    "        opt = eval(optimizer)()\n",
    "    \n",
    "    model.compile(loss = 'mean_squared_error', optimizer = opt)\n",
    "    \n",
    "    return model   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first pass an educated guess is taken for what might work well on the dataset.  This provides an initial baseline, and then hyperparameter tuning an occur to refine the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -17.67 (7.86)\n"
     ]
    }
   ],
   "source": [
    "# Define vars and init\n",
    "folds = 10\n",
    "seed = 10\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = KerasRegressor(build_fn = buildModel, epochs = 200, batch_size = 5, verbose = 0)\n",
    "kFold = KFold(n_splits = folds, random_state = seed)\n",
    "results = cross_val_score(model, xTrain, yTrain, cv = kFold)\n",
    "\n",
    "print(\"MSE: %.2f (%.2f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better then what the [previous](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.ipynb) write-up's models accomplished with no tuning as of yet:\n",
    "\n",
    "<pre>\n",
    "         Model    MSE  StdDev\n",
    "3    scaledKNN -20.35   11.87\n",
    "0     scaledLR -21.26    7.11\n",
    "4   scaledCART -22.66    9.31\n",
    "1  scaledLASSO -26.94   10.38\n",
    "5    scaledSVR -28.52   13.98\n",
    "2     scaledEN -28.60   11.65\n",
    "</pre>\n",
    "\n",
    "It does not; however, compare to the results achieved via the ensemble methods:\n",
    "\n",
    "<pre>\n",
    "       Model     MSE  StdDev\n",
    "1  scaledGBM -9.700   5.342 \n",
    "3  scaledET  -10.339  5.399 \n",
    "2  scaledRF  -13.695  7.276 \n",
    "0  scaledAB  -14.176  8.917\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a [previous write-up](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.Keras.1.ipynb) we utilized [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).  We'd like to now examine [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html).\n",
    "\n",
    "We observed in the last write-up that the `KerasRegressor` estimator utilizing the `Adam` optimizer give us good results.  We'll continue working with this combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atler tuneModel for RandomizedSearchCV support \n",
    "\n",
    "We need to alter the `tuneModel` function to support RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T03:24:14.299953Z",
     "start_time": "2018-08-27T03:24:14.012937Z"
    }
   },
   "outputs": [],
   "source": [
    "def tuneModel(modelName, modelObj, params, iterations = 20, returnModel = False, showSummary = True):\n",
    "    # Init vars and params\n",
    "    featureResults = {}\n",
    "    featureFolds = 10\n",
    "    featureSeed = 10\n",
    "    \n",
    "    np.random.seed(featureSeed)\n",
    "    \n",
    "    # Use MSE since this is a regression problem\n",
    "    score = 'neg_mean_squared_error'\n",
    "\n",
    "    # Create a Pandas DF to hold all our spiffy results\n",
    "    featureDF = DataFrame(columns = ['Model', 'MSE', 'StdDev', 'Best Params'])\n",
    "\n",
    "    # Create feature union (adding SelectKBest)\n",
    "    features = []\n",
    "    features.append(('Scaler', StandardScaler()))\n",
    "    features.append(('SelectKBest', SelectKBest()))\n",
    "    featureUnion = FeatureUnion(features)\n",
    "\n",
    "    # Search for the best combination of parameters\n",
    "    featureResults = RandomizedSearchCV(\n",
    "        Pipeline(\n",
    "            steps = [\n",
    "                ('FeatureUnion', featureUnion),\n",
    "                (modelName, modelObj)\n",
    "        ]),\n",
    "        param_distributions = params,\n",
    "        n_iter = iterations,\n",
    "        scoring = score,\n",
    "        cv = KFold(n_splits = featureFolds, random_state = featureSeed),\n",
    "        random_state = featureSeed\n",
    "    ).fit(xTrain, yTrain)\n",
    "\n",
    "    featureDF.loc[len(featureDF)] = list([\n",
    "        modelName, \n",
    "        featureResults.best_score_,\n",
    "        featureResults.best_params_,\n",
    "        featureResults.cv_results_['std_test_score'][featureResults.best_index_]\n",
    "    ])\n",
    "\n",
    "    if showSummary:\n",
    "        set_option('display.max_colwidth', -1)\n",
    "        display(featureDF)\n",
    "    \n",
    "    if returnModel:\n",
    "        return featureResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's dig in and see what sort of parameter combinations `RandomizedSearchCV` might be able to find for us that provide good algorithm performance.  If we would have utilized `GridSearchCV` as in the [previous write-up](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.Keras.1.ipynb) we'd probably be here all week waiting for the tuning process to finish.  ;)\n",
    "\n",
    "(Note that we'll run the tuning session in batches of five due to the length of time it takes the process to complete.  In a production setting I'd likely spin up multiple computing instances, have each one iterate N number of times in parallel, and then pick the hyperparameter set from whatever batch of testing yielded the best results.)\n",
    "\n",
    "So, let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"housingModel\"\n",
    "modelObj =  KerasRegressor(build_fn = buildModel, verbose = 0)\n",
    "params = {\n",
    "    'housingModel__optimizer' : ['Adam'],\n",
    "    'housingModel__epochs' : makeRange(200, 600, 50),\n",
    "    'housingModel__batch_size' : makeRange(4, 68, 4),\n",
    "    'FeatureUnion__SelectKBest__k': makeRange(1, xTrain.shape[1]),\n",
    "    'housingModel__lr' : makeRange(1, 9, 1, .001, 3),\n",
    "    'housingModel__epsilon' : makeRange(2, 8, 1, .5, 1),\n",
    "}\n",
    "\n",
    "set1 = tuneModel(modelName, modelObj, params, 20, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_option('precision', 3)\n",
    "DataFrame(set1.cv_results_).sort_values(by=['mean_test_score', 'std_test_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.86220981452563"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.best_index_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.319312481377029"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.cv_results_['std_test_score'][m3.best_index_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_FeatureUnion__SelectKBest__k</th>\n",
       "      <th>param_housingModel__batch_size</th>\n",
       "      <th>param_housingModel__epochs</th>\n",
       "      <th>param_housingModel__epsilon</th>\n",
       "      <th>param_housingModel__lr</th>\n",
       "      <th>param_housingModel__optimizer</th>\n",
       "      <th>...</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.463</td>\n",
       "      <td>1.099</td>\n",
       "      <td>-12.862</td>\n",
       "      <td>-5.731</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>350</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.075</td>\n",
       "      <td>-5.461</td>\n",
       "      <td>-9.447</td>\n",
       "      <td>-6.562</td>\n",
       "      <td>-25.432</td>\n",
       "      <td>-4.450</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.045</td>\n",
       "      <td>10.319</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.218</td>\n",
       "      <td>1.163</td>\n",
       "      <td>-13.031</td>\n",
       "      <td>-7.489</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>400</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.002</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.129</td>\n",
       "      <td>-6.937</td>\n",
       "      <td>-7.180</td>\n",
       "      <td>-7.150</td>\n",
       "      <td>-28.626</td>\n",
       "      <td>-5.735</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.035</td>\n",
       "      <td>9.203</td>\n",
       "      <td>0.706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>96.735</td>\n",
       "      <td>3.122</td>\n",
       "      <td>-13.111</td>\n",
       "      <td>-4.450</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>550</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.003</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-42.452</td>\n",
       "      <td>-4.032</td>\n",
       "      <td>-7.232</td>\n",
       "      <td>-4.309</td>\n",
       "      <td>-23.099</td>\n",
       "      <td>-4.444</td>\n",
       "      <td>4.888</td>\n",
       "      <td>0.143</td>\n",
       "      <td>10.898</td>\n",
       "      <td>0.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57.280</td>\n",
       "      <td>1.353</td>\n",
       "      <td>-14.783</td>\n",
       "      <td>-7.921</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>550</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.006</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-40.018</td>\n",
       "      <td>-5.304</td>\n",
       "      <td>-6.855</td>\n",
       "      <td>-7.011</td>\n",
       "      <td>-30.046</td>\n",
       "      <td>-6.898</td>\n",
       "      <td>1.543</td>\n",
       "      <td>0.082</td>\n",
       "      <td>10.714</td>\n",
       "      <td>2.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18.614</td>\n",
       "      <td>2.178</td>\n",
       "      <td>-15.075</td>\n",
       "      <td>-9.850</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.716</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>-11.461</td>\n",
       "      <td>-10.445</td>\n",
       "      <td>-25.758</td>\n",
       "      <td>-6.847</td>\n",
       "      <td>1.673</td>\n",
       "      <td>0.143</td>\n",
       "      <td>5.476</td>\n",
       "      <td>1.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>40.815</td>\n",
       "      <td>3.164</td>\n",
       "      <td>-15.332</td>\n",
       "      <td>-7.959</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>500</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.007</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-32.447</td>\n",
       "      <td>-5.418</td>\n",
       "      <td>-9.410</td>\n",
       "      <td>-10.991</td>\n",
       "      <td>-35.423</td>\n",
       "      <td>-10.435</td>\n",
       "      <td>1.256</td>\n",
       "      <td>0.082</td>\n",
       "      <td>9.775</td>\n",
       "      <td>1.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12.974</td>\n",
       "      <td>1.883</td>\n",
       "      <td>-15.467</td>\n",
       "      <td>-8.785</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>250</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.002</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.122</td>\n",
       "      <td>-8.231</td>\n",
       "      <td>-8.275</td>\n",
       "      <td>-8.602</td>\n",
       "      <td>-30.644</td>\n",
       "      <td>-6.896</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.069</td>\n",
       "      <td>11.340</td>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11.835</td>\n",
       "      <td>1.583</td>\n",
       "      <td>-15.518</td>\n",
       "      <td>-11.314</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>300</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-29.896</td>\n",
       "      <td>-11.974</td>\n",
       "      <td>-9.298</td>\n",
       "      <td>-10.569</td>\n",
       "      <td>-28.931</td>\n",
       "      <td>-9.488</td>\n",
       "      <td>1.570</td>\n",
       "      <td>0.122</td>\n",
       "      <td>7.787</td>\n",
       "      <td>2.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>143.409</td>\n",
       "      <td>2.105</td>\n",
       "      <td>-15.539</td>\n",
       "      <td>-9.461</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>450</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.598</td>\n",
       "      <td>-11.016</td>\n",
       "      <td>-10.489</td>\n",
       "      <td>-8.382</td>\n",
       "      <td>-21.079</td>\n",
       "      <td>-7.865</td>\n",
       "      <td>4.902</td>\n",
       "      <td>0.176</td>\n",
       "      <td>6.211</td>\n",
       "      <td>2.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>116.901</td>\n",
       "      <td>2.905</td>\n",
       "      <td>-15.680</td>\n",
       "      <td>-11.468</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.890</td>\n",
       "      <td>-9.208</td>\n",
       "      <td>-8.809</td>\n",
       "      <td>-10.790</td>\n",
       "      <td>-20.497</td>\n",
       "      <td>-10.364</td>\n",
       "      <td>15.009</td>\n",
       "      <td>0.381</td>\n",
       "      <td>6.204</td>\n",
       "      <td>1.448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.745</td>\n",
       "      <td>1.401</td>\n",
       "      <td>-15.765</td>\n",
       "      <td>-12.127</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.844</td>\n",
       "      <td>-10.909</td>\n",
       "      <td>-8.685</td>\n",
       "      <td>-10.705</td>\n",
       "      <td>-27.893</td>\n",
       "      <td>-10.549</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.039</td>\n",
       "      <td>6.789</td>\n",
       "      <td>2.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.229</td>\n",
       "      <td>1.008</td>\n",
       "      <td>-16.017</td>\n",
       "      <td>-10.612</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.528</td>\n",
       "      <td>-10.316</td>\n",
       "      <td>-10.569</td>\n",
       "      <td>-10.538</td>\n",
       "      <td>-35.029</td>\n",
       "      <td>-10.180</td>\n",
       "      <td>1.003</td>\n",
       "      <td>0.045</td>\n",
       "      <td>9.492</td>\n",
       "      <td>0.563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17.747</td>\n",
       "      <td>2.261</td>\n",
       "      <td>-16.973</td>\n",
       "      <td>-11.350</td>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>400</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-29.160</td>\n",
       "      <td>-9.504</td>\n",
       "      <td>-11.027</td>\n",
       "      <td>-11.955</td>\n",
       "      <td>-37.446</td>\n",
       "      <td>-9.154</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.113</td>\n",
       "      <td>9.201</td>\n",
       "      <td>2.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11.228</td>\n",
       "      <td>1.800</td>\n",
       "      <td>-17.596</td>\n",
       "      <td>-13.192</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>200</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.191</td>\n",
       "      <td>-10.360</td>\n",
       "      <td>-9.331</td>\n",
       "      <td>-14.902</td>\n",
       "      <td>-43.884</td>\n",
       "      <td>-10.971</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.063</td>\n",
       "      <td>11.670</td>\n",
       "      <td>1.558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21.305</td>\n",
       "      <td>2.469</td>\n",
       "      <td>-17.861</td>\n",
       "      <td>-11.981</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>300</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.004</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.880</td>\n",
       "      <td>-9.673</td>\n",
       "      <td>-9.244</td>\n",
       "      <td>-10.622</td>\n",
       "      <td>-36.087</td>\n",
       "      <td>-8.939</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.140</td>\n",
       "      <td>8.612</td>\n",
       "      <td>2.490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50.580</td>\n",
       "      <td>3.807</td>\n",
       "      <td>-18.514</td>\n",
       "      <td>-14.088</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>250</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-29.585</td>\n",
       "      <td>-11.202</td>\n",
       "      <td>-13.477</td>\n",
       "      <td>-12.384</td>\n",
       "      <td>-29.793</td>\n",
       "      <td>-14.566</td>\n",
       "      <td>1.877</td>\n",
       "      <td>0.132</td>\n",
       "      <td>7.223</td>\n",
       "      <td>1.619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13.648</td>\n",
       "      <td>1.694</td>\n",
       "      <td>-18.917</td>\n",
       "      <td>-11.773</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>550</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.003</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-36.579</td>\n",
       "      <td>-8.231</td>\n",
       "      <td>-10.523</td>\n",
       "      <td>-11.639</td>\n",
       "      <td>-39.598</td>\n",
       "      <td>-10.644</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.080</td>\n",
       "      <td>10.595</td>\n",
       "      <td>1.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>33.328</td>\n",
       "      <td>3.616</td>\n",
       "      <td>-19.831</td>\n",
       "      <td>-13.021</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-37.325</td>\n",
       "      <td>-9.934</td>\n",
       "      <td>-9.960</td>\n",
       "      <td>-12.399</td>\n",
       "      <td>-39.905</td>\n",
       "      <td>-10.407</td>\n",
       "      <td>3.027</td>\n",
       "      <td>0.195</td>\n",
       "      <td>10.186</td>\n",
       "      <td>1.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>157.646</td>\n",
       "      <td>2.471</td>\n",
       "      <td>-20.861</td>\n",
       "      <td>-15.350</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.008</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-46.279</td>\n",
       "      <td>-13.416</td>\n",
       "      <td>-14.237</td>\n",
       "      <td>-18.496</td>\n",
       "      <td>-34.242</td>\n",
       "      <td>-9.637</td>\n",
       "      <td>7.125</td>\n",
       "      <td>0.173</td>\n",
       "      <td>11.051</td>\n",
       "      <td>3.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.379</td>\n",
       "      <td>2.955</td>\n",
       "      <td>-33.026</td>\n",
       "      <td>-29.473</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>250</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.803</td>\n",
       "      <td>-21.004</td>\n",
       "      <td>-34.467</td>\n",
       "      <td>-30.187</td>\n",
       "      <td>-81.156</td>\n",
       "      <td>-41.000</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.105</td>\n",
       "      <td>18.142</td>\n",
       "      <td>6.300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "1   10.463         1.099           -12.862          -5.731              \n",
       "2   10.218         1.163           -13.031          -7.489              \n",
       "16  96.735         3.122           -13.111          -4.450              \n",
       "3   57.280         1.353           -14.783          -7.921              \n",
       "10  18.614         2.178           -15.075          -9.850              \n",
       "17  40.815         3.164           -15.332          -7.959              \n",
       "8   12.974         1.883           -15.467          -8.785              \n",
       "5   11.835         1.583           -15.518          -11.314             \n",
       "9   143.409        2.105           -15.539          -9.461              \n",
       "14  116.901        2.905           -15.680          -11.468             \n",
       "4   10.745         1.401           -15.765          -12.127             \n",
       "0   19.229         1.008           -16.017          -10.612             \n",
       "11  17.747         2.261           -16.973          -11.350             \n",
       "7   11.228         1.800           -17.596          -13.192             \n",
       "13  21.305         2.469           -17.861          -11.981             \n",
       "19  50.580         3.807           -18.514          -14.088             \n",
       "6   13.648         1.694           -18.917          -11.773             \n",
       "18  33.328         3.616           -19.831          -13.021             \n",
       "12  157.646        2.471           -20.861          -15.350             \n",
       "15  16.379         2.955           -33.026          -29.473             \n",
       "\n",
       "   param_FeatureUnion__SelectKBest__k param_housingModel__batch_size  \\\n",
       "1   2                                  36                              \n",
       "2   2                                  44                              \n",
       "16  2                                  12                              \n",
       "3   7                                  8                               \n",
       "10  8                                  48                              \n",
       "17  6                                  28                              \n",
       "8   2                                  36                              \n",
       "5   3                                  44                              \n",
       "9   10                                 4                               \n",
       "14  12                                 4                               \n",
       "4   3                                  60                              \n",
       "0   9                                  16                              \n",
       "11  9                                  48                              \n",
       "7   2                                  36                              \n",
       "13  9                                  28                              \n",
       "19  11                                 16                              \n",
       "6   7                                  64                              \n",
       "18  6                                  20                              \n",
       "12  9                                  4                               \n",
       "15  6                                  60                              \n",
       "\n",
       "   param_housingModel__epochs param_housingModel__epsilon  \\\n",
       "1   350                        1.5                          \n",
       "2   400                        2.5                          \n",
       "16  550                        3.5                          \n",
       "3   550                        1.5                          \n",
       "10  450                        1                            \n",
       "17  500                        2.5                          \n",
       "8   250                        2.5                          \n",
       "5   300                        2                            \n",
       "9   450                        2                            \n",
       "14  250                        1                            \n",
       "4   450                        1                            \n",
       "0   400                        1                            \n",
       "11  400                        3                            \n",
       "7   200                        1.5                          \n",
       "13  300                        3.5                          \n",
       "19  250                        2.5                          \n",
       "6   550                        3.5                          \n",
       "18  200                        3                            \n",
       "12  400                        3.5                          \n",
       "15  250                        3.5                          \n",
       "\n",
       "   param_housingModel__lr param_housingModel__optimizer       ...         \\\n",
       "1   0.005                  Adam                               ...          \n",
       "2   0.002                  Adam                               ...          \n",
       "16  0.003                  Adam                               ...          \n",
       "3   0.006                  Adam                               ...          \n",
       "10  0.004                  Adam                               ...          \n",
       "17  0.007                  Adam                               ...          \n",
       "8   0.002                  Adam                               ...          \n",
       "5   0.002                  Adam                               ...          \n",
       "9   0.001                  Adam                               ...          \n",
       "14  0.007                  Adam                               ...          \n",
       "4   0.001                  Adam                               ...          \n",
       "0   0.001                  Adam                               ...          \n",
       "11  0.005                  Adam                               ...          \n",
       "7   0.001                  Adam                               ...          \n",
       "13  0.004                  Adam                               ...          \n",
       "19  0.001                  Adam                               ...          \n",
       "6   0.003                  Adam                               ...          \n",
       "18  0.002                  Adam                               ...          \n",
       "12  0.008                  Adam                               ...          \n",
       "15  0.001                  Adam                               ...          \n",
       "\n",
       "   split7_test_score  split7_train_score  split8_test_score  \\\n",
       "1  -39.075           -5.461              -9.447               \n",
       "2  -33.129           -6.937              -7.180               \n",
       "16 -42.452           -4.032              -7.232               \n",
       "3  -40.018           -5.304              -6.855               \n",
       "10 -22.716           -8.291              -11.461              \n",
       "17 -32.447           -5.418              -9.410               \n",
       "8  -43.122           -8.231              -8.275               \n",
       "5  -29.896           -11.974             -9.298               \n",
       "9  -26.598           -11.016             -10.489              \n",
       "14 -25.890           -9.208              -8.809               \n",
       "4  -23.844           -10.909             -8.685               \n",
       "0  -34.528           -10.316             -10.569              \n",
       "11 -29.160           -9.504              -11.027              \n",
       "7  -31.191           -10.360             -9.331               \n",
       "13 -23.880           -9.673              -9.244               \n",
       "19 -29.585           -11.202             -13.477              \n",
       "6  -36.579           -8.231              -10.523              \n",
       "18 -37.325           -9.934              -9.960               \n",
       "12 -46.279           -13.416             -14.237              \n",
       "15 -17.803           -21.004             -34.467              \n",
       "\n",
       "    split8_train_score  split9_test_score  split9_train_score  std_fit_time  \\\n",
       "1  -6.562              -25.432            -4.450               0.642          \n",
       "2  -7.150              -28.626            -5.735               0.432          \n",
       "16 -4.309              -23.099            -4.444               4.888          \n",
       "3  -7.011              -30.046            -6.898               1.543          \n",
       "10 -10.445             -25.758            -6.847               1.673          \n",
       "17 -10.991             -35.423            -10.435              1.256          \n",
       "8  -8.602              -30.644            -6.896               0.258          \n",
       "5  -10.569             -28.931            -9.488               1.570          \n",
       "9  -8.382              -21.079            -7.865               4.902          \n",
       "14 -10.790             -20.497            -10.364              15.009         \n",
       "4  -10.705             -27.893            -10.549              0.305          \n",
       "0  -10.538             -35.029            -10.180              1.003          \n",
       "11 -11.955             -37.446            -9.154               1.190          \n",
       "7  -14.902             -43.884            -10.971              0.976          \n",
       "13 -10.622             -36.087            -8.939               0.662          \n",
       "19 -12.384             -29.793            -14.566              1.877          \n",
       "6  -11.639             -39.598            -10.644              0.388          \n",
       "18 -12.399             -39.905            -10.407              3.027          \n",
       "12 -18.496             -34.242            -9.637               7.125          \n",
       "15 -30.187             -81.156            -41.000              0.708          \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "1   0.045           10.319          0.740            \n",
       "2   0.035           9.203           0.706            \n",
       "16  0.143           10.898          0.579            \n",
       "3   0.082           10.714          2.132            \n",
       "10  0.143           5.476           1.720            \n",
       "17  0.082           9.775           1.703            \n",
       "8   0.069           11.340          0.861            \n",
       "5   0.122           7.787           2.496            \n",
       "9   0.176           6.211           2.298            \n",
       "14  0.381           6.204           1.448            \n",
       "4   0.039           6.789           2.495            \n",
       "0   0.045           9.492           0.563            \n",
       "11  0.113           9.201           2.086            \n",
       "7   0.063           11.670          1.558            \n",
       "13  0.140           8.612           2.490            \n",
       "19  0.132           7.223           1.619            \n",
       "6   0.080           10.595          1.918            \n",
       "18  0.195           10.186          1.980            \n",
       "12  0.173           11.051          3.749            \n",
       "15  0.105           18.142          6.300            \n",
       "\n",
       "[20 rows x 36 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_option('precision', 3)\n",
    "DataFrame(m3.cv_results_).sort_values(by=['mean_test_score', 'std_test_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "Again, if we were creating a production quality model we would have started with randomized parameter optimization process.  The results from that process would then lead to a set of smaller grids focusing more and more on whatever parameter option permutations showed the most promise.\n",
    "\n",
    "You can see an example of this type of process I worked on previously [here](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/03-ComputerVision-Classification/Classification-03.ipynb).\n",
    "\n",
    "Also, unless the randomized parameter optimization process were to lead to signifigant improvements from what we've seen so far we'd be better of utilizing the gradient boosting algorithm we utilized in [previous write-up](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.ipynb#Initial-pass---Ensemble-methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "Hopefully to same some one else some pain down the road:\n",
    "\n",
    "I kept getting the following error when working on this prediction section, which frankly was driving me nuts:\n",
    "    \n",
    "```\n",
    "TypeError: call() missing 1 required positional argument: 'inputs'\n",
    "```\n",
    "\n",
    "After researching the error message I came upon this comment which let me to the resolution:\n",
    "\n",
    "_The thing here is that KerasRegressor expects a callable that builds a model, rather than the model itself. By wrapping your function in this way you can return the build function (without calling it)._  [Source](https://stackoverflow.com/questions/47944463/specify-input-argument-with-kerasregressor)\n",
    "\n",
    "Solution:  I needed to **wrap** the `buildModel()` function!  :(\n",
    "\n",
    "Once I 'wrapped' the `buildModel()` function the prediction code blocks finally started working, and that's why we have the `wrapper()` function implemented below...\n",
    "\n",
    "**END NOTE**\n",
    "\n",
    "And now that that's out of the way we'll take a look at some predictions using the test data set based on the tuning results from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See NOTE above on why we have this new function\n",
    "def wrapper(optimizer = 'Adam', lr = 0.001, decay = 0.0, epsilon = None):\n",
    "    \n",
    "    def buildModel():\n",
    "        opt = None\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # kernel_initializer='normal' -> Initializer capable of adapting its scale to the shape of weights\n",
    "        # bias_initializer -> 'zeros' (default per the docs)\n",
    "\n",
    "        model.add(Dense(20, input_dim = xTrain.shape[1], kernel_initializer='normal', activation = 'relu'))\n",
    "        model.add(Dense(10, kernel_initializer='normal', activation = 'relu'))\n",
    "        model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "        if optimizer.lower() == 'adam':\n",
    "            opt = Adam(lr = lr, decay = decay, epsilon = epsilon)\n",
    "        else:\n",
    "            # Please don't ever use eval where you're recieving input from non-trusted sources!\n",
    "            # A Jupyter notebook is OK; a public facing service is certainly not\n",
    "            opt = eval(optimizer)()\n",
    "\n",
    "        model.compile(loss = 'mean_squared_error', optimizer = opt)\n",
    "\n",
    "        return model\n",
    "\n",
    "    return buildModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE =  11.93922294223878\n",
      "RMSE =  3.4553180667253747\n"
     ]
    }
   ],
   "source": [
    "# Build the model, and pass the KerasRegressor a callable function to the 'build_fn' argument\n",
    "# Use the parameters we found were most effective during the hyperparameter tuning\n",
    "m =  KerasRegressor(\n",
    "    build_fn = wrapper(optimizer = 'Adam', lr = 0.003, epsilon = 1), \n",
    "    epochs = 300, \n",
    "    batch_size = 32, \n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "# Now fit the model to the training data ensuring we perform the same sort of pipeline transformations\n",
    "# that occured during the hyperparameter tuning (i.e. feature scaling)\n",
    "xScaled = StandardScaler().fit(xTrain).transform(xTrain)\n",
    "m.fit(xScaled, yTrain)\n",
    "\n",
    "# Now we can finally make some predictions using our trained model on unseen data\n",
    "xScaled = StandardScaler().fit(xTrain).transform(xVal)\n",
    "preds = m.predict(xScaled)\n",
    "mse = mean_squared_error(yVal, preds)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "print(\"MSE = \", mse)\n",
    "print(\"RMSE = \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T01:52:09.339400Z",
     "start_time": "2018-08-27T01:52:09.114369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makeRange(4, 68, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T01:50:45.294455Z",
     "start_time": "2018-08-27T01:50:45.040458Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-3715696215e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxTrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'col'"
     ]
    }
   ],
   "source": [
    "xTrain.col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T02:23:35.274301Z",
     "start_time": "2018-08-27T02:23:35.055287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makeRange(1, 9, 1, .001, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T02:29:43.169231Z",
     "start_time": "2018-08-27T02:29:42.949309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.5, 2.0, 2.5, 3.0, 3.5]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makeRange(2, 8, 1, .5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
