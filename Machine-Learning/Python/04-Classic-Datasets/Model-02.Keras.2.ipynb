{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Boston-Housing-Prices-Regression-Modeling-with-Keras\" data-toc-modified-id=\"Boston-Housing-Prices-Regression-Modeling-with-Keras-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Boston Housing Prices Regression Modeling with Keras</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Load-libraries-and-data\" data-toc-modified-id=\"Load-libraries-and-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load libraries and data</a></span></li><li><span><a href=\"#Helper-functions\" data-toc-modified-id=\"Helper-functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Helper functions</a></span></li><li><span><a href=\"#Inspect-and-visualize-the-data\" data-toc-modified-id=\"Inspect-and-visualize-the-data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Inspect and visualize the data</a></span></li><li><span><a href=\"#Model-the-data\" data-toc-modified-id=\"Model-the-data-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-validation-data-set\" data-toc-modified-id=\"Create-validation-data-set-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Create validation data set</a></span></li><li><span><a href=\"#Build-models\" data-toc-modified-id=\"Build-models-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Build models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-model-function\" data-toc-modified-id=\"Build-model-function-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Build model function</a></span></li><li><span><a href=\"#Initial-pass\" data-toc-modified-id=\"Initial-pass-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Initial pass</a></span></li><li><span><a href=\"#Grid-search-hyperparameter-tuning\" data-toc-modified-id=\"Grid-search-hyperparameter-tuning-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Grid search hyperparameter tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Alter-tuneModel-for-RandomizedSearchCV-support\" data-toc-modified-id=\"Alter-tuneModel-for-RandomizedSearchCV-support-6.2.3.1\"><span class=\"toc-item-num\">6.2.3.1&nbsp;&nbsp;</span>Alter tuneModel for RandomizedSearchCV support</a></span></li><li><span><a href=\"#First-execution\" data-toc-modified-id=\"First-execution-6.2.3.2\"><span class=\"toc-item-num\">6.2.3.2&nbsp;&nbsp;</span>First execution</a></span></li><li><span><a href=\"#Graph-performance-of-the-best-model\" data-toc-modified-id=\"Graph-performance-of-the-best-model-6.2.3.3\"><span class=\"toc-item-num\">6.2.3.3&nbsp;&nbsp;</span>Graph performance of the best model</a></span></li><li><span><a href=\"#Comments\" data-toc-modified-id=\"Comments-6.2.3.4\"><span class=\"toc-item-num\">6.2.3.4&nbsp;&nbsp;</span>Comments</a></span></li><li><span><a href=\"#Tune-network-topology\" data-toc-modified-id=\"Tune-network-topology-6.2.3.5\"><span class=\"toc-item-num\">6.2.3.5&nbsp;&nbsp;</span>Tune network topology</a></span></li><li><span><a href=\"#Graph-final-model-performance\" data-toc-modified-id=\"Graph-final-model-performance-6.2.3.6\"><span class=\"toc-item-num\">6.2.3.6&nbsp;&nbsp;</span>Graph final model performance</a></span></li><li><span><a href=\"#Comments\" data-toc-modified-id=\"Comments-6.2.3.7\"><span class=\"toc-item-num\">6.2.3.7&nbsp;&nbsp;</span>Comments</a></span></li></ul></li><li><span><a href=\"#Predictions\" data-toc-modified-id=\"Predictions-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;</span>Predictions</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Boston Housing Prices Regression Modeling with Keras</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px; width: 40%; height: 40%; \" src=\"images/boston.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source:  [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The purpose of this write-up is to build upon the [first](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.ipynb) and [second](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.Keras.1.ipynb) write-ups involving the Boston housing prices dataset.  \n",
    "\n",
    "Goals include:\n",
    "* Utilize RandomizedSearchCV for hyperparameter tuning\n",
    "* Feature selection with SelectKBest\n",
    "* Examine algorithm performance visually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:16:41.212369Z",
     "start_time": "2018-08-28T17:16:35.040891Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T21:50:37.657600Z",
     "start_time": "2018-08-28T21:50:37.405284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "\n",
    "#import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.callbacks import History\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:17:15.648589Z",
     "start_time": "2018-08-28T17:17:15.368101Z"
    }
   },
   "outputs": [],
   "source": [
    "dataFile = os.path.join(\".\", \"datasets\", \"housing.csv\")\n",
    "data = read_csv(dataFile, header = 0, delim_whitespace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:17:15.804824Z",
     "start_time": "2018-08-28T17:17:15.648589Z"
    }
   },
   "outputs": [],
   "source": [
    "def corrTableColors(value):\n",
    "    color = 'black'\n",
    "\n",
    "    if value == 1:\n",
    "        color = 'white'\n",
    "    elif value < -0.7:\n",
    "        color = 'red'\n",
    "    elif value > 0.7:\n",
    "        color = 'green'\n",
    "\n",
    "    return 'color: %s' % color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:17:15.961016Z",
     "start_time": "2018-08-28T17:17:15.804824Z"
    }
   },
   "outputs": [],
   "source": [
    "def makeRange(start, stop, step = 1, multi = 1, dec = 1):\n",
    "    vals = []\n",
    "    for i in range(start, stop, step):\n",
    "        vals.append(np.round(multi * i, decimals = dec))\n",
    "        \n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect and visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please the [first Boston housing data's write-up](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.ipynb#Inspect-and-visualize-the-data) details on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:17:16.148576Z",
     "start_time": "2018-08-28T17:17:15.961016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =  (506, 13)\n",
      "y.shape =  (506,)\n",
      "--------\n",
      "xTrain.shape =  (404, 13)\n",
      "yTrain.shape =  (404,)\n",
      "xVal.shape =  (102, 13)\n",
      "yVal.shape =  (102,)\n"
     ]
    }
   ],
   "source": [
    "# Seperate X and Y values\n",
    "x = data.values[:, 0:len(data.columns) - 1]\n",
    "y = data.values[:, len(data.columns) - 1]\n",
    "\n",
    "print(\"x.shape = \", x.shape)\n",
    "print(\"y.shape = \", y.shape)\n",
    "\n",
    "# Split out validation set -- 80/20 split\n",
    "seed = 10\n",
    "valSize = 0.2\n",
    "\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(x, y, test_size = valSize, random_state = seed)\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"xTrain.shape = \", xTrain.shape)\n",
    "print(\"yTrain.shape = \", yTrain.shape)\n",
    "print(\"xVal.shape = \", xVal.shape)\n",
    "print(\"yVal.shape = \", yVal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info on the `kernal_initializer`:  https://keras.io/initializers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T22:54:50.386508Z",
     "start_time": "2018-08-28T22:54:50.167719Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildModel(optimizer = 'Adam', lr = 0.001, decay = 0.0, epsilon = None, debug = False):\n",
    "    \n",
    "    opt = None\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # kernel_initializer='normal' -> Initializer capable of adapting its scale to the shape of weights\n",
    "    # bias_initializer -> 'zeros' (default per the docs) \n",
    "    model.add(Dense(20, kernel_initializer='normal', activation = 'relu'))\n",
    "    model.add(Dense(10, kernel_initializer='normal', activation = 'relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    if optimizer.lower() == 'adam':\n",
    "        opt = Adam(lr = lr, decay = decay, epsilon = epsilon)\n",
    "    else:\n",
    "        # Please don't ever use eval where you're recieving input from non-trusted sources!\n",
    "        # A Jupyter notebook is OK; a public facing service is certainly not\n",
    "        opt = eval(optimizer)()\n",
    "    \n",
    "    model.compile(loss = 'mean_squared_error', optimizer = opt)\n",
    "    \n",
    "    return model   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first pass an educated guess is taken for what might work well on the dataset.  This provides an initial baseline, and then hyperparameter tuning an occur to refine the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -17.67 (7.86)\n"
     ]
    }
   ],
   "source": [
    "# Define vars and init\n",
    "folds = 10\n",
    "seed = 10\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = KerasRegressor(build_fn = buildModel, epochs = 200, batch_size = 5, verbose = 0)\n",
    "kFold = KFold(n_splits = folds, random_state = seed)\n",
    "results = cross_val_score(model, xTrain, yTrain, cv = kFold)\n",
    "\n",
    "print(\"MSE: %.2f (%.2f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better then what the [previous](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.ipynb) write-up's models accomplished with no tuning as of yet:\n",
    "\n",
    "<pre>\n",
    "         Model    MSE  StdDev\n",
    "3    scaledKNN -20.35   11.87\n",
    "0     scaledLR -21.26    7.11\n",
    "4   scaledCART -22.66    9.31\n",
    "1  scaledLASSO -26.94   10.38\n",
    "5    scaledSVR -28.52   13.98\n",
    "2     scaledEN -28.60   11.65\n",
    "</pre>\n",
    "\n",
    "It does not; however, compare to the results achieved via the ensemble methods:\n",
    "\n",
    "<pre>\n",
    "       Model     MSE  StdDev\n",
    "1  scaledGBM -9.700   5.342 \n",
    "3  scaledET  -10.339  5.399 \n",
    "2  scaledRF  -13.695  7.276 \n",
    "0  scaledAB  -14.176  8.917\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a [previous write-up](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.Keras.1.ipynb) we utilized [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).  We'd like to now examine [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html).\n",
    "\n",
    "We observed in the last write-up that the `KerasRegressor` estimator utilizing the `Adam` optimizer give us good results.  We'll continue working with this combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alter tuneModel for RandomizedSearchCV support \n",
    "\n",
    "We need to alter the `tuneModel` function to support RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T21:00:50.821028Z",
     "start_time": "2018-08-28T21:00:50.555364Z"
    }
   },
   "outputs": [],
   "source": [
    "def tuneModel(modelName, modelObj, params, iterations = 20, returnModel = False, showSummary = True):\n",
    "    # Init vars and params\n",
    "    featureResults = {}\n",
    "    featureFolds = 10\n",
    "    featureSeed = 10\n",
    "    \n",
    "    np.random.seed(featureSeed)\n",
    "    \n",
    "    # Use MSE since this is a regression problem\n",
    "    score = 'neg_mean_squared_error'\n",
    "\n",
    "    # Create a Pandas DF to hold all our spiffy results\n",
    "    featureDF = DataFrame(columns = ['Model', 'MSE', 'StdDev', 'Best Params'])\n",
    "\n",
    "    # Create feature union (adding SelectKBest)\n",
    "    features = []\n",
    "    features.append(('Scaler', StandardScaler()))\n",
    "    features.append(('SelectKBest', SelectKBest()))\n",
    "    featureUnion = FeatureUnion(features)\n",
    "\n",
    "    # Search for the best combination of parameters\n",
    "    featureResults = RandomizedSearchCV(\n",
    "        Pipeline(\n",
    "            steps = [\n",
    "                ('FeatureUnion', featureUnion),\n",
    "                (modelName, modelObj)\n",
    "        ]),\n",
    "        param_distributions = params,\n",
    "        n_iter = iterations,\n",
    "        scoring = score,\n",
    "        cv = KFold(n_splits = featureFolds, random_state = featureSeed),\n",
    "        random_state = featureSeed\n",
    "    ).fit(xTrain, yTrain)\n",
    "\n",
    "    featureDF.loc[len(featureDF)] = list([\n",
    "        modelName, \n",
    "        featureResults.best_score_,\n",
    "        featureResults.cv_results_['std_test_score'][featureResults.best_index_],\n",
    "        featureResults.best_params_,\n",
    "    ])\n",
    "\n",
    "    if showSummary:\n",
    "        set_option('display.max_colwidth', -1)\n",
    "        display(featureDF)\n",
    "    \n",
    "    if returnModel:\n",
    "        return featureResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's dig in and see what sort of parameter combinations `RandomizedSearchCV` might be able to find for us that provide good algorithm performance.  If we would have utilized `GridSearchCV` as in the [previous write-up](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.Keras.1.ipynb) we'd probably be here all week waiting for the tuning process to finish.  ;)\n",
    "\n",
    "So, let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:20:26.574462Z",
     "start_time": "2018-08-28T17:59:33.228185Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>StdDev</th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>housingModel</td>\n",
       "      <td>-12.923</td>\n",
       "      <td>10.544</td>\n",
       "      <td>{'housingModel__optimizer': 'Adam', 'housingModel__lr': 0.005, 'housingModel__epsilon': 1.5, 'housingModel__epochs': 350, 'housingModel__batch_size': 36, 'FeatureUnion__SelectKBest__k': 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model     MSE  StdDev  \\\n",
       "0  housingModel -12.923  10.544   \n",
       "\n",
       "                                                                                                                                                                                    Best Params  \n",
       "0  {'housingModel__optimizer': 'Adam', 'housingModel__lr': 0.005, 'housingModel__epsilon': 1.5, 'housingModel__epochs': 350, 'housingModel__batch_size': 36, 'FeatureUnion__SelectKBest__k': 2}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelName = \"housingModel\"\n",
    "modelObj =  KerasRegressor(build_fn = buildModel, verbose = 0)\n",
    "params = {\n",
    "    'housingModel__optimizer' : ['Adam'],\n",
    "    'housingModel__epochs' : makeRange(200, 600, 50),\n",
    "    'housingModel__batch_size' : makeRange(4, 68, 4),\n",
    "    'FeatureUnion__SelectKBest__k': makeRange(1, xTrain.shape[1]),\n",
    "    'housingModel__lr' : makeRange(1, 9, 1, .001, 3),\n",
    "    'housingModel__epsilon' : makeRange(2, 8, 1, .5, 1),\n",
    "}\n",
    "\n",
    "set1 = tuneModel(modelName, modelObj, params, 10, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:20:26.777691Z",
     "start_time": "2018-08-28T19:20:26.574462Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_FeatureUnion__SelectKBest__k</th>\n",
       "      <th>param_housingModel__batch_size</th>\n",
       "      <th>param_housingModel__epochs</th>\n",
       "      <th>param_housingModel__epsilon</th>\n",
       "      <th>param_housingModel__lr</th>\n",
       "      <th>param_housingModel__optimizer</th>\n",
       "      <th>...</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.162</td>\n",
       "      <td>0.975</td>\n",
       "      <td>-12.923</td>\n",
       "      <td>-5.768</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>350</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.856</td>\n",
       "      <td>-5.547</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-6.600</td>\n",
       "      <td>-25.413</td>\n",
       "      <td>-4.548</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.054</td>\n",
       "      <td>10.544</td>\n",
       "      <td>0.755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.126</td>\n",
       "      <td>1.164</td>\n",
       "      <td>-13.062</td>\n",
       "      <td>-7.470</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>400</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.002</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.158</td>\n",
       "      <td>-6.962</td>\n",
       "      <td>-6.769</td>\n",
       "      <td>-7.158</td>\n",
       "      <td>-29.250</td>\n",
       "      <td>-5.741</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.050</td>\n",
       "      <td>9.324</td>\n",
       "      <td>0.706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81.680</td>\n",
       "      <td>1.363</td>\n",
       "      <td>-14.276</td>\n",
       "      <td>-7.638</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>550</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.006</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.706</td>\n",
       "      <td>-4.547</td>\n",
       "      <td>-6.938</td>\n",
       "      <td>-7.380</td>\n",
       "      <td>-31.150</td>\n",
       "      <td>-7.372</td>\n",
       "      <td>3.276</td>\n",
       "      <td>0.053</td>\n",
       "      <td>8.743</td>\n",
       "      <td>1.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>245.504</td>\n",
       "      <td>2.589</td>\n",
       "      <td>-14.861</td>\n",
       "      <td>-9.099</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>450</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-24.559</td>\n",
       "      <td>-9.178</td>\n",
       "      <td>-12.357</td>\n",
       "      <td>-7.707</td>\n",
       "      <td>-21.949</td>\n",
       "      <td>-6.987</td>\n",
       "      <td>4.468</td>\n",
       "      <td>0.074</td>\n",
       "      <td>5.186</td>\n",
       "      <td>2.525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21.193</td>\n",
       "      <td>2.366</td>\n",
       "      <td>-15.473</td>\n",
       "      <td>-8.751</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>250</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.002</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.321</td>\n",
       "      <td>-8.241</td>\n",
       "      <td>-8.194</td>\n",
       "      <td>-8.593</td>\n",
       "      <td>-30.608</td>\n",
       "      <td>-6.901</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.072</td>\n",
       "      <td>11.361</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.455</td>\n",
       "      <td>1.752</td>\n",
       "      <td>-15.506</td>\n",
       "      <td>-11.334</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>300</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.041</td>\n",
       "      <td>-11.962</td>\n",
       "      <td>-9.391</td>\n",
       "      <td>-10.604</td>\n",
       "      <td>-28.895</td>\n",
       "      <td>-9.480</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.065</td>\n",
       "      <td>7.814</td>\n",
       "      <td>2.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.434</td>\n",
       "      <td>1.562</td>\n",
       "      <td>-15.764</td>\n",
       "      <td>-12.118</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.848</td>\n",
       "      <td>-10.908</td>\n",
       "      <td>-8.697</td>\n",
       "      <td>-10.701</td>\n",
       "      <td>-28.050</td>\n",
       "      <td>-10.452</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.086</td>\n",
       "      <td>6.847</td>\n",
       "      <td>2.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.618</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-16.429</td>\n",
       "      <td>-10.603</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-37.575</td>\n",
       "      <td>-12.045</td>\n",
       "      <td>-9.254</td>\n",
       "      <td>-9.697</td>\n",
       "      <td>-36.836</td>\n",
       "      <td>-10.174</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.045</td>\n",
       "      <td>10.529</td>\n",
       "      <td>0.718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.628</td>\n",
       "      <td>2.328</td>\n",
       "      <td>-17.656</td>\n",
       "      <td>-13.179</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>200</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.712</td>\n",
       "      <td>-10.148</td>\n",
       "      <td>-9.390</td>\n",
       "      <td>-14.949</td>\n",
       "      <td>-43.857</td>\n",
       "      <td>-10.971</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.316</td>\n",
       "      <td>11.727</td>\n",
       "      <td>1.596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20.488</td>\n",
       "      <td>2.080</td>\n",
       "      <td>-18.634</td>\n",
       "      <td>-11.783</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>550</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.003</td>\n",
       "      <td>Adam</td>\n",
       "      <td>...</td>\n",
       "      <td>-36.326</td>\n",
       "      <td>-8.258</td>\n",
       "      <td>-11.253</td>\n",
       "      <td>-11.688</td>\n",
       "      <td>-42.961</td>\n",
       "      <td>-12.149</td>\n",
       "      <td>1.454</td>\n",
       "      <td>0.365</td>\n",
       "      <td>11.097</td>\n",
       "      <td>1.657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "1  12.162         0.975           -12.923          -5.768              \n",
       "2  13.126         1.164           -13.062          -7.470              \n",
       "3  81.680         1.363           -14.276          -7.638              \n",
       "9  245.504        2.589           -14.861          -9.099              \n",
       "8  21.193         2.366           -15.473          -8.751              \n",
       "5  15.455         1.752           -15.506          -11.334             \n",
       "4  15.434         1.562           -15.764          -12.118             \n",
       "0  22.618         0.830           -16.429          -10.603             \n",
       "7  16.628         2.328           -17.656          -13.179             \n",
       "6  20.488         2.080           -18.634          -11.783             \n",
       "\n",
       "  param_FeatureUnion__SelectKBest__k param_housingModel__batch_size  \\\n",
       "1  2                                  36                              \n",
       "2  2                                  44                              \n",
       "3  7                                  8                               \n",
       "9  10                                 4                               \n",
       "8  2                                  36                              \n",
       "5  3                                  44                              \n",
       "4  3                                  60                              \n",
       "0  9                                  16                              \n",
       "7  2                                  36                              \n",
       "6  7                                  64                              \n",
       "\n",
       "  param_housingModel__epochs param_housingModel__epsilon  \\\n",
       "1  350                        1.5                          \n",
       "2  400                        2.5                          \n",
       "3  550                        1.5                          \n",
       "9  450                        2                            \n",
       "8  250                        2.5                          \n",
       "5  300                        2                            \n",
       "4  450                        1                            \n",
       "0  400                        1                            \n",
       "7  200                        1.5                          \n",
       "6  550                        3.5                          \n",
       "\n",
       "  param_housingModel__lr param_housingModel__optimizer       ...         \\\n",
       "1  0.005                  Adam                               ...          \n",
       "2  0.002                  Adam                               ...          \n",
       "3  0.006                  Adam                               ...          \n",
       "9  0.001                  Adam                               ...          \n",
       "8  0.002                  Adam                               ...          \n",
       "5  0.002                  Adam                               ...          \n",
       "4  0.001                  Adam                               ...          \n",
       "0  0.001                  Adam                               ...          \n",
       "7  0.001                  Adam                               ...          \n",
       "6  0.003                  Adam                               ...          \n",
       "\n",
       "  split7_test_score  split7_train_score  split8_test_score  \\\n",
       "1 -39.856           -5.547              -9.200               \n",
       "2 -33.158           -6.962              -6.769               \n",
       "3 -30.706           -4.547              -6.938               \n",
       "9 -24.559           -9.178              -12.357              \n",
       "8 -43.321           -8.241              -8.194               \n",
       "5 -30.041           -11.962             -9.391               \n",
       "4 -23.848           -10.908             -8.697               \n",
       "0 -37.575           -12.045             -9.254               \n",
       "7 -31.712           -10.148             -9.390               \n",
       "6 -36.326           -8.258              -11.253              \n",
       "\n",
       "   split8_train_score  split9_test_score  split9_train_score  std_fit_time  \\\n",
       "1 -6.600              -25.413            -4.548               0.551          \n",
       "2 -7.158              -29.250            -5.741               0.490          \n",
       "3 -7.380              -31.150            -7.372               3.276          \n",
       "9 -7.707              -21.949            -6.987               4.468          \n",
       "8 -8.593              -30.608            -6.901               0.540          \n",
       "5 -10.604             -28.895            -9.480               0.539          \n",
       "4 -10.701             -28.050            -10.452              0.755          \n",
       "0 -9.697              -36.836            -10.174              0.619          \n",
       "7 -14.949             -43.857            -10.971              0.592          \n",
       "6 -11.688             -42.961            -12.149              1.454          \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "1  0.054           10.544          0.755            \n",
       "2  0.050           9.324           0.706            \n",
       "3  0.053           8.743           1.411            \n",
       "9  0.074           5.186           2.525            \n",
       "8  0.072           11.361          0.837            \n",
       "5  0.065           7.814           2.542            \n",
       "4  0.086           6.847           2.520            \n",
       "0  0.045           10.529          0.718            \n",
       "7  0.316           11.727          1.596            \n",
       "6  0.365           11.097          1.657            \n",
       "\n",
       "[10 rows x 36 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_option('precision', 3)\n",
    "DataFrame(set1.cv_results_).sort_values(by=['mean_test_score', 'std_test_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph performance of the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:20:27.136971Z",
     "start_time": "2018-08-28T19:20:26.965128Z"
    }
   },
   "source": [
    "#### Comments\n",
    "\n",
    "From the results above it's pretty clear we're likely at a point of diminishing returns for further tuning with this set of hyperparamter options. (I also tested a number of additional iterations outside of this notebook which further confirmed this conclusion.)\n",
    "\n",
    "Likely the next logical step would be to experiment with the topology of the neural network itself such as adding additional layers and/or neurons, or we could simply disregard neural networks as an options for this problem and utilize one of the effective algorithms covered in previous write-ups. So let's do a few rounds of topology modification testing next, make a final model choice, examine some predictions, and then wrap up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune network topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T20:55:44.328142Z",
     "start_time": "2018-08-28T20:55:39.411064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Model     MSE  StdDev\n",
      "0  housingModel -15.451  9.548 \n"
     ]
    }
   ],
   "source": [
    "# Init vars and params\n",
    "eModels = []\n",
    "eResults = {}\n",
    "eFolds = 10\n",
    "eSeed = 10\n",
    "\n",
    "# Create the model object && expose the defaults\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(20, 10), \n",
    "    activation='relu',\n",
    "    solver='adam', \n",
    "    alpha=0.0001, \n",
    "    batch_size=36, \n",
    "    learning_rate='constant', \n",
    "    learning_rate_init=0.005, \n",
    "    power_t=0.5, \n",
    "    max_iter=350, \n",
    "    #shuffle=True, \n",
    "    random_state=eSeed, \n",
    "    tol=0.0001, \n",
    "    verbose=False, \n",
    "    warm_start=False, \n",
    "    momentum=0.9, \n",
    "    nesterovs_momentum=True, \n",
    "    early_stopping=False, \n",
    "    #validation_fraction=0.0, \n",
    "    beta_1=0.9, \n",
    "    beta_2=0.999, \n",
    "    epsilon=1.5\n",
    ")\n",
    "\n",
    "# Create the pipeline to match what we had above\n",
    "pipe = Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    #('SelectKBest', SelectKBest(k = 7)),\n",
    "    ('housingModel', model),   \n",
    "])\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler().fit(xTrain)\n",
    "xTrainScaled = scaler.transform(xTrain)\n",
    "xTestScaled = scaler.transform(xVal)\n",
    "\n",
    "# Use MSE since this is a regression problem\n",
    "eScore = 'neg_mean_squared_error'\n",
    "\n",
    "# Create a Pandas DF to hold all our spiffy results\n",
    "eDF = DataFrame(columns = ['Model', 'MSE', 'StdDev'])\n",
    "\n",
    "# Train the model\n",
    "kFold = KFold(n_splits = eFolds, random_state = eSeed)\n",
    "eResults = cross_val_score(pipe, xTrain, yTrain, cv = kFold, scoring = eScore)\n",
    "eDF.loc[len(eDF)] = list([modelName, eResults.mean(), eResults.std()])\n",
    "\n",
    "print(eDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T22:51:22.102035Z",
     "start_time": "2018-08-28T22:51:21.867721Z"
    }
   },
   "outputs": [],
   "source": [
    "def tuneFinalModel(modelName, modelObj, params, iterations = 20, returnModel = False, showSummary = True):\n",
    "    # Init vars and params\n",
    "    featureResults = {}\n",
    "    featureFolds = 10\n",
    "    featureSeed = 10\n",
    "    \n",
    "    np.random.seed(featureSeed)\n",
    "    \n",
    "    # Use MSE since this is a regression problem\n",
    "    score = 'neg_mean_squared_error'\n",
    "\n",
    "    # Create a Pandas DF to hold all our spiffy results\n",
    "    featureDF = DataFrame(columns = ['Model', 'MSE', 'StdDev', 'Best Params'])\n",
    "\n",
    "    # Create feature union (adding SelectKBest)\n",
    "    features = []\n",
    "    features.append(('Scaler', StandardScaler()))\n",
    "    features.append(('SelectKBest', SelectKBest()))\n",
    "    featureUnion = FeatureUnion(features)\n",
    "\n",
    "    # Search for the best combination of parameters\n",
    "    featureResults = GridSearchCV(\n",
    "        Pipeline(\n",
    "            steps = [\n",
    "                ('FeatureUnion', featureUnion),\n",
    "                (modelName, modelObj)\n",
    "        ]),\n",
    "        param_grid = params,\n",
    "        scoring = score,\n",
    "        cv = KFold(n_splits = featureFolds, random_state = featureSeed),\n",
    "    ).fit(xTrain, yTrain)\n",
    "\n",
    "    featureDF.loc[len(featureDF)] = list([\n",
    "        modelName, \n",
    "        featureResults.best_score_,\n",
    "        featureResults.cv_results_['std_test_score'][featureResults.best_index_],\n",
    "        featureResults.best_params_,\n",
    "    ])\n",
    "\n",
    "    if showSummary:\n",
    "        set_option('display.max_colwidth', -1)\n",
    "        display(featureDF)\n",
    "    \n",
    "    if returnModel:\n",
    "        return featureResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T21:12:59.013310Z",
     "start_time": "2018-08-28T21:06:37.978926Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>StdDev</th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>housingModel</td>\n",
       "      <td>-13.386</td>\n",
       "      <td>10.404</td>\n",
       "      <td>{'FeatureUnion__SelectKBest__k': 2, 'housingModel__batch_size': 36, 'housingModel__epochs': 350, 'housingModel__epsilon': 1.5, 'housingModel__lr': 0.005, 'housingModel__optimizer': 'Adam'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model     MSE  StdDev  \\\n",
       "0  housingModel -13.386  10.404   \n",
       "\n",
       "                                                                                                                                                                                    Best Params  \n",
       "0  {'FeatureUnion__SelectKBest__k': 2, 'housingModel__batch_size': 36, 'housingModel__epochs': 350, 'housingModel__epsilon': 1.5, 'housingModel__lr': 0.005, 'housingModel__optimizer': 'Adam'}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finalModelName = \"housingModel\"\n",
    "finalModelObj =  KerasRegressor(build_fn = buildModel, verbose = 0)\n",
    "finalParams = {\n",
    "    'housingModel__optimizer' : ['Adam'],\n",
    "    'housingModel__epochs' : [350],\n",
    "    'housingModel__batch_size' : [36],\n",
    "    'FeatureUnion__SelectKBest__k': [2],\n",
    "    'housingModel__lr' : [0.005],\n",
    "    'housingModel__epsilon' : [1.5],\n",
    "}\n",
    "\n",
    "final = tuneFinalModel(finalModelName, finalModelObj, finalParams, 2, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T19:20:26.965128Z",
     "start_time": "2018-08-28T19:20:26.777691Z"
    }
   },
   "source": [
    "#### Graph final model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T23:29:24.621755Z",
     "start_time": "2018-08-28T23:29:24.387912Z"
    }
   },
   "outputs": [],
   "source": [
    "def wrapper(optimizer = 'Adam', lr = 0.001, decay = 0.0, epsilon = None):\n",
    "    \n",
    "    def buildModel():\n",
    "        opt = None\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_dim = 7, kernel_initializer='normal', activation = 'relu'))\n",
    "        model.add(Dense(10, kernel_initializer='normal', activation = 'relu'))\n",
    "        model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "        if optimizer.lower() == 'adam':\n",
    "            opt = Adam(lr = lr, decay = decay, epsilon = epsilon)\n",
    "        else:\n",
    "            opt = eval(optimizer)()\n",
    "\n",
    "        model.compile(loss = 'mean_squared_error', optimizer = opt)\n",
    "\n",
    "        return model\n",
    "\n",
    "    return buildModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T23:05:15.135499Z",
     "start_time": "2018-08-28T22:57:02.440076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Model     MSE  StdDev\n",
      "0  finalModel -12.803  9.607 \n"
     ]
    }
   ],
   "source": [
    "# Init vars and params\n",
    "folds = 10\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "modelName = 'finalModel'\n",
    "\n",
    "# Use MSE since this is a regression problem\n",
    "score = 'neg_mean_squared_error'\n",
    "\n",
    "# Create a Pandas DF to hold all our spiffy results\n",
    "df = DataFrame(columns = ['Model', 'MSE', 'StdDev'])\n",
    "\n",
    "# Build the model\n",
    "model = KerasRegressor(\n",
    "    build_fn = wrapper(\n",
    "        optimizer = 'Adam', \n",
    "        lr = 0.005, \n",
    "        epsilon = 1.5\n",
    "    ), \n",
    "    epochs = 350,\n",
    "    batch_size = 36,\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "# Create feature union\n",
    "features = []\n",
    "features.append(('Scaler', StandardScaler()))\n",
    "features.append(('SelectKBest', SelectKBest( k = 2)))\n",
    "featureUnion = FeatureUnion(features)\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = Pipeline(\n",
    "    steps = [\n",
    "        ('FeatureUnion', featureUnion),\n",
    "        (modelName, model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model and records results\n",
    "kFold = KFold(n_splits = folds, random_state = seed)\n",
    "results = cross_val_score(pipe, xTrain, yTrain, cv = kFold, scoring = score)\n",
    "df.loc[len(df)] = list([modelName, results.mean(), results.std()])\n",
    "\n",
    "# Print results\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "#history = History()\n",
    "#history = model.fit(xTrain, yTrain, callbacks=[history])\n",
    "#history = model.fit(xTrain, yTrain)\n",
    "\n",
    "#model.fit(xTrain, yTrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to put the pipeline steps in own lines\n",
    "fit the model on train -> history -> graphing\n",
    "\n",
    "fit model w/out cv and all that on train\n",
    "predict test\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T23:27:57.622974Z",
     "start_time": "2018-08-28T23:19:25.832428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Model     MSE  StdDev\n",
      "0  finalModel -23.755  10.682\n"
     ]
    }
   ],
   "source": [
    "# Init vars and params\n",
    "folds = 10\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "modelName = 'finalModel'\n",
    "\n",
    "# Use MSE since this is a regression problem\n",
    "score = 'neg_mean_squared_error'\n",
    "\n",
    "# Create a Pandas DF to hold all our spiffy results\n",
    "df = DataFrame(columns = ['Model', 'MSE', 'StdDev'])\n",
    "\n",
    "# Build the model\n",
    "model = KerasRegressor(\n",
    "    build_fn = wrapper(\n",
    "        optimizer = 'Adam', \n",
    "        lr = 0.005, \n",
    "        epsilon = 1.5\n",
    "    ), \n",
    "    epochs = 350,\n",
    "    batch_size = 36,\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler().fit(xTrain)\n",
    "xTrainScaled = scaler.transform(xTrain)\n",
    "xTestScaled = scaler.transform(xVal)\n",
    "\n",
    "select = SelectKBest(k = 2).fit(xTrainScaled, yTrain)\n",
    "xTrainK = select.fit_transform(xTrainScaled, yTrain)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#features.append(('Scaler', StandardScaler()))\n",
    "#features.append(('SelectKBest', SelectKBest( k = 2)))\n",
    "#featureUnion = FeatureUnion(features)\n",
    "\n",
    "# Create the pipeline\n",
    "#pipe = Pipeline(\n",
    "#    steps = [\n",
    "#        ('FeatureUnion', featureUnion),\n",
    "#        (modelName, model)\n",
    "#    ]\n",
    "#)\n",
    "\n",
    "\n",
    "# Train the model and records results\n",
    "kFold = KFold(n_splits = folds, random_state = seed)\n",
    "results = cross_val_score(model, xTrainK, yTrain, cv = kFold, scoring = score)\n",
    "#results = cross_val_score(pipe, xTrain, yTrain, cv = kFold, scoring = score)\n",
    "df.loc[len(df)] = list([modelName, results.mean(), results.std()])\n",
    "\n",
    "# Print results\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "#history = History()\n",
    "#history = model.fit(xTrain, yTrain, callbacks=[history])\n",
    "#history = model.fit(xTrain, yTrain)\n",
    "\n",
    "#model.fit(xTrain, yTrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T23:38:36.094171Z",
     "start_time": "2018-08-28T23:29:35.166250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Model     MSE  StdDev\n",
      "0  finalModel -15.327  10.448\n"
     ]
    }
   ],
   "source": [
    "# Init vars and params\n",
    "folds = 10\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "modelName = 'finalModel'\n",
    "\n",
    "# Use MSE since this is a regression problem\n",
    "score = 'neg_mean_squared_error'\n",
    "\n",
    "# Create a Pandas DF to hold all our spiffy results\n",
    "df = DataFrame(columns = ['Model', 'MSE', 'StdDev'])\n",
    "\n",
    "# Build the model\n",
    "model = KerasRegressor(\n",
    "    build_fn = wrapper(\n",
    "        optimizer = 'Adam', \n",
    "        lr = 0.005, \n",
    "        epsilon = 1.5\n",
    "    ), \n",
    "    epochs = 350,\n",
    "    batch_size = 36,\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler().fit(xTrain)\n",
    "xTrainScaled = scaler.transform(xTrain)\n",
    "xTestScaled = scaler.transform(xVal)\n",
    "\n",
    "select = SelectKBest(k = 7).fit(xTrainScaled, yTrain)\n",
    "xTrainK = select.fit_transform(xTrainScaled, yTrain)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#features.append(('Scaler', StandardScaler()))\n",
    "#features.append(('SelectKBest', SelectKBest( k = 2)))\n",
    "#featureUnion = FeatureUnion(features)\n",
    "\n",
    "# Create the pipeline\n",
    "#pipe = Pipeline(\n",
    "#    steps = [\n",
    "#        ('FeatureUnion', featureUnion),\n",
    "#        (modelName, model)\n",
    "#    ]\n",
    "#)\n",
    "\n",
    "\n",
    "# Train the model and records results\n",
    "kFold = KFold(n_splits = folds, random_state = seed)\n",
    "results = cross_val_score(model, xTrainK, yTrain, cv = kFold, scoring = score)\n",
    "#results = cross_val_score(pipe, xTrain, yTrain, cv = kFold, scoring = score)\n",
    "df.loc[len(df)] = list([modelName, results.mean(), results.std()])\n",
    "\n",
    "# Print results\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "#history = History()\n",
    "#history = model.fit(xTrain, yTrain, callbacks=[history])\n",
    "#history = model.fit(xTrain, yTrain)\n",
    "\n",
    "#model.fit(xTrain, yTrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T23:38:36.500422Z",
     "start_time": "2018-08-28T23:38:36.094171Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (102,13) (7,) (102,13) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-238-9ea6b00a8e0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxTrainK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mxTrainScaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxTrainK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mxTestScaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxVal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nathan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, y, copy)\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                 \u001b[0mX\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (102,13) (7,) (102,13) "
     ]
    }
   ],
   "source": [
    "# Init vars and params\n",
    "folds = 10\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "modelName = 'finalModel'\n",
    "\n",
    "# Use MSE since this is a regression problem\n",
    "score = 'neg_mean_squared_error'\n",
    "\n",
    "# Create a Pandas DF to hold all our spiffy results\n",
    "df = DataFrame(columns = ['Model', 'MSE', 'StdDev'])\n",
    "\n",
    "# Build the model\n",
    "model = KerasRegressor(\n",
    "    build_fn = wrapper(\n",
    "        optimizer = 'Adam', \n",
    "        lr = 0.005, \n",
    "        epsilon = 1.5\n",
    "    ), \n",
    "    epochs = 350,\n",
    "    batch_size = 36,\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "# Scale the data\n",
    "\n",
    "select = SelectKBest(k = 7).fit(xTrain, yTrain)\n",
    "xTrainK = select.fit_transform(xTrain, yTrain)\n",
    "\n",
    "scaler = StandardScaler().fit(xTrainK)\n",
    "xTrainScaled = scaler.transform(xTrainK)\n",
    "xTestScaled = scaler.transform(xVal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#features.append(('Scaler', StandardScaler()))\n",
    "#features.append(('SelectKBest', SelectKBest( k = 2)))\n",
    "#featureUnion = FeatureUnion(features)\n",
    "\n",
    "# Create the pipeline\n",
    "#pipe = Pipeline(\n",
    "#    steps = [\n",
    "#        ('FeatureUnion', featureUnion),\n",
    "#        (modelName, model)\n",
    "#    ]\n",
    "#)\n",
    "\n",
    "\n",
    "# Train the model and records results\n",
    "kFold = KFold(n_splits = folds, random_state = seed)\n",
    "results = cross_val_score(model, xTrainK, yTrain, cv = kFold, scoring = score)\n",
    "#results = cross_val_score(pipe, xTrain, yTrain, cv = kFold, scoring = score)\n",
    "df.loc[len(df)] = list([modelName, results.mean(), results.std()])\n",
    "\n",
    "# Print results\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "#history = History()\n",
    "#history = model.fit(xTrain, yTrain, callbacks=[history])\n",
    "#history = model.fit(xTrain, yTrain)\n",
    "\n",
    "#model.fit(xTrain, yTrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T22:03:48.533228Z",
     "start_time": "2018-08-28T22:03:45.306632Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model.predict(xVal)\n",
    "mse = mean_squared_error(yVal, preds)\n",
    "rmse = sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T22:03:56.890058Z",
     "start_time": "2018-08-28T22:03:56.702529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE =  25.27281203166481\n",
      "RMSE =  5.027207180101573\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE = \", mse)\n",
    "print(\"RMSE = \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T22:06:11.937912Z",
     "start_time": "2018-08-28T22:06:11.766098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 36,\n",
       " 'build_fn': <function __main__.buildModel>,\n",
       " 'epochs': 350,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "Again, if we were creating a production quality model we would have started with randomized parameter optimization process.  The results from that process would then lead to a set of smaller grids focusing more and more on whatever parameter option permutations showed the most promise.\n",
    "\n",
    "You can see an example of this type of process I worked on previously [here](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/03-ComputerVision-Classification/Classification-03.ipynb).\n",
    "\n",
    "Also, unless the randomized parameter optimization process were to lead to signifigant improvements from what we've seen so far we'd be better of utilizing the gradient boosting algorithm we utilized in [previous write-up](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/04-Classic-Datasets/Model-02.ipynb#Initial-pass---Ensemble-methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "Hopefully to same some one else some pain down the road:\n",
    "\n",
    "I kept getting the following error when working on this prediction section, which frankly was driving me nuts:\n",
    "    \n",
    "```\n",
    "TypeError: call() missing 1 required positional argument: 'inputs'\n",
    "```\n",
    "\n",
    "After researching the error message I came upon this comment which let me to the resolution:\n",
    "\n",
    "_The thing here is that KerasRegressor expects a callable that builds a model, rather than the model itself. By wrapping your function in this way you can return the build function (without calling it)._  [Source](https://stackoverflow.com/questions/47944463/specify-input-argument-with-kerasregressor)\n",
    "\n",
    "Solution:  I needed to **wrap** the `buildModel()` function!  :(\n",
    "\n",
    "Once I 'wrapped' the `buildModel()` function the prediction code blocks finally started working, and that's why we have the `wrapper()` function implemented below...\n",
    "\n",
    "**END NOTE**\n",
    "\n",
    "And now that that's out of the way we'll take a look at some predictions using the test data set based on the tuning results from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See NOTE above on why we have this new function\n",
    "def wrapper(optimizer = 'Adam', lr = 0.001, decay = 0.0, epsilon = None):\n",
    "    \n",
    "    def buildModel():\n",
    "        opt = None\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # kernel_initializer='normal' -> Initializer capable of adapting its scale to the shape of weights\n",
    "        # bias_initializer -> 'zeros' (default per the docs)\n",
    "\n",
    "        model.add(Dense(20, input_dim = xTrain.shape[1], kernel_initializer='normal', activation = 'relu'))\n",
    "        model.add(Dense(10, kernel_initializer='normal', activation = 'relu'))\n",
    "        model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "        if optimizer.lower() == 'adam':\n",
    "            opt = Adam(lr = lr, decay = decay, epsilon = epsilon)\n",
    "        else:\n",
    "            # Please don't ever use eval where you're recieving input from non-trusted sources!\n",
    "            # A Jupyter notebook is OK; a public facing service is certainly not\n",
    "            opt = eval(optimizer)()\n",
    "\n",
    "        model.compile(loss = 'mean_squared_error', optimizer = opt)\n",
    "\n",
    "        return model\n",
    "\n",
    "    return buildModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE =  11.93922294223878\n",
      "RMSE =  3.4553180667253747\n"
     ]
    }
   ],
   "source": [
    "# Build the model, and pass the KerasRegressor a callable function to the 'build_fn' argument\n",
    "# Use the parameters we found were most effective during the hyperparameter tuning\n",
    "m =  KerasRegressor(\n",
    "    build_fn = wrapper(optimizer = 'Adam', lr = 0.003, epsilon = 1), \n",
    "    epochs = 300, \n",
    "    batch_size = 32, \n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "# Now fit the model to the training data ensuring we perform the same sort of pipeline transformations\n",
    "# that occured during the hyperparameter tuning (i.e. feature scaling)\n",
    "xScaled = StandardScaler().fit(xTrain).transform(xTrain)\n",
    "m.fit(xScaled, yTrain)\n",
    "\n",
    "# Now we can finally make some predictions using our trained model on unseen data\n",
    "xScaled = StandardScaler().fit(xTrain).transform(xVal)\n",
    "preds = m.predict(xScaled)\n",
    "mse = mean_squared_error(yVal, preds)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "print(\"MSE = \", mse)\n",
    "print(\"RMSE = \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T01:52:09.339400Z",
     "start_time": "2018-08-27T01:52:09.114369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makeRange(4, 68, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T01:50:45.294455Z",
     "start_time": "2018-08-27T01:50:45.040458Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-3715696215e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxTrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'col'"
     ]
    }
   ],
   "source": [
    "xTrain.col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T02:23:35.274301Z",
     "start_time": "2018-08-27T02:23:35.055287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makeRange(1, 9, 1, .001, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-27T02:29:43.169231Z",
     "start_time": "2018-08-27T02:29:42.949309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.5, 2.0, 2.5, 3.0, 3.5]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makeRange(2, 8, 1, .5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T22:44:52.794003Z",
     "start_time": "2018-08-28T22:44:52.606469Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(a1, a2, a3):\n",
    "    print(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T22:44:53.840847Z",
     "start_time": "2018-08-28T22:44:53.668944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a3': 'abc', 'a2': 2, 'a1': 1}\n"
     ]
    }
   ],
   "source": [
    "test(1, 2, 'abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "notify_time": "30",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
