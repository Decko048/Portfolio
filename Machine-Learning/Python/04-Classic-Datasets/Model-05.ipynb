{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#MNIST-Classification-with-a-Long-Short-Term-Memory-(LSTM)-network\" data-toc-modified-id=\"MNIST-Classification-with-a-Long-Short-Term-Memory-(LSTM)-network-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MNIST Classification with a Long Short Term Memory (LSTM) network</a></span></li><li><span><a href=\"#Init-vars\" data-toc-modified-id=\"Init-vars-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Init vars</a></span></li><li><span><a href=\"#Build-the-computational-graph\" data-toc-modified-id=\"Build-the-computational-graph-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Build the computational graph</a></span><ul class=\"toc-item\"><li><span><a href=\"#ELU-model\" data-toc-modified-id=\"ELU-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>ELU model</a></span></li><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Second-Run\" data-toc-modified-id=\"TensorBoard---Second-Run-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>TensorBoard - Second Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Third-Run\" data-toc-modified-id=\"TensorBoard---Third-Run-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>TensorBoard - Third Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Fourth-Run\" data-toc-modified-id=\"TensorBoard---Fourth-Run-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>TensorBoard - Fourth Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Fifth-Run\" data-toc-modified-id=\"TensorBoard---Fifth-Run-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>TensorBoard - Fifth Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MNIST Classification with a Long Short Term Memory (LSTM) network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T21:53:39.078786Z",
     "start_time": "2018-09-17T21:53:39.075786Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T21:53:44.170786Z",
     "start_time": "2018-09-17T21:53:43.708786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T21:53:52.954786Z",
     "start_time": "2018-09-17T21:53:52.949786Z"
    }
   },
   "outputs": [],
   "source": [
    "def resetGraph(seed= 10):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T21:53:54.566786Z",
     "start_time": "2018-09-17T21:53:54.562786Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanLogs():\n",
    "    os.system('rm -rf ./logs/dnnTensorBoard/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init vars\n",
    "\n",
    "The LSTM wants inputs of shape `[batchSize, timeSteps, numberOfInputs]`, and we have several thousand MNIST images of size 28 x 28 pixels.  \n",
    "\n",
    "One way to think of this is a complete image is comprised of 28 rows of 28 pixels each.  If we were to step through the rows one by one and stack them up then the image would be more and more complete as time went by.  So our units of \"time\" will be the rows stacking together to create a complete image, and the number of inputs will be the number of pixels in the image row at that step in time (i.e. 28).  This gives us:\n",
    "\n",
    "* batchSize       = number of observations (i.e. number of images in the mini batch)\n",
    "* timeSteps       = number of rows we need to step through/stack up to make a complete image\n",
    "* numberOfInputs  = the number of features in each row we are stepping through (i.e. also 28)\n",
    "\n",
    "Additionally, we only care about the final output of the LSTM network which should give us the prediction of which numeral the image represents.  Other LSTM networks do care about the outputs of each LSTM cell (translating each word in a sentence for example), but that doesn't apply in our case.\n",
    "\n",
    "Having said this we can continue with initializing the various variables we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:54:58.125183Z",
     "start_time": "2018-09-17T22:54:57.652183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Setup vars for the MINST data set\n",
    "timeSteps = 28\n",
    "numOfInputs = 28\n",
    "\n",
    "lstmUnits = 128\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batchSize = 50\n",
    "\n",
    "numOfClasses = 10\n",
    "\n",
    "mninst = input_data.read_data_sets(\"./datasets/mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-07T20:47:13.613722Z",
     "start_time": "2018-09-07T20:47:13.609737Z"
    }
   },
   "source": [
    "# Build the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:55:05.563183Z",
     "start_time": "2018-09-17T22:55:05.142183Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ouput' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7edec6131270>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# We only care about the final output which should be the model's prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0myH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# Add loss function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ouput' is not defined"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir\n",
    "logDir = './logs/mnistLSTM/runOne/train'\n",
    "\n",
    "# Create place holders\n",
    "x = tf.placeholder(tf.float32, shape = [None, timeSteps, numOfInputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None, numOfClasses], name = 'y')\n",
    "\n",
    "# Create weights and bias tensors\n",
    "with tf.name_scope(\"weightBias\"):\n",
    "    # Todo: Better init!!\n",
    "    w = tf.Variable(tf.random_normal([lstmUnits, numOfClasses]))\n",
    "    b = tf.Variable(tf.random_normal([numOfClasses]))\n",
    "\n",
    "\n",
    "# Add the LSTM cells\n",
    "with tf.name_scope(\"LSTM\"):\n",
    "    \n",
    "    # Later in the code we'll make a call to tf.contrib.rnn.static_rnn\n",
    "    # tf.contrib.rnn.static_rnn expects a length T list of inputs, each a Tensor of shape [batch_size, input_size]\n",
    "    # So we need to convert our inputs of shape [batchSize, timeSteps, numberOfInputs] to [batch_size, input_size]\n",
    "    #\n",
    "    # https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/static_rnn\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/unstack\n",
    "    inputs = tf.unstack(x, num = timeSteps, axis = 1)\n",
    "    \n",
    "    # Create the basic LSTM cell\n",
    "    # It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline.\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "    \n",
    "    # Add the cell to the RNN\n",
    "    # https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/static_rnn\n",
    "    output, state = tf.contrib.rnn.static_rnn(cell, inputs, dtype = tf.float32)\n",
    "    \n",
    "    # We only care about the final output which should be the model's prediction\n",
    "    yH = tf.matmul(ouput[-1], w) + b\n",
    "    \n",
    "# Add loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits = yH, labels = y)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.equal(tf.argmax(yH, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Test Acc:  0.9039\n",
      "10 Train Acc:  0.98 Test Acc:  0.9598\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9707\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDir, sess.graph)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                merge = tf.summary.merge_all()\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:54.082231Z",
     "start_time": "2018-09-09T05:14:53.792180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Set the TB logdir\n",
    "logDir = './logs/dnnTensorBoard/elu/train'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # Use ELU\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.elu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.elu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.114258Z",
     "start_time": "2018-09-09T05:14:56.169295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.94 Test Acc:  0.9004\n",
      "10 Train Acc:  1.0 Test Acc:  0.9471\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9612\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDir, sess.graph)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                merge = tf.summary.merge_all()\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB First Run](./images/tb-1st-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard - Second Run\n",
    "\n",
    "Execute a single model, and capture loss and accuracy on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.96 Test Acc:  0.9021\n",
      "10 Train Acc:  0.96 Test Acc:  0.9593\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.97\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/secondRun/train'\n",
    "logDirTest = './logs/dnnTensorBoard/secondRun/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB First Run](./images/tb-2nd-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard - Third Run\n",
    "\n",
    "Execute a single model; capture loss and accuracy on the same plot; and capture metadata such as memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Test Acc:  0.903\n",
      "10 Train Acc:  1.0 Test Acc:  0.9594\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9701\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/thirdRun/train'\n",
    "logDirTest = './logs/dnnTensorBoard/thirdRun/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Generate and capture metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run(\n",
    "                    [merge, opt], \n",
    "                    feed_dict = {x: xBatch, y: yBatch},\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata\n",
    "                )\n",
    "                \n",
    "                # Write metadata\n",
    "                trainWriter.add_run_metadata(run_metadata, 'step%03d' % counter)\n",
    "                \n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "            else:\n",
    "                # Train as normal\n",
    "                summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB Third Run](./images/tb-3rd-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard - Fourth Run\n",
    "\n",
    "Execute a single model multiple times with various hyperparameters; capture loss and accuracy on the same plot; and capture metadata such as memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.88 Test Acc:  0.905\n",
      "10 Train Acc:  0.96 Test Acc:  0.9598\n",
      " \n",
      "FINAL ::  Train Acc:  0.94 Test Acc:  0.9709\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batchSize = 50\n",
    "\n",
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/fourthRun/1/train'\n",
    "logDirTest = './logs/dnnTensorBoard/fourthRun/1/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Generate and capture metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run(\n",
    "                    [merge, opt], \n",
    "                    feed_dict = {x: xBatch, y: yBatch},\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata\n",
    "                )\n",
    "                \n",
    "                # Write metadata\n",
    "                trainWriter.add_run_metadata(run_metadata, 'step%03d' % counter)\n",
    "                \n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "            else:\n",
    "                # Train as normal\n",
    "                summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.53 Test Acc:  0.552\n",
      "10 Train Acc:  0.91 Test Acc:  0.8856\n",
      "20 Train Acc:  0.94 Test Acc:  0.9062\n",
      " \n",
      "FINAL ::  Train Acc:  0.91 Test Acc:  0.9156\n"
     ]
    }
   ],
   "source": [
    "# Alter hyperparams for second run\n",
    "lr = 0.001\n",
    "epochs = 30\n",
    "batchSize = 100\n",
    "\n",
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/fourthRun/2/train'\n",
    "logDirTest = './logs/dnnTensorBoard/fourthRun/2/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Generate and capture metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run(\n",
    "                    [merge, opt], \n",
    "                    feed_dict = {x: xBatch, y: yBatch},\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata\n",
    "                )\n",
    "                \n",
    "                # Write metadata\n",
    "                trainWriter.add_run_metadata(run_metadata, 'step%03d' % counter)\n",
    "                \n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "            else:\n",
    "                # Train as normal\n",
    "                summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB Fourth Run](./images/tb-4th-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard - Fifth Run\n",
    "\n",
    "Execute a single model; capture loss and accuracy on the same plot; capture metadata such as memory consumption; and capture the each layer's bias, weights, and activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T16:15:15.073150Z",
     "start_time": "2018-09-13T16:14:32.373150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name hLayerOne/kernel:0 is illegal; using hLayerOne/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name hLayerOne/bias:0 is illegal; using hLayerOne/bias_0 instead.\n",
      "INFO:tensorflow:Summary name hLayerTwo/kernel:0 is illegal; using hLayerTwo/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name hLayerTwo/bias:0 is illegal; using hLayerTwo/bias_0 instead.\n",
      "INFO:tensorflow:Summary name yH/kernel:0 is illegal; using yH/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name yH/bias:0 is illegal; using yH/bias_0 instead.\n",
      "0 Train Acc:  0.96 Test Acc:  0.9056\n",
      "10 Train Acc:  0.98 Test Acc:  0.9624\n",
      " \n",
      "FINAL ::  Train Acc:  0.96 Test Acc:  0.9705\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/fifthRun/train'\n",
    "logDirTest = './logs/dnnTensorBoard/fifthRun/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    # Create the first hidden layer\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    \n",
    "    # Record the weights of bias of the layer and feed to the summary\n",
    "    # This is the verbose method....\n",
    "    with tf.variable_scope(\"hLayerOne\", reuse=True):\n",
    "        weights = tf.get_variable(\"kernel\")\n",
    "        bias = tf.get_variable(\"bias\")\n",
    "        tf.summary.histogram(\"layer1Weights\", weights)\n",
    "        tf.summary.histogram(\"layer1Bias\", bias)\n",
    "    \n",
    "    \n",
    "    # Create the second hidden layer\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    \n",
    "    # Record the weights of bias of the layer and feed to the summary\n",
    "    # And this time we'll do it the \"easy\" way\n",
    "    tf.contrib.layers.summarize_activation(layer2)\n",
    "    \n",
    "    \n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")  \n",
    "    # And the easy way again!  ;)\n",
    "    tf.contrib.layers.summarize_activation(yH)\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    # Capture predictions and activation\n",
    "    tf.summary.histogram(\"outputActivations\", entropy)\n",
    "    \n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.name, var)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Generate and capture metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run(\n",
    "                    [merge, opt], \n",
    "                    feed_dict = {x: xBatch, y: yBatch},\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata\n",
    "                )\n",
    "                \n",
    "                # Write metadata\n",
    "                trainWriter.add_run_metadata(run_metadata, 'step%03d' % counter)\n",
    "                \n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "            else:\n",
    "                # Train as normal\n",
    "                summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB First Run](./images/tb-5th-run.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
