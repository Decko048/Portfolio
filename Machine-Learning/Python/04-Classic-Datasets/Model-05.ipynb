{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#MNIST-Classification-with-a-Long-Short-Term-Memory-(LSTM)-network\" data-toc-modified-id=\"MNIST-Classification-with-a-Long-Short-Term-Memory-(LSTM)-network-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MNIST Classification with a Long Short Term Memory (LSTM) network</a></span></li><li><span><a href=\"#Init-vars\" data-toc-modified-id=\"Init-vars-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Init vars</a></span></li><li><span><a href=\"#Build-the-computational-graph\" data-toc-modified-id=\"Build-the-computational-graph-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Build the computational graph</a></span><ul class=\"toc-item\"><li><span><a href=\"#ELU-model\" data-toc-modified-id=\"ELU-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>ELU model</a></span></li><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Second-Run\" data-toc-modified-id=\"TensorBoard---Second-Run-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>TensorBoard - Second Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Third-Run\" data-toc-modified-id=\"TensorBoard---Third-Run-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>TensorBoard - Third Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Fourth-Run\" data-toc-modified-id=\"TensorBoard---Fourth-Run-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>TensorBoard - Fourth Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Fifth-Run\" data-toc-modified-id=\"TensorBoard---Fifth-Run-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>TensorBoard - Fifth Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MNIST classification with a Long Short Term Memory (LSTM) network</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px; width: 30%; height: 30%;\" src=\"images/mnist-image.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The purpose of this write-up is create a predictive classification model utilizing a Long Short Term Memory (LSTM) network written in TensorFlow.    \n",
    "\n",
    "Goals include:\n",
    "* Build a LSTM predictive regression model\n",
    "* Collect and graph model performance via TensorBoard \n",
    "* Make predictions with the training model on the test data set and examine accuracy\n",
    "\n",
    "Dataset source:  [The MNIST Database](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:08:44.796666Z",
     "start_time": "2018-09-18T21:08:44.793666Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:08:44.914666Z",
     "start_time": "2018-09-18T21:08:44.798666Z"
    }
   },
   "outputs": [],
   "source": [
    "def resetGraph(seed= 10):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    accTrain = accValidation = accTest = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:08:45.153666Z",
     "start_time": "2018-09-18T21:08:44.915666Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanLogs():\n",
    "    os.system('rm -rf ./logs/mnistLSTM/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init vars\n",
    "\n",
    "The LSTM wants inputs of shape `[samples, timeSteps, features]`, and we have several thousand MNIST images of size 28 x 28 pixels.  \n",
    "\n",
    "One way to think of this is a complete image is comprised of 28 rows of 28 pixels each.  If we were to step through the rows one by one and stack them up then the image would be more and more complete as time went by.  So our units of \"time\" will be the rows stacking together to create a complete image, and the number of features will be the number of pixels in the image row at that step in time (i.e. 28).  This gives us:\n",
    "\n",
    "* samples     = number of observations (i.e. number of images in the mini batch)\n",
    "* timeSteps   = number of rows we need to step through/stack up to make a complete image\n",
    "* features    = the number of features in each row we are stepping through (i.e. also 28)\n",
    "\n",
    "Additionally, we only care about the final output of the LSTM network which should give us the prediction of which numeral the image represents.  Other LSTM networks do care about the outputs of each LSTM cell (translating each word in a sentence for example), but that doesn't apply in our case.\n",
    "\n",
    "Having said this we can continue with initializing the various variables we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:10:06.564666Z",
     "start_time": "2018-09-18T21:10:06.098666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "\n",
      " Example label:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Setup vars for the MINST data set\n",
    "timeSteps = 28\n",
    "features = 28\n",
    "\n",
    "lstmUnits = 128\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "samples = 50\n",
    "\n",
    "classes = 10\n",
    "\n",
    "# Allow results to be reproduced\n",
    "seed = 10\n",
    "\n",
    "# Notice we are pulling in the labels as one hot encodings!\n",
    "mninst = input_data.read_data_sets(\"./datasets/mnist\", one_hot = True)\n",
    "\n",
    "# For use when we create the LSTM network below\n",
    "testShape = mninst.test.images.shape\n",
    "\n",
    "# Note the one hot encoding on the label:\n",
    "print(\"\\n\", \"Example label: \", mninst.test.labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-07T20:47:13.613722Z",
     "start_time": "2018-09-07T20:47:13.609737Z"
    }
   },
   "source": [
    "# Build the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static vs Dynamic TensorFlow RNNs\n",
    "\n",
    "> Dynamic RNN's allow for variable sequence lengths. You might have an input shape (batch_size, max_sequence_length), but this will allow you to run the RNN for the correct number of time steps on those sequences that are shorter than max_sequence_length.\n",
    " \n",
    "> In contrast, there are static RNNs, which expect to run the entire fixed RNN length. There are cases where you might prefer to do this, such as if you are padding your inputs to max_sequence_length anyway.\n",
    "\n",
    ">In short, dynamic_rnn is usually what you want for variable length sequential data. It has a sequence_length parameter, and it is your friend.  [Source](https://stackoverflow.com/questions/43100981/what-is-a-dynamic-rnn-in-tensorflow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM v1\n",
    "\n",
    "* Utilize f.contrib.rnn.BasicLSTMCell\n",
    "* Utilize tf.contrib.rnn.static_rnn\n",
    "* Manual weight and bias definitions with tf.random_normal for initialization\n",
    "* Track training and validiation loss and accuracy in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:15:16.094666Z",
     "start_time": "2018-09-18T21:15:14.061666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the seed\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/mnistLSTM/runOne/train'\n",
    "logDirValidation = './logs/mnistLSTM/runOne/validation'\n",
    "\n",
    "\n",
    "# Create place holders\n",
    "x = tf.placeholder(tf.float32, shape = [None, timeSteps, features], name = 'x')\n",
    "# Give 2nd dimension arg to shape since we are using one hot encodings\n",
    "y = tf.placeholder(tf.int64, shape = [None, classes], name = 'y')\n",
    "\n",
    "# Create weights and bias tensors\n",
    "with tf.name_scope(\"weightBias\"):\n",
    "    w = tf.Variable(tf.random_normal([lstmUnits, classes]))\n",
    "    b = tf.Variable(tf.random_normal([classes]))\n",
    "\n",
    "\n",
    "# Add the LSTM cells\n",
    "with tf.name_scope(\"LSTM\"):\n",
    "    \n",
    "    # Later in the code we'll make a call to tf.contrib.rnn.static_rnn\n",
    "    # tf.contrib.rnn.static_rnn expects a length T list of inputs, each a Tensor of shape [batch_size, input_size]\n",
    "    # So we need to convert our inputs of shape [batchSize, timeSteps, numberOfInputs] to [batch_size, input_size]\n",
    "    #\n",
    "    # https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/static_rnn\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/unstack\n",
    "    inputs = tf.unstack(x, num = timeSteps, axis = 1)\n",
    "    \n",
    "    # Create the basic LSTM cell\n",
    "    # It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline.\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "    \n",
    "    # Add the cell to the RNN\n",
    "    # https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/static_rnn\n",
    "    output, state = tf.contrib.rnn.static_rnn(cell, inputs, dtype = tf.float32)\n",
    "    \n",
    "    # We only care about the final output which should be the model's prediction\n",
    "    yH = tf.matmul(output[-1], w) + b\n",
    "    \n",
    "# Add loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # We don't use \"tf.nn.sparse_softmax_cross_entropy_with_logits\" here since we have one hot encodings\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits = yH, labels = y)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    # We don't use \"tf.nn.in_top_k(yH, y, 1)\" here since are aren't using \"tf.nn.sparse_softmax_cross_entropy_with_logits\"\n",
    "    correct = tf.equal(tf.argmax(yH, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:32:47.556966Z",
     "start_time": "2018-09-18T21:28:10.774141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.96 Validation Acc:  0.9686\n",
      "10 Train Acc:  1.0 Validation Acc:  0.9898\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Validation Acc:  0.9898 Test Acc:  0.9882\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    validationWriter = tf.summary.FileWriter(logDirValidation)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs + 1):\n",
    "        for i in range(mninst.train.num_examples // samples):      \n",
    "            counter += 1     \n",
    "            \n",
    "            # Grab the next minibatch\n",
    "            xBatch, yBatch = mninst.train.next_batch(samples)\n",
    "            \n",
    "            # Reshape x to [samples, timeSteps, features] for the LSTM:\n",
    "            #   The image is given to us as a single vector of dimensionality 784\n",
    "            #   So to use them we need to gather the number of rows together to be the timeSteps\n",
    "            xBatch = xBatch.reshape(samples, timeSteps, features)\n",
    "            \n",
    "            # Train the model\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 10 == 0:\n",
    "                # Manually add to the train accuracy summary value\n",
    "                summary, accTrain = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter) \n",
    "                \n",
    "                # Manually add to the test accuracy summary value\n",
    "                \n",
    "                # If test accuracy calcs are causing speed issues you can reduce the number tested via the following:\n",
    "                #summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                #    x: mninst.validation.images[:450].reshape(-1, timeSteps, features), \n",
    "                #    y: mninst.validation.labels[:450]})\n",
    "                summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.validation.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.validation.labels})\n",
    "                validationWriter.add_summary(summary, counter)\n",
    "                \n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation)\n",
    "        \n",
    "    print(\" \")\n",
    "    # Compute test set accuracy rating\n",
    "    summary, accTest = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.test.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.test.labels})\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px;\" src=\"images/mnist-run-one.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a little slower than some other models we've looked at the LSTM has exellent accuracy on this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM v2\n",
    "\n",
    "* Enclose the CG architecture in a graph object; pass to the training session\n",
    "* Utilize f.contrib.rnn.BasicLSTMCell\n",
    "* Utilize tf.contrib.rnn.dynamic_rnn, so we don't need to unstack the 'x' tensor\n",
    "* Remove manual weight and bias definitions and relace with a dense layer\n",
    "* Utilize He initialization\n",
    "* Track training and validiation loss and accuracy in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:15:16.094666Z",
     "start_time": "2018-09-18T21:15:14.061666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the seed\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/mnistLSTM/runTwo/train'\n",
    "logDirValidation = './logs/mnistLSTM/runTwo/validation'\n",
    "\n",
    "# Create the graph object and populate it\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Create place holders\n",
    "    x = tf.placeholder(tf.float32, shape = [None, timeSteps, features], name = 'x')\n",
    "    # Give 2nd dimension arg to shape since we are using one hot encodings\n",
    "    y = tf.placeholder(tf.int64, shape = [None, classes], name = 'y')\n",
    "\n",
    "    # Add the LSTM cells with He initialization (we'll let TF worry about the \"w\" and \"b\" values)\n",
    "    # Notice to do this we switch from \"tf.name_scope\" to \"tf.variable_scope\" and add the \"initializer\" param\n",
    "    with tf.variable_scope(\"LSTM\", initializer = tf.variance_scaling_initializer()):\n",
    "\n",
    "        # Create the basic LSTM cell\n",
    "        # It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "\n",
    "        # Add the cell to the RNN\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "        # Notice we don't have to unstack x as in the previous model\n",
    "        output, state = tf.nn.dynamic_rnn(cell, x, dtype = tf.float32)\n",
    "\n",
    "        # We only care about the final output which should be the model's prediction\n",
    "        yH = tf.layers.dense(state[-1], classes)\n",
    "\n",
    "    # Add loss function\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # We don't use \"tf.nn.sparse_softmax_cross_entropy_with_logits\" here since we have one hot encodings\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(logits = yH, labels = y)\n",
    "        loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "        # Capture loss\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "\n",
    "    # Eval the model's accuracy\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        # We don't use \"tf.nn.in_top_k(yH, y, 1)\" here since are aren't using \"tf.nn.sparse_softmax_cross_entropy_with_logits\"\n",
    "        correct = tf.equal(tf.argmax(yH, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        # Capture accuracy\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:32:47.556966Z",
     "start_time": "2018-09-18T21:28:10.774141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Validation Acc:  0.9606\n",
      "10 Train Acc:  1.0 Validation Acc:  0.9884\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Validation Acc:  0.9884 Test Acc:  0.9877\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    validationWriter = tf.summary.FileWriter(logDirValidation)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs + 1):\n",
    "        for i in range(mninst.train.num_examples // samples):      \n",
    "            counter += 1     \n",
    "            \n",
    "            # Grab the next minibatch\n",
    "            xBatch, yBatch = mninst.train.next_batch(samples)\n",
    "            \n",
    "            # Reshape x to [samples, timeSteps, features] for the LSTM:\n",
    "            #   The image is given to us as a single vector of dimensionality 784\n",
    "            #   So to use them we need to gather the number of rows together to be the timeSteps\n",
    "            xBatch = xBatch.reshape(samples, timeSteps, features)\n",
    "            \n",
    "            # Train the model\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 10 == 0:\n",
    "                # Manually add to the train accuracy summary value\n",
    "                summary, accTrain = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter) \n",
    "                \n",
    "                # Manually add to the test accuracy summary value\n",
    "                \n",
    "                # If test accuracy calcs are causing speed issues you can reduce the number tested via the following:\n",
    "                #summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                #    x: mninst.validation.images[:450].reshape(-1, timeSteps, features), \n",
    "                #    y: mninst.validation.labels[:450]})\n",
    "                summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.validation.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.validation.labels})\n",
    "                validationWriter.add_summary(summary, counter)\n",
    "                \n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation)\n",
    "        \n",
    "    print(\" \")\n",
    "    # Compute test set accuracy rating\n",
    "    summary, accTest = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.test.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.test.labels})\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px;\" src=\"images/mnist-run-two.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM v3\n",
    "\n",
    "* Enclose the CG architecture in a graph object; pass to the training session\n",
    "* Utilize tf.contrib.rnn.LSTMBlockCell\n",
    "* Utilize tf.contrib.rnn.dynamic_rnn, so we don't need to unstack the 'x' tensor\n",
    "* Apply [Batch normalization](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization)\n",
    "* Dense layer with He initialization for weights and biases\n",
    "* Using [tf.nn.softmax_cross_entropy_with_logits_v2](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2) in the 'loss' calculations\n",
    "* Perform gradient clipping via [tf.clip_by_global_norm](https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/clip_by_global_norm) during optimization\n",
    "* Track training and validiation loss and accuracy in TensorBoard\n",
    "\n",
    "LSTM types and benchmarks:  https://returnn.readthedocs.io/en/latest/tf_lstm_benchmark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:15:16.094666Z",
     "start_time": "2018-09-18T21:15:14.061666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the seed\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/mnistLSTM/runThree/train'\n",
    "logDirValidation = './logs/mnistLSTM/runThree/validation'\n",
    "\n",
    "# Create the graph object and populate it\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # We need a way to track if we are training or not for the gradient clipping\n",
    "    isTraining = tf.placeholder_with_default(False, shape = (), name = 'isTraining')\n",
    "    \n",
    "    # Create place holders\n",
    "    x = tf.placeholder(tf.float32, shape = [None, timeSteps, features], name = 'x')\n",
    "    # Give 2nd dimension arg to shape since we are using one hot encodings\n",
    "    y = tf.placeholder(tf.int64, shape = [None, classes], name = 'y')\n",
    "\n",
    "    # Add the LSTM cells with He initialization (we'll let TF worry about the \"w\" and \"b\" values)\n",
    "    # Notice to do this we switch from \"tf.name_scope\" to \"tf.variable_scope\" and add the \"initializer\" param\n",
    "    with tf.variable_scope(\"LSTM\", initializer = tf.variance_scaling_initializer()):\n",
    "\n",
    "        # Create LSTMBlockCell which should be faster\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMBlockCell\n",
    "        # LSTM types and benchmarks:  https://returnn.readthedocs.io/en/latest/tf_lstm_benchmark.html\n",
    "        cell = tf.contrib.rnn.LSTMBlockCell(lstmUnits)\n",
    "\n",
    "        # Add the LSTMBlockCell to the RNN\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "        # Notice we don't have to unstack x as in the previous model\n",
    "        output, state = tf.nn.dynamic_rnn(cell, x, dtype = tf.float32)\n",
    "        \n",
    "        # Return the last output for each sample and apply batch normalization\n",
    "        # Ex: \n",
    "        #   x = np.arange(24)\n",
    "        #   x = x.reshape((2,3,4))\n",
    "        #   x\n",
    "        #   >>>\n",
    "        #   array([[[ 0,  1,  2,  3],\n",
    "        #           [ 4,  5,  6,  7],\n",
    "        #           [ 8,  9, 10, 11]],\n",
    "        #   \n",
    "        #          [[12, 13, 14, 15],\n",
    "        #           [16, 17, 18, 19],\n",
    "        #           [20, 21, 22, 23]]])\n",
    "        #\n",
    "        #   x[:,-1,:]\n",
    "        #   >>>\n",
    "        #   array([[ 8,  9, 10, 11],\n",
    "        #          [20, 21, 22, 23]])\n",
    "        #\n",
    "        # Don't forget to enable/disable training!\n",
    "        bnormOutput = tf.layers.batch_normalization(output[:, -1, :], training = isTraining)\n",
    "        \n",
    "        # Apply the dense layer to output prediction probabilities\n",
    "        yH = tf.layers.dense(bnormOutput, classes)\n",
    "\n",
    "    # Add loss function\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # We don't use \"tf.nn.sparse_softmax_cross_entropy_with_logits\" here since we have one hot encodings\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = yH, labels = y)\n",
    "        loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "        # Capture loss\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        # Since we want to apply gradient clipping we need to compute the gradients,\n",
    "        # process them, and then update the model's parameters by hand\n",
    "        # https://stackoverflow.com/questions/36498127/how-to-apply-gradient-clipping-in-tensorflow\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/clip_by_global_norm\n",
    "        _opt = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "        gvs = _opt.compute_gradients(loss)\n",
    "        cappedGvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "        opt = _opt.apply_gradients(cappedGvs)       \n",
    "\n",
    "    # Eval the model's accuracy\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        # We don't use \"tf.nn.in_top_k(yH, y, 1)\" here since are aren't using \"tf.nn.sparse_softmax_cross_entropy_with_logits\"\n",
    "        correct = tf.equal(tf.argmax(yH, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        # Capture accuracy\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:32:47.556966Z",
     "start_time": "2018-09-18T21:28:10.774141Z"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    validationWriter = tf.summary.FileWriter(logDirValidation)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs + 1):\n",
    "        for i in range(mninst.train.num_examples // samples):      \n",
    "            counter += 1     \n",
    "            \n",
    "            # Grab the next minibatch\n",
    "            xBatch, yBatch = mninst.train.next_batch(samples)\n",
    "            \n",
    "            # Reshape x to [samples, timeSteps, features] for the LSTM:\n",
    "            #   The image is given to us as a single vector of dimensionality 784\n",
    "            #   So to use them we need to gather the number of rows together to be the timeSteps\n",
    "            xBatch = xBatch.reshape(samples, timeSteps, features)\n",
    "            \n",
    "            # Train the model\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 10 == 0:\n",
    "                # Manually add to the train accuracy summary value\n",
    "                summary, accTrain = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter) \n",
    "                \n",
    "                # Manually add to the test accuracy summary value\n",
    "                \n",
    "                # If test accuracy calcs are causing speed issues you can reduce the number tested via the following:\n",
    "                #summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                #    x: mninst.validation.images[:450].reshape(-1, timeSteps, features), \n",
    "                #    y: mninst.validation.labels[:450]})\n",
    "                summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.validation.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.validation.labels})\n",
    "                validationWriter.add_summary(summary, counter)\n",
    "                \n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation)\n",
    "        \n",
    "    print(\" \")\n",
    "    # Compute test set accuracy rating\n",
    "    summary, accTest = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.test.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.test.labels})\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:15:16.094666Z",
     "start_time": "2018-09-18T21:15:14.061666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LSTMStateTuple(c=<tf.Tensor 'LSTM/rnn/while/Exit_3:0' shape=(?, 40) dtype=float32>, h=<tf.Tensor 'LSTM/rnn/while/Exit_4:0' shape=(?, 40) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'LSTM/rnn/while/Exit_5:0' shape=(?, 40) dtype=float32>, h=<tf.Tensor 'LSTM/rnn/while/Exit_6:0' shape=(?, 40) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'LSTM/rnn/while/Exit_7:0' shape=(?, 40) dtype=float32>, h=<tf.Tensor 'LSTM/rnn/while/Exit_8:0' shape=(?, 40) dtype=float32>))\n",
      " \n",
      "Tensor(\"LSTM/Reshape:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"LSTM/dense/BiasAdd:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Add new variable: Number of LSTM cells\n",
    "lstmCells = 3\n",
    "\n",
    "# Reduce the number of units per cell\n",
    "lstmUnits = 40\n",
    "\n",
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/mnistLSTM/runThree/train'\n",
    "logDirTest = './logs/mnistLSTM/runThree/test'\n",
    "\n",
    "\n",
    "# Create place holders\n",
    "x = tf.placeholder(tf.float32, shape = [None, timeSteps, features], name = 'x')\n",
    "# Give 2nd dimension arg to shape since we are using one hot encodings\n",
    "y = tf.placeholder(tf.int64, shape = [None, classes], name = 'y')\n",
    "\n",
    "# Add the LSTM cells with He initialization (we'll let TF worry about the \"w\" and \"b\" values)\n",
    "# Notice to do this we switch from \"tf.name_scope\" to \"tf.variable_scope\" and add the \"initializer\" param\n",
    "with tf.variable_scope(\"LSTM\", initializer = tf.variance_scaling_initializer()):\n",
    "        \n",
    "    # Create the basic LSTM cell\n",
    "    # It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline.\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell\n",
    "    #cell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "    cells = [tf.contrib.rnn.BasicLSTMCell(lstmUnits) for c in range(lstmCells)]\n",
    "    stackedCells = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    # Add the cell to the RNN\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "    # Notice we don't have to unstack x as in the previous model\n",
    "    #output, state = tf.nn.dynamic_rnn(cell, x, dtype = tf.float32)\n",
    "    output, state = tf.nn.dynamic_rnn(stackedCells, x, dtype = tf.float32)\n",
    "    \n",
    "    print(state)\n",
    "    print(\" \")\n",
    "    print(tf.reshape(state, [-1, lstmCells]))\n",
    "    \n",
    "    # Output now has shape \"(?, 28, 40)\"\n",
    "    # Output needs to flatten, so that it can be passed to the softmax loss function. \n",
    "    # OutputFlat will have shape \"(?, 3)\"\n",
    "    #outputFlat = tf.reshape(output, [-1, lstmCells])\n",
    "    \n",
    "    #yH = tf.layers.dense(state[-1], classes)\n",
    "    #yH = tf.layers.dense(outputFlat, classes)\n",
    "    yH = tf.layers.dense(tf.reshape(state, [-1, lstmCells]), classes)\n",
    "    print(yH)\n",
    "    \n",
    "    \n",
    "# Add loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # We don't use \"tf.nn.sparse_softmax_cross_entropy_with_logits\" here since we have one hot encodings\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits = yH, labels = y)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    # We don't use \"tf.nn.in_top_k(yH, y, 1)\" here since are aren't using \"tf.nn.sparse_softmax_cross_entropy_with_logits\"\n",
    "    correct = tf.equal(tf.argmax(yH, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:32:47.556966Z",
     "start_time": "2018-09-18T21:28:10.774141Z"
    }
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be broadcastable: logits_size=[4000,10] labels_size=[50,10]\n\t [[Node: loss/softmax_cross_entropy_with_logits_sg = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](LSTM/dense/BiasAdd, loss/softmax_cross_entropy_with_logits_sg/Reshape_1)]]\n\nCaused by op 'loss/softmax_cross_entropy_with_logits_sg', defined at:\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-64-fdc33bb2fc8d>\", line 56, in <module>\n    entropy = tf.nn.softmax_cross_entropy_with_logits(logits = yH, labels = y)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 250, in new_func\n    return func(*args, **kwargs)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1968, in softmax_cross_entropy_with_logits\n    labels=labels, logits=logits, dim=dim, name=name)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1879, in softmax_cross_entropy_with_logits_v2\n    precise_logits, labels, name=name)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 7738, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must be broadcastable: logits_size=[4000,10] labels_size=[50,10]\n\t [[Node: loss/softmax_cross_entropy_with_logits_sg = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](LSTM/dense/BiasAdd, loss/softmax_cross_entropy_with_logits_sg/Reshape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[4000,10] labels_size=[50,10]\n\t [[Node: loss/softmax_cross_entropy_with_logits_sg = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](LSTM/dense/BiasAdd, loss/softmax_cross_entropy_with_logits_sg/Reshape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-85e7ca5610f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mxBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0myBatch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;31m# Capture summary data every N steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[4000,10] labels_size=[50,10]\n\t [[Node: loss/softmax_cross_entropy_with_logits_sg = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](LSTM/dense/BiasAdd, loss/softmax_cross_entropy_with_logits_sg/Reshape_1)]]\n\nCaused by op 'loss/softmax_cross_entropy_with_logits_sg', defined at:\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-64-fdc33bb2fc8d>\", line 56, in <module>\n    entropy = tf.nn.softmax_cross_entropy_with_logits(logits = yH, labels = y)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 250, in new_func\n    return func(*args, **kwargs)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1968, in softmax_cross_entropy_with_logits\n    labels=labels, logits=logits, dim=dim, name=name)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1879, in softmax_cross_entropy_with_logits_v2\n    precise_logits, labels, name=name)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 7738, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"c:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must be broadcastable: logits_size=[4000,10] labels_size=[50,10]\n\t [[Node: loss/softmax_cross_entropy_with_logits_sg = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](LSTM/dense/BiasAdd, loss/softmax_cross_entropy_with_logits_sg/Reshape_1)]]\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs + 1):\n",
    "        for i in range(mninst.train.num_examples // samples):      \n",
    "            counter += 1     \n",
    "            \n",
    "            # Grab the next minibatch\n",
    "            xBatch, yBatch = mninst.train.next_batch(samples)\n",
    "            \n",
    "            # Reshape x to [samples, timeSteps, features] for the LSTM:\n",
    "            #   The image is given to us as a single vector of dimensionality 784\n",
    "            #   So to use them we need to gather the number of rows together to be the timeSteps\n",
    "            xBatch = xBatch.reshape(samples, timeSteps, features)\n",
    "            \n",
    "            # Train the model\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 10 == 0:\n",
    "                # Manually add to the train accuracy summary value\n",
    "                summary, accTrain = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter) \n",
    "                \n",
    "                # Manually add to the test accuracy summary value\n",
    "                \n",
    "                # If test accuracy calcs are causing speed issues you can reduce the number tested via the following:\n",
    "                #summary, accTest = sess.run([merge, accuracy], feed_dict = {\n",
    "                #    x: mninst.test.images[:450].reshape(-1, timeSteps, features), \n",
    "                #    y: mninst.test.labels[:450]})\n",
    "                summary, accTest = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.test.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:15:16.094666Z",
     "start_time": "2018-09-18T21:15:14.061666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMStateTuple(c=<tf.Tensor 'LSTM/rnn/while/Exit_3:0' shape=(?, 40) dtype=float32>, h=<tf.Tensor 'LSTM/rnn/while/Exit_4:0' shape=(?, 40) dtype=float32>)\n",
      " \n",
      "Tensor(\"LSTM/rnn/while/Exit_4:0\", shape=(?, 40), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/mnistLSTM/runTwo/train'\n",
    "logDirTest = './logs/mnistLSTM/runTwo/test'\n",
    "\n",
    "\n",
    "# Create place holders\n",
    "x = tf.placeholder(tf.float32, shape = [None, timeSteps, features], name = 'x')\n",
    "# Give 2nd dimension arg to shape since we are using one hot encodings\n",
    "y = tf.placeholder(tf.int64, shape = [None, classes], name = 'y')\n",
    "\n",
    "# Add the LSTM cells with He initialization (we'll let TF worry about the \"w\" and \"b\" values)\n",
    "# Notice to do this we switch from \"tf.name_scope\" to \"tf.variable_scope\" and add the \"initializer\" param\n",
    "with tf.variable_scope(\"LSTM\", initializer = tf.variance_scaling_initializer()):\n",
    "        \n",
    "    # Create the basic LSTM cell\n",
    "    # It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline.\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "    \n",
    "    # Add the cell to the RNN\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "    # Notice we don't have to unstack x as in the previous model\n",
    "    output, state = tf.nn.dynamic_rnn(cell, x, dtype = tf.float32)\n",
    "    \n",
    "    #print(output)\n",
    "    print(state)\n",
    "    print(\" \")\n",
    "    print(state[-1])\n",
    "    \n",
    "    # We only care about the final output which should be the model's prediction\n",
    "    yH = tf.layers.dense(state[-1], classes)\n",
    "    \n",
    "# Add loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # We don't use \"tf.nn.sparse_softmax_cross_entropy_with_logits\" here since we have one hot encodings\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits = yH, labels = y)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    # We don't use \"tf.nn.in_top_k(yH, y, 1)\" here since are aren't using \"tf.nn.sparse_softmax_cross_entropy_with_logits\"\n",
    "    correct = tf.equal(tf.argmax(yH, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
