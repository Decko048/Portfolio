{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#MNIST-classification-with-a-Long-Short-Term-Memory-(LSTM)-network\" data-toc-modified-id=\"MNIST-classification-with-a-Long-Short-Term-Memory-(LSTM)-network-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MNIST classification with a Long Short Term Memory (LSTM) network</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Load-libraries-and-data\" data-toc-modified-id=\"Load-libraries-and-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load libraries and data</a></span></li><li><span><a href=\"#Init-vars\" data-toc-modified-id=\"Init-vars-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Init vars</a></span></li><li><span><a href=\"#Build-the-computational-graph\" data-toc-modified-id=\"Build-the-computational-graph-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Build the computational graph</a></span><ul class=\"toc-item\"><li><span><a href=\"#Static-vs-Dynamic-TensorFlow-RNNs\" data-toc-modified-id=\"Static-vs-Dynamic-TensorFlow-RNNs-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Static vs Dynamic TensorFlow RNNs</a></span></li><li><span><a href=\"#LSTM-v1\" data-toc-modified-id=\"LSTM-v1-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>LSTM v1</a></span></li><li><span><a href=\"#LSTM-v2\" data-toc-modified-id=\"LSTM-v2-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>LSTM v2</a></span></li><li><span><a href=\"#LSTM-v3\" data-toc-modified-id=\"LSTM-v3-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>LSTM v3</a></span></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MNIST classification with a Long Short Term Memory (LSTM) network</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px; width: 30%; height: 30%;\" src=\"images/mnist-image.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The purpose of this write-up is create a predictive MNIST data set classification model utilizing a Long Short Term Memory (LSTM) network written in TensorFlow.    \n",
    "\n",
    "Goals include:\n",
    "* Build and train three different LSTM predictive classification models with differing architectures\n",
    "* Collect metrics and graph each model's performance in TensorBoard \n",
    "* Make predictions with the training model on the test data set and examine accuracy\n",
    "\n",
    "Dataset source:  [The MNIST Database](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T21:24:53.267081Z",
     "start_time": "2018-10-20T21:24:53.258083Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T21:24:53.768144Z",
     "start_time": "2018-10-20T21:24:53.762113Z"
    }
   },
   "outputs": [],
   "source": [
    "def resetGraph(seed= 10):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    accTrain = accValidation = accTest = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T21:24:54.176116Z",
     "start_time": "2018-10-20T21:24:54.172104Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanLogs():\n",
    "    os.system('rm -rf ./logs/mnistLSTM/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init vars\n",
    "\n",
    "The LSTM wants inputs of shape `[samples, timeSteps, features]`, and we have several thousand MNIST images of size 28 x 28 pixels.  \n",
    "\n",
    "One way to think of this is a complete image is comprised of 28 rows of 28 pixels each.  If we were to step through the rows one by one and stack them up then the image would be more and more complete as time went by.  So our units of \"time\" will be the rows stacking together to create a complete image, and the number of features will be the number of pixels in the image row at that step in time (i.e. 28).  This gives us:\n",
    "\n",
    "* samples     = number of observations (i.e. number of images in the mini batch)\n",
    "* timeSteps   = number of rows we need to step through/stack up to make a complete image\n",
    "* features    = the number of features in each row we are stepping through (i.e. also 28)\n",
    "\n",
    "Additionally, we only care about the final output of the LSTM network which should give us the prediction of which numeral the image represents.  Other LSTM networks do care about the outputs of each LSTM cell (translating each word in a sentence for example), but that doesn't apply in our case.\n",
    "\n",
    "Having said this we can continue with initializing the various variables we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T21:25:46.376125Z",
     "start_time": "2018-10-20T21:25:45.581110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "\n",
      " Example label:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Setup vars for the MINST data set\n",
    "timeSteps = 28\n",
    "features = 28\n",
    "\n",
    "lstmUnits = 128\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "samples = 50\n",
    "\n",
    "classes = 10\n",
    "\n",
    "# Allow results to be reproduced\n",
    "seed = 10\n",
    "\n",
    "# Notice we are pulling in the labels as one hot encodings!\n",
    "mninst = input_data.read_data_sets(\"./datasets/mnist\", one_hot = True)\n",
    "\n",
    "# For use when we create the LSTM network below\n",
    "testShape = mninst.test.images.shape\n",
    "\n",
    "# Note the one hot encoding on the label:\n",
    "print(\"\\n\", \"Example label: \", mninst.test.labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-07T20:47:13.613722Z",
     "start_time": "2018-09-07T20:47:13.609737Z"
    }
   },
   "source": [
    "# Build the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static vs Dynamic TensorFlow RNNs\n",
    "\n",
    "> Dynamic RNN's allow for variable sequence lengths. You might have an input shape (batch_size, max_sequence_length), but this will allow you to run the RNN for the correct number of time steps on those sequences that are shorter than max_sequence_length.\n",
    " \n",
    "> In contrast, there are static RNNs, which expect to run the entire fixed RNN length. There are cases where you might prefer to do this, such as if you are padding your inputs to max_sequence_length anyway.\n",
    "\n",
    ">In short, dynamic_rnn is usually what you want for variable length sequential data. It has a sequence_length parameter, and it is your friend.  [Source](https://stackoverflow.com/questions/43100981/what-is-a-dynamic-rnn-in-tensorflow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM v1\n",
    "\n",
    "* Utilize f.contrib.rnn.BasicLSTMCell\n",
    "* Utilize tf.contrib.rnn.static_rnn\n",
    "* Manual weight and bias definitions with tf.random_normal for initialization\n",
    "* Track training and validiation loss and accuracy in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:15:16.094666Z",
     "start_time": "2018-09-18T21:15:14.061666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the seed\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/mnistLSTM/runOne/train'\n",
    "logDirValidation = './logs/mnistLSTM/runOne/validation'\n",
    "\n",
    "\n",
    "# Create place holders\n",
    "x = tf.placeholder(tf.float32, shape = [None, timeSteps, features], name = 'x')\n",
    "# Give 2nd dimension arg to shape since we are using one hot encodings\n",
    "y = tf.placeholder(tf.int64, shape = [None, classes], name = 'y')\n",
    "\n",
    "# Create weights and bias tensors\n",
    "with tf.name_scope(\"weightBias\"):\n",
    "    w = tf.Variable(tf.random_normal([lstmUnits, classes]))\n",
    "    b = tf.Variable(tf.random_normal([classes]))\n",
    "\n",
    "\n",
    "# Add the LSTM cells\n",
    "with tf.name_scope(\"LSTM\"):\n",
    "    \n",
    "    # Later in the code we'll make a call to tf.contrib.rnn.static_rnn\n",
    "    # tf.contrib.rnn.static_rnn expects a length T list of inputs, each a Tensor of shape [batch_size, input_size]\n",
    "    # So we need to convert our inputs of shape [batchSize, timeSteps, numberOfInputs] to [batch_size, input_size]\n",
    "    #\n",
    "    # https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/static_rnn\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/unstack\n",
    "    inputs = tf.unstack(x, num = timeSteps, axis = 1)\n",
    "    \n",
    "    # Create the basic LSTM cell\n",
    "    # It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline.\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "    \n",
    "    # Add the cell to the RNN\n",
    "    # https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/static_rnn\n",
    "    output, state = tf.contrib.rnn.static_rnn(cell, inputs, dtype = tf.float32)\n",
    "    \n",
    "    # We only care about the final output which should be the model's prediction\n",
    "    yH = tf.matmul(output[-1], w) + b\n",
    "    \n",
    "# Add loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # We don't use \"tf.nn.sparse_softmax_cross_entropy_with_logits\" here since we have one hot encodings\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits = yH, labels = y)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    # We don't use \"tf.nn.in_top_k(yH, y, 1)\" here since are aren't using \"tf.nn.sparse_softmax_cross_entropy_with_logits\"\n",
    "    correct = tf.equal(tf.argmax(yH, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:32:47.556966Z",
     "start_time": "2018-09-18T21:28:10.774141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.96 Validation Acc:  0.9686\n",
      "10 Train Acc:  1.0 Validation Acc:  0.9898\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Validation Acc:  0.9898 Test Acc:  0.9882\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    validationWriter = tf.summary.FileWriter(logDirValidation)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs + 1):\n",
    "        for i in range(mninst.train.num_examples // samples):      \n",
    "            counter += 1     \n",
    "            \n",
    "            # Grab the next minibatch\n",
    "            xBatch, yBatch = mninst.train.next_batch(samples)\n",
    "            \n",
    "            # Reshape x to [samples, timeSteps, features] for the LSTM:\n",
    "            #   The image is given to us as a single vector of dimensionality 784\n",
    "            #   So to use them we need to gather the number of rows together to be the timeSteps\n",
    "            xBatch = xBatch.reshape(samples, timeSteps, features)\n",
    "            \n",
    "            # Train the model\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 10 == 0:\n",
    "                # Manually add to the train accuracy summary value\n",
    "                summary, accTrain = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter) \n",
    "                \n",
    "                # Manually add to the test accuracy summary value\n",
    "                \n",
    "                # If test accuracy calcs are causing speed issues you can reduce the number tested via the following:\n",
    "                #summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                #    x: mninst.validation.images[:450].reshape(-1, timeSteps, features), \n",
    "                #    y: mninst.validation.labels[:450]})\n",
    "                summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.validation.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.validation.labels})\n",
    "                validationWriter.add_summary(summary, counter)\n",
    "                \n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation)\n",
    "        \n",
    "    print(\" \")\n",
    "    # Compute test set accuracy rating\n",
    "    summary, accTest = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.test.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.test.labels})\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px;\" src=\"images/mnist-run-one.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a little slower than some other models we've looked at the LSTM has exellent accuracy on this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM v2\n",
    "\n",
    "* Enclose the CG architecture in a graph object; pass to the training session\n",
    "* Utilize f.contrib.rnn.BasicLSTMCell\n",
    "* Utilize tf.contrib.rnn.dynamic_rnn, so we don't need to unstack the 'x' tensor\n",
    "* Remove manual weight and bias definitions and relace with a dense layer\n",
    "* Utilize He initialization\n",
    "* Track training and validiation loss and accuracy in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:15:16.094666Z",
     "start_time": "2018-09-18T21:15:14.061666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the seed\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/mnistLSTM/runTwo/train'\n",
    "logDirValidation = './logs/mnistLSTM/runTwo/validation'\n",
    "\n",
    "# Create the graph object and populate it\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Create place holders\n",
    "    x = tf.placeholder(tf.float32, shape = [None, timeSteps, features], name = 'x')\n",
    "    # Give 2nd dimension arg to shape since we are using one hot encodings\n",
    "    y = tf.placeholder(tf.int64, shape = [None, classes], name = 'y')\n",
    "\n",
    "    # Add the LSTM cells with He initialization (we'll let TF worry about the \"w\" and \"b\" values)\n",
    "    # Notice to do this we switch from \"tf.name_scope\" to \"tf.variable_scope\" and add the \"initializer\" param\n",
    "    with tf.variable_scope(\"LSTM\", initializer = tf.variance_scaling_initializer()):\n",
    "\n",
    "        # Create the basic LSTM cell\n",
    "        # It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline.\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "\n",
    "        # Add the cell to the RNN\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "        # Notice we don't have to unstack x as in the previous model\n",
    "        output, state = tf.nn.dynamic_rnn(cell, x, dtype = tf.float32)\n",
    "\n",
    "        # We only care about the final output which should be the model's prediction\n",
    "        yH = tf.layers.dense(state[-1], classes)\n",
    "\n",
    "    # Add loss function\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # We don't use \"tf.nn.sparse_softmax_cross_entropy_with_logits\" here since we have one hot encodings\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(logits = yH, labels = y)\n",
    "        loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "        # Capture loss\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "\n",
    "    # Eval the model's accuracy\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        # We don't use \"tf.nn.in_top_k(yH, y, 1)\" here since are aren't using \"tf.nn.sparse_softmax_cross_entropy_with_logits\"\n",
    "        correct = tf.equal(tf.argmax(yH, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        # Capture accuracy\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:32:47.556966Z",
     "start_time": "2018-09-18T21:28:10.774141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Validation Acc:  0.9606\n",
      "10 Train Acc:  1.0 Validation Acc:  0.9884\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Validation Acc:  0.9884 Test Acc:  0.9877\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    validationWriter = tf.summary.FileWriter(logDirValidation)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs + 1):\n",
    "        for i in range(mninst.train.num_examples // samples):      \n",
    "            counter += 1     \n",
    "            \n",
    "            # Grab the next minibatch\n",
    "            xBatch, yBatch = mninst.train.next_batch(samples)\n",
    "            \n",
    "            # Reshape x to [samples, timeSteps, features] for the LSTM:\n",
    "            #   The image is given to us as a single vector of dimensionality 784\n",
    "            #   So to use them we need to gather the number of rows together to be the timeSteps\n",
    "            xBatch = xBatch.reshape(samples, timeSteps, features)\n",
    "            \n",
    "            # Train the model\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 10 == 0:\n",
    "                # Manually add to the train accuracy summary value\n",
    "                summary, accTrain = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter) \n",
    "                \n",
    "                # Manually add to the test accuracy summary value\n",
    "                \n",
    "                # If test accuracy calcs are causing speed issues you can reduce the number tested via the following:\n",
    "                #summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                #    x: mninst.validation.images[:450].reshape(-1, timeSteps, features), \n",
    "                #    y: mninst.validation.labels[:450]})\n",
    "                summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.validation.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.validation.labels})\n",
    "                validationWriter.add_summary(summary, counter)\n",
    "                \n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation)\n",
    "        \n",
    "    print(\" \")\n",
    "    # Compute test set accuracy rating\n",
    "    summary, accTest = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.test.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.test.labels})\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px;\" src=\"images/mnist-run-two.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM v3\n",
    "\n",
    "* Enclose the CG architecture in a graph object; pass to the training session\n",
    "* Utilize tf.contrib.rnn.LSTMBlockCell\n",
    "* Utilize tf.contrib.rnn.dynamic_rnn, so we don't need to unstack the 'x' tensor\n",
    "* Apply [Batch normalization](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization)\n",
    "* Dense layer with He initialization for weights and biases\n",
    "* Using [tf.nn.softmax_cross_entropy_with_logits_v2](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2) in the 'loss' calculations\n",
    "* Perform gradient clipping via [tf.clip_by_global_norm](https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/clip_by_global_norm) during optimization\n",
    "* Track training and validiation loss and accuracy in TensorBoard\n",
    "\n",
    "LSTM types and benchmarks:  https://returnn.readthedocs.io/en/latest/tf_lstm_benchmark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T21:25:58.854807Z",
     "start_time": "2018-10-20T21:25:57.592797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the seed\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/mnistLSTM/runThree/train'\n",
    "logDirValidation = './logs/mnistLSTM/runThree/validation'\n",
    "\n",
    "# Create the graph object and populate it\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # We need a way to track if we are training or not for the gradient clipping\n",
    "    isTraining = tf.placeholder_with_default(False, shape = (), name = 'isTraining')\n",
    "    \n",
    "    # Create place holders\n",
    "    x = tf.placeholder(tf.float32, shape = [None, timeSteps, features], name = 'x')\n",
    "    # Give 2nd dimension arg to shape since we are using one hot encodings\n",
    "    y = tf.placeholder(tf.int64, shape = [None, classes], name = 'y')\n",
    "\n",
    "    # Add the LSTM cells with He initialization (we'll let TF worry about the \"w\" and \"b\" values)\n",
    "    # Notice to do this we switch from \"tf.name_scope\" to \"tf.variable_scope\" and add the \"initializer\" param\n",
    "    with tf.variable_scope(\"LSTM\", initializer = tf.variance_scaling_initializer()):\n",
    "\n",
    "        # Create LSTMBlockCell which should be faster\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMBlockCell\n",
    "        # LSTM types and benchmarks:  https://returnn.readthedocs.io/en/latest/tf_lstm_benchmark.html\n",
    "        cell = tf.contrib.rnn.LSTMBlockCell(lstmUnits)\n",
    "\n",
    "        # Add the LSTMBlockCell to the RNN\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "        # Notice we don't have to unstack x as in the previous model\n",
    "        output, state = tf.nn.dynamic_rnn(cell, x, dtype = tf.float32)\n",
    "        \n",
    "        # Return the last output for each sample and apply batch normalization\n",
    "        # Ex: \n",
    "        #   x = np.arange(24)\n",
    "        #   x = x.reshape((2,3,4))\n",
    "        #   x\n",
    "        #   >>>\n",
    "        #   array([[[ 0,  1,  2,  3],\n",
    "        #           [ 4,  5,  6,  7],\n",
    "        #           [ 8,  9, 10, 11]],\n",
    "        #   \n",
    "        #          [[12, 13, 14, 15],\n",
    "        #           [16, 17, 18, 19],\n",
    "        #           [20, 21, 22, 23]]])\n",
    "        #\n",
    "        #   x[:,-1,:]\n",
    "        #   >>>\n",
    "        #   array([[ 8,  9, 10, 11],\n",
    "        #          [20, 21, 22, 23]])\n",
    "        #\n",
    "        # Don't forget to enable/disable training!\n",
    "        bnormOutput = tf.layers.batch_normalization(output[:, -1, :], training = isTraining)\n",
    "        \n",
    "        # Apply the dense layer to output prediction probabilities\n",
    "        yH = tf.layers.dense(bnormOutput, classes)\n",
    "\n",
    "    # Add loss function\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # We don't use \"tf.nn.sparse_softmax_cross_entropy_with_logits\" here since we have one hot encodings\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = yH, labels = y)\n",
    "        loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "        # Capture loss\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        # Since we want to apply gradient clipping we need to compute the gradients,\n",
    "        # process them, and then update the model's parameters by hand\n",
    "        # https://stackoverflow.com/questions/36498127/how-to-apply-gradient-clipping-in-tensorflow\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/clip_by_global_norm\n",
    "        _opt = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "        gvs = _opt.compute_gradients(loss)\n",
    "        cappedGvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "        opt = _opt.apply_gradients(cappedGvs)       \n",
    "\n",
    "    # Eval the model's accuracy\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        # We don't use \"tf.nn.in_top_k(yH, y, 1)\" here since are aren't using \"tf.nn.sparse_softmax_cross_entropy_with_logits\"\n",
    "        correct = tf.equal(tf.argmax(yH, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        # Capture accuracy\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T22:14:53.562888Z",
     "start_time": "2018-10-20T21:25:58.857871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.98 Validation Acc:  0.9666\n",
      "10 Train Acc:  1.0 Validation Acc:  0.9888\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Validation Acc:  0.9888 Test Acc:  0.9875\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    validationWriter = tf.summary.FileWriter(logDirValidation)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs + 1):\n",
    "        for i in range(mninst.train.num_examples // samples):      \n",
    "            counter += 1     \n",
    "            \n",
    "            # Grab the next minibatch\n",
    "            xBatch, yBatch = mninst.train.next_batch(samples)\n",
    "            \n",
    "            # Reshape x to [samples, timeSteps, features] for the LSTM:\n",
    "            #   The image is given to us as a single vector of dimensionality 784\n",
    "            #   So to use them we need to gather the number of rows together to be the timeSteps\n",
    "            xBatch = xBatch.reshape(samples, timeSteps, features)\n",
    "            \n",
    "            # Train the model\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 10 == 0:\n",
    "                # Manually add to the train accuracy summary value\n",
    "                summary, accTrain = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter) \n",
    "                \n",
    "                # Manually add to the test accuracy summary value\n",
    "                \n",
    "                # If test accuracy calcs are causing speed issues you can reduce the number tested via the following:\n",
    "                #summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                #    x: mninst.validation.images[:450].reshape(-1, timeSteps, features), \n",
    "                #    y: mninst.validation.labels[:450]})\n",
    "                summary, accValidation = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.validation.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.validation.labels})\n",
    "                validationWriter.add_summary(summary, counter)\n",
    "                \n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation)\n",
    "        \n",
    "    print(\" \")\n",
    "    # Compute test set accuracy rating\n",
    "    summary, accTest = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.test.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.test.labels})\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Validation Acc: \", accValidation, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px;\" src=\"images/mnist-run-three.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We developed, trained, and gathered metrics for three implementations of a MNIST LSTM classifier.  The three models had the following accuracy ratings on the train, validation, and test data sets:\n",
    "\n",
    "|Model  |Train Accuracy |Validation Accuracy|Test Accuracy|\n",
    "|-------|---------------|-------------------|-------------|\n",
    "|LSTM v1|1.0            |0.9898             |0.9882       |\n",
    "|LSTM v2|1.0            |0.9884             |0.9877       |\n",
    "|LSTM v3|1.0            |0.9888             |0.9875       |\n",
    "\n",
    "Each of the models had around the same performance, and the TensorBoard graphs were also almost identical.  Likely the choice of one of these models over other would come down to personal preference.  However, given a different data set this could certainly change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
