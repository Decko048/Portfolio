{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#IMDB-Movie-Review-Sentiment-Classification\" data-toc-modified-id=\"IMDB-Movie-Review-Sentiment-Classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>IMDB Movie Review Sentiment Classification</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Process\" data-toc-modified-id=\"Process-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Process</a></span></li><li><span><a href=\"#Configure-notebook,-import-libraries,-and-import-dataset\" data-toc-modified-id=\"Configure-notebook,-import-libraries,-and-import-dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Configure notebook, import libraries, and import dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries\" data-toc-modified-id=\"Import-libraries-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Import libraries</a></span></li><li><span><a href=\"#Define-global-variables\" data-toc-modified-id=\"Define-global-variables-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Define global variables</a></span></li></ul></li><li><span><a href=\"#Helper-Functions\" data-toc-modified-id=\"Helper-Functions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Helper Functions</a></span></li><li><span><a href=\"#Examine-the-data\" data-toc-modified-id=\"Examine-the-data-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Examine the data</a></span></li><li><span><a href=\"#Cleaning-and-preprocessing\" data-toc-modified-id=\"Cleaning-and-preprocessing-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Cleaning and preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-labeled-training-data\" data-toc-modified-id=\"Load-labeled-training-data-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Load labeled training data</a></span></li><li><span><a href=\"#Clean/Process-reviews\" data-toc-modified-id=\"Clean/Process-reviews-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Clean/Process reviews</a></span><ul class=\"toc-item\"><li><span><a href=\"#Option-A:--Read-previous-text-cleaning-work-from-disk\" data-toc-modified-id=\"Option-A:--Read-previous-text-cleaning-work-from-disk-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>Option A:  Read previous text cleaning work from disk</a></span></li><li><span><a href=\"#Option-B:--Perform-text-cleaning-work-and-write-to-disk\" data-toc-modified-id=\"Option-B:--Perform-text-cleaning-work-and-write-to-disk-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>Option B:  Perform text cleaning work and write to disk</a></span></li></ul></li></ul></li><li><span><a href=\"#Create-Models\" data-toc-modified-id=\"Create-Models-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Create Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenize-the-cleaned-data\" data-toc-modified-id=\"Tokenize-the-cleaned-data-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Tokenize the cleaned data</a></span></li><li><span><a href=\"#Algorithm-v1:--LSTM\" data-toc-modified-id=\"Algorithm-v1:--LSTM-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Algorithm v1:  LSTM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comments\" data-toc-modified-id=\"Comments-8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;</span>Comments</a></span></li></ul></li><li><span><a href=\"#Algorithm-v2:--CNN\" data-toc-modified-id=\"Algorithm-v2:--CNN-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Algorithm v2:  CNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comments\" data-toc-modified-id=\"Comments-8.3.1\"><span class=\"toc-item-num\">8.3.1&nbsp;&nbsp;</span>Comments</a></span></li></ul></li><li><span><a href=\"#Algorithm-v3:--CNN-with-tuning\" data-toc-modified-id=\"Algorithm-v3:--CNN-with-tuning-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Algorithm v3:  CNN with tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comments\" data-toc-modified-id=\"Comments-8.4.1\"><span class=\"toc-item-num\">8.4.1&nbsp;&nbsp;</span>Comments</a></span></li></ul></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>IMDB Movie Review Sentiment Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px; width: 30%; height: 30%;\" src=\"images/imdb.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The overall goal of this set of write-ups is to explore a number of machine learning algorithms utilizing natural language processing (NLP) to classify sentiment IMDB movie reviews.\n",
    "\n",
    "The specific goals of this write-up include:\n",
    "1. Create a feature set of document vectors from the IMDb movie review text utilizing [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "2. Utilize LSTM and CNN algorithms to create predictive models from the feature set(s) created above \n",
    "2. Obtain Kaggle scores on the outputs of the models\n",
    "3. Determine if utilizing LSTM and CNN networks on feature sets created with Doc2Vec improves our ability to correctly classify movie review sentiment\n",
    "\n",
    "This series of write-ups is inspired by the Kaggle [\n",
    "Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial) competition.\n",
    "\n",
    "References:\n",
    "* [Gensim Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "\n",
    "Dataset source:  [IMDB Movie Reviews](https://www.kaggle.com/c/word2vec-nlp-tutorial/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "\n",
    "Previously covered [here](./Model-06.ipynb#Process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure notebook, import libraries, and import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T15:16:22.416553Z",
     "start_time": "2018-10-16T15:16:22.413553Z"
    }
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T16:06:07.210761Z",
     "start_time": "2018-11-26T16:06:06.627761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import set_option\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# http://www.nltk.org/index.html\n",
    "# pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# Creating function implementing punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "\n",
    "# Only need this the first time...\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "# pip install BeautifulSoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# https://pypi.org/project/gensim/\n",
    "# pip install gensim\n",
    "import gensim.models.doc2vec\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "#import multiprocessing\n",
    "\n",
    "#cores = multiprocessing.cpu_count()\n",
    "#assert(gensim.models.doc2vec.FAST_VERSION > -1, \"Going to be slow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T16:06:08.490960Z",
     "start_time": "2018-11-26T16:06:07.928960Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Opens a GUI that allows us to download the NLTK data\n",
    "# nltk.download()\n",
    "\n",
    "dataPath = os.path.join('.', 'datasets', 'imdb_movie_reviews')\n",
    "labeledTrainData = os.path.join(dataPath, 'labeledTrainData.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T16:06:09.248960Z",
     "start_time": "2018-11-26T16:06:08.569960Z"
    }
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "contractionsObj = re.compile('(%s)' % '|'.join(contractions.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T16:06:10.291159Z",
     "start_time": "2018-11-26T16:06:09.728960Z"
    }
   },
   "outputs": [],
   "source": [
    "def expandContractions(txt, contractions = contractions):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractionsObj.sub(replace, txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Function to train a given algorithm, create predictions on the test set from the resulting model, and then prepare a Kaggle submission file in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T16:06:10.907159Z",
     "start_time": "2018-11-26T16:06:10.337159Z"
    }
   },
   "outputs": [],
   "source": [
    "def createKaggleSubmission(model, writeData = False, readData = False):\n",
    "    \n",
    "    # Pull in the labeled data\n",
    "    print(\"** Loading test data.\")\n",
    "    dataPath = os.path.join('.', 'datasets', 'imdb_movie_reviews')\n",
    "    testData = os.path.join(dataPath, 'testData.tsv')\n",
    "\n",
    "    testDF = pd.read_csv(testData, sep = '\\t', header = 0, quoting = 3)\n",
    "    testDF['id'] = testDF['id'].str.replace('\"', '')\n",
    "        \n",
    "    if readData:\n",
    "        print(\"\\n** Reading processed test data from disk.\")\n",
    "        with open('Model-06.p5.finalTest.pkl','rb') as f:\n",
    "            finalTest =  pickle.load(f) \n",
    "    \n",
    "    else:\n",
    "        # Sanity check\n",
    "        print(\"** Test data loaded into dataframe.\")\n",
    "        print('testDF.shape :', testDF.shape)\n",
    "        print(\"\")\n",
    "        print(testDF.head())\n",
    "\n",
    "        # Clean the test data\n",
    "        cleanTest = []\n",
    "\n",
    "        # Clean the reviews\n",
    "        print(\"\\n** Cleaning the test data.\")\n",
    "        for i, s in tqdm(enumerate(testDF.iloc[:,1])):\n",
    "            cleanTest.append(cleanReview(s, True))\n",
    "\n",
    "        # Examine a portion of the first clean review\n",
    "        print(\"Examine a portion of the first clean review:\\n\")\n",
    "        print(cleanTest[0][:10])\n",
    "\n",
    "\n",
    "        print(\"\\n** Processing the clean test data.\")\n",
    "        finalTest = []\n",
    "        for doc in tqdm(cleanTest):\n",
    "            finalTest.append(processCleanReview(doc, vocab))\n",
    "\n",
    "        print(\"** Processing complete.\")\n",
    "        print('Records:', len(finalTest))\n",
    "        print(\"First record:\")\n",
    "        print(finalTest[0])\n",
    "\n",
    "        if writeData:\n",
    "            print(\"\\n** Writing processed test data to disk.\")\n",
    "            with open('Model-06.p5.finalTest.pkl','wb') as f:\n",
    "                pickle.dump(finalTest, f)\n",
    "\n",
    "\n",
    "    # Create test data sequences for use by the Keras CNN model\n",
    "    print(\"\\n** Creating Keras sequences.\")\n",
    "    sequences = tokenizer.texts_to_sequences(finalTest)\n",
    "    data = pad_sequences(sequences, maxlen = seqLength, padding = \"post\")\n",
    "\n",
    "    print(\"\\n** Predicting classes.\")\n",
    "    yHat = model.predict(data)\n",
    "    yHat = np.round(yHat).astype(np.int)\n",
    "    print(\"** First 10 predictions:\")\n",
    "    print(yHat[:10])\n",
    "\n",
    "\n",
    "    # Add the predictions to the test data frame\n",
    "    print(\"\\n** Adding predictions to test data frame.\")\n",
    "    testDF['sentiment'] = yHat\n",
    "    testDF.head()\n",
    "\n",
    "\n",
    "    # Create the Kaggle submission file\n",
    "    print(\"\\n** Writing submission CSV file.\")\n",
    "    import csv\n",
    "    header = ['id', 'sentiment']\n",
    "    testDF.to_csv('submission.csv', columns = header, index = False, quoting = csv.QUOTE_NONE)\n",
    "    \n",
    "    print(\"\\n** Finished!\\n\")\n",
    "    \n",
    "    return testDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the data\n",
    "\n",
    "Previously covered [here](./Model-06.ipynb#Examine-the-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labeled training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Previous process justification and methodology also previously covered [here](./Model-06.ipynb#Cleaning-and-preprocessing).)\n",
    "\n",
    "We need to load the labeled training data exactly as we've done in previous write-ups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T16:06:24.670958Z",
     "start_time": "2018-11-26T16:06:21.745159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape : (25000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Pull in the labeled data\n",
    "df = pd.read_csv(labeledTrainData, sep = '\\t', header = 0, quoting = 3)\n",
    "\n",
    "# Sanity check\n",
    "print('df.shape :', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean/Process reviews\n",
    "\n",
    "Take a given sentence and process/clean it (i.e. remove HTML and other cruft, lower case the text, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T16:06:28.237958Z",
     "start_time": "2018-11-26T16:06:27.695958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Update stop word helper function to output a list of words\n",
    "\n",
    "# Clean IMDB review text\n",
    "def cleanReview(review, removeStopWords = False, applyStemming = False):\n",
    "    \n",
    "    # Convert the stop words to a set\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Remove HTML\n",
    "    clean = BeautifulSoup(review).get_text()\n",
    "    \n",
    "    # Expand contractions (i.e. wasn't => was not)\n",
    "    clean = expandContractions(clean)\n",
    "    \n",
    "    # Remove non-alpha chars\n",
    "    clean = re.sub(\"[^a-zA-Z]\", ' ', clean)\n",
    "    \n",
    "    # Convert to lower case and \"tokenize\"\n",
    "    clean = clean.lower().split()\n",
    "       \n",
    "    # Remove stop words and add to global vocab\n",
    "    if removeStopWords:\n",
    "        clean = [x for x in clean if not x in stopWords]\n",
    "        \n",
    "    # Stemming\n",
    "    if applyStemming:\n",
    "        clean = [PorterStemmer().stem(x) for x in clean]\n",
    "    \n",
    "    # Return results\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1. Remove any words from the cleaned review not found in the vocab tokens list\n",
    "2. Remove any words with length less than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T16:06:28.797958Z",
     "start_time": "2018-11-26T16:06:28.239958Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def processCleanReview(review, vocab):\n",
    "    results = []\n",
    "    \n",
    "    for x in review:\n",
    "        if (x in vocab and len(x) > 1):\n",
    "            results.append(x)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A:  Read previous text cleaning work from disk\n",
    "\n",
    "__Run the block below if you've already processed the data and saved the vocab to disk__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:38:34.891916Z",
     "start_time": "2018-11-19T20:38:33.867916Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('Model-06.p5.finalDocs.pkl','rb') as f:\n",
    "    finalDocs =  pickle.load(f)\n",
    "    \n",
    "with open('Model-06.p5.vocab.pkl','rb') as f:\n",
    "    vocab =  pickle.load(f)\n",
    "\n",
    "with open('Model-06.p5.counts.pkl','wb') as f:\n",
    "    counts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T16:38:37.122509Z",
     "start_time": "2018-11-20T16:38:36.506509Z"
    }
   },
   "source": [
    "### Option B:  Perform text cleaning work and write to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick examination of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:38:25.069916Z",
     "start_time": "2018-11-19T20:38:24.452916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['look',\n",
       " 'quo',\n",
       " 'vadi',\n",
       " 'local',\n",
       " 'video',\n",
       " 'store',\n",
       " 'found',\n",
       " 'version',\n",
       " 'look',\n",
       " 'interest',\n",
       " 'wow',\n",
       " 'amaz']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine\n",
    "cleanReview(df.iloc[25,2], True, True)[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the reviews and create the vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T19:03:00.381341Z",
     "start_time": "2018-11-19T19:02:08.571916Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000it [00:51, 488.16it/s]\n"
     ]
    }
   ],
   "source": [
    "cleanDocs = []\n",
    "vocab = Counter()\n",
    "\n",
    "# Clean the reviews\n",
    "for i, s in tqdm(enumerate(df.iloc[:,2])):\n",
    "    _ = cleanReview(s, True)\n",
    "    cleanDocs.append(_)\n",
    "    vocab.update(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine some vocab metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T16:36:00.115509Z",
     "start_time": "2018-11-20T16:35:59.519509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46199\n",
      "['stuff', 'going', 'moment', 'mj', 'started']\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(vocab[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is the vocab if we only consider words appearing at least N times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T19:03:01.616217Z",
     "start_time": "2018-11-19T19:03:00.996279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46199\n"
     ]
    }
   ],
   "source": [
    "ocurring = 2\n",
    "vocab = [k for k,c in vocab.items() if c >= ocurring]\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cleanReview(df.iloc[25,2], True, True)\n",
    "processCleanReview(_, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to do final processing of the cleaned reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDocs = []\n",
    "\n",
    "for doc in tqdm(cleanDocs):\n",
    "    finalDocs.append(processCleanReview(doc, vocab))\n",
    "    \n",
    "print(len(finalDocs))\n",
    "print(finalDocs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a collection of review sizes in words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:39:12.710916Z",
     "start_time": "2018-11-19T20:39:12.099916Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000it [00:00, 1666655.01it/s]\n"
     ]
    }
   ],
   "source": [
    "counts = []\n",
    "\n",
    "for i, d in tqdm(enumerate(finalDocs)):\n",
    "    counts.append(len(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the max and min review sizes in words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:39:14.950916Z",
     "start_time": "2018-11-19T20:39:14.361916Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max review size in words: 1418\n",
      "Min review size in words: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Max review size in words:\", max(counts))\n",
    "print(\"Min review size in words:\", min(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspect the max review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:39:32.943916Z",
     "start_time": "2018-11-19T20:39:32.360916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'match tag team table match bubba ray spike dudley vs eddie guerrero chris benoit bubba ray spike dudley started things tag team table match eddie guerrero chris benoit according rules match opponents go tables order get win benoit guerrero heated early taking turns hammering first spike bubba ray german suplex benoit bubba took wind dudley brother spike tried help brother referee restrained benoit guerrero ganged corner benoit stomping away bubba guerrero set table outside spike dashed ring somersaulted top rope onto guerrero outside recovering taking care spike guerrero slipped table ring helped wolverine set tandem set double superplex middle rope'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(finalDocs[counts.index(max(counts))][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspect the min review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:39:33.539916Z",
     "start_time": "2018-11-19T20:39:32.945916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie terrible good effects'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(finalDocs[counts.index(min(counts))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the 10 largest and 10 smallest reviews in words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:39:35.434916Z",
     "start_time": "2018-11-19T20:39:34.840916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 largest reviews in words: [700, 707, 735, 796, 797, 812, 874, 913, 924, 1418]\n",
      "10 smallest reviews in words: [4, 6, 6, 7, 7, 8, 8, 8, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "counts.sort()\n",
    "print(\"10 largest reviews in words:\", counts[-10:])\n",
    "print(\"10 smallest reviews in words:\", counts[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the cleanDocs vocab, and counts to save time if/when we run this again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T19:08:31.135916Z",
     "start_time": "2018-11-19T19:08:29.191916Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('Model-06.p5.finalDocs.pkl','wb') as f:\n",
    "    pickle.dump(finalDocs, f)\n",
    "    \n",
    "with open('Model-06.p5.vocab.pkl','wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "    \n",
    "with open('Model-06.p5.counts.pkl','wb') as f:\n",
    "    pickle.dump(counts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest review in words frankly looks like junk.  We are going to ignore it and make the max seq. length the 2nd largest review size in words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T19:46:02.685509Z",
     "start_time": "2018-11-20T19:46:02.011509Z"
    }
   },
   "source": [
    "vocabSize = len(vocab)\n",
    "seqLength = counts[-2]\n",
    "\n",
    "print(\"Vocab size:\", vocabSize)\n",
    "print(\"seqLength:\", seqLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T19:46:43.316509Z",
     "start_time": "2018-11-20T19:46:40.328509Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocabSize)\n",
    "tokenizer.fit_on_texts(vocab)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(finalDocs)\n",
    "data = pad_sequences(sequences, maxlen = seqLength, padding = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T21:25:44.360509Z",
     "start_time": "2018-11-20T21:25:43.731509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46199\n",
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "print(len(sequences))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm Keras was smart enough to understand we already tokenized the review text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T19:52:54.891509Z",
     "start_time": "2018-11-20T19:52:54.269509Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(len(sequences) == len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also examine what the final product looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T19:52:55.512509Z",
     "start_time": "2018-11-20T19:52:54.893509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     2,     3, ...,     0,     0,     0],\n",
       "       [  167,   168,   169, ...,     0,     0,     0],\n",
       "       [   32,    76,   225, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [   19,  2353,    16, ...,     0,     0,     0],\n",
       "       [ 1235,    10, 19681, ...,     0,     0,     0],\n",
       "       [ 1063,    66,  3099, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T18:26:28.198509Z",
     "start_time": "2018-11-20T18:26:27.517509Z"
    }
   },
   "source": [
    "## Algorithm v1:  LSTM\n",
    "\n",
    "We'll utilize a LSTM algorithm to create the fist model and assess its predictive powers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T19:54:07.604509Z",
     "start_time": "2018-11-20T19:54:06.095509Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, 100, input_length = seqLength))\n",
    "model.add(LSTM(100, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:08:30.137509Z",
     "start_time": "2018-11-20T19:54:07.606509Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f02b5df40f4d2baa6ae6da26df7590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=3, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c01dcba039449c0bada37e6e438bc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=15000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897ecf24be4e44cba788eecee44ca5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=15000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b620296663834fecb4a08dbc74e45542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=15000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b082cbe0>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    data, \n",
    "    df.iloc[:, 1], \n",
    "    validation_split = 0.4, \n",
    "    epochs = 3, \n",
    "    verbose = 0, \n",
    "    callbacks = [TQDMNotebookCallback(leave_inner = True, leave_outer = True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "Final epoch reported loss: 0.693, acc: 0.495, val_loss: 0.693, val_acc: 0.501.\n",
    "\n",
    "Kaggle score model v1.0:  0.84780\n",
    "\n",
    "Pretty poor results...   Let's try another, deeper model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm v2:  CNN\n",
    "\n",
    "We'll switch over to a CNN for our second attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:13:02.501509Z",
     "start_time": "2018-11-20T20:13:01.883509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create, compile, and return the CNN model\n",
    "def createModelv2(vocabSize, seqLength):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabSize, 100, input_length = seqLength))\n",
    "    model.add(Conv1D(filters = 32, kernel_size = 8, activation = 'relu'))\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:13:03.321509Z",
     "start_time": "2018-11-20T20:13:02.503509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_66 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 917, 32)           25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 458, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_64 (Flatten)         (None, 14656)             0         \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 10)                146570    \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,792,113\n",
      "Trainable params: 4,792,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m2 = createModelv2(vocabSize, seqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:20:38.103509Z",
     "start_time": "2018-11-20T20:13:03.322509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd099c14b40451baaf860cfd705a97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=3, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47277c551a945968f3669066494a3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=25000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfe13ef05dc49a880bbb3a5490b362b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=25000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c56a873fec64cccbd48f6e8384b58b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=25000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b483aef0>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.fit(\n",
    "    data, \n",
    "    df.iloc[:, 1], \n",
    "    epochs = 3, \n",
    "    verbose = 0, \n",
    "    callbacks = [TQDMNotebookCallback(leave_inner = True, leave_outer = True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:20:41.491509Z",
     "start_time": "2018-11-20T20:20:38.105509Z"
    }
   },
   "outputs": [],
   "source": [
    "m2.save('Model-06.p5.CNN-v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:35:25.213509Z",
     "start_time": "2018-11-20T20:34:43.333509Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Loading test data.\n",
      "\n",
      "** Reading processed test data from disk.\n",
      "\n",
      "** Creating Keras sequences.\n",
      "\n",
      "** Predicting classes.\n",
      "** First 10 predictions:\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "** Adding predictions to test data frame.\n",
      "\n",
      "** Writing submission CSV file.\n",
      "\n",
      "** Finished!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m2DF = createKaggleSubmission(m2, writeData = False, readData = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle score model v2.0:  0.85424\n",
    "\n",
    "The CNN have provided some improvement to the test set predictive Kaggle score.  Next we'll examine hyperparameter fine tuning, and see if we can further increase accuracy gains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm v3:  CNN with tuning\n",
    "\n",
    "Let's do some gridsearch on the algorithm's hyperparameters, and see if we can reduce the variance issues (i.e. over fitting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T23:44:24.344598Z",
     "start_time": "2018-11-19T23:44:23.724617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create, compile, and return the CNN model\n",
    "def createModelv3(vocabSize, seqLength, filters, kernelSize, debug = False):    \n",
    "    if debug:\n",
    "        print(\"\\n*****\")\n",
    "        print(\"filters:\", filters)\n",
    "        print(\"kernel_size:\", kernel_size)\n",
    "        print(\"*****\\n\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabSize, 100, input_length = seqLength))\n",
    "    model.add(Conv1D(filters = filters, kernel_size = kernelSize, activation = 'relu'))\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    if debug:\n",
    "        model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T23:45:15.810121Z",
     "start_time": "2018-11-19T23:45:15.207138Z"
    }
   },
   "outputs": [],
   "source": [
    "m3 = KerasClassifier(\n",
    "    build_fn = define_model2, \n",
    "    vocab_size = vocabSize, \n",
    "    max_length = seqLength, \n",
    "    verbose = 0,\n",
    "    epochs = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T03:33:25.908909Z",
     "start_time": "2018-11-19T23:45:23.365903Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_32 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 919, 32)           19232     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 459, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 14688)             0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 10)                146890    \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,786,033\n",
      "Trainable params: 4,786,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_33 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 919, 32)           19232     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 459, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 14688)             0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 10)                146890    \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,786,033\n",
      "Trainable params: 4,786,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_34 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 919, 32)           19232     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 459, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 14688)             0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 10)                146890    \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,786,033\n",
      "Trainable params: 4,786,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_35 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 917, 32)           25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 458, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 14656)             0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 10)                146570    \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,792,113\n",
      "Trainable params: 4,792,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_36 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 917, 32)           25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 458, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_36 (Flatten)         (None, 14656)             0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 10)                146570    \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,792,113\n",
      "Trainable params: 4,792,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_37 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 917, 32)           25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 458, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_37 (Flatten)         (None, 14656)             0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 10)                146570    \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,792,113\n",
      "Trainable params: 4,792,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_38 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 915, 32)           32032     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 457, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_38 (Flatten)         (None, 14624)             0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 10)                146250    \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,798,193\n",
      "Trainable params: 4,798,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_39 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 915, 32)           32032     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 457, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_39 (Flatten)         (None, 14624)             0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 10)                146250    \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,798,193\n",
      "Trainable params: 4,798,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_40 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 915, 32)           32032     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 457, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 14624)             0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 10)                146250    \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,798,193\n",
      "Trainable params: 4,798,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_41 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 919, 100)          60100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 459, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         (None, 45900)             0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 10)                459010    \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,139,021\n",
      "Trainable params: 5,139,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_42 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 919, 100)          60100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 459, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_42 (Flatten)         (None, 45900)             0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 10)                459010    \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,139,021\n",
      "Trainable params: 5,139,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_43 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 919, 100)          60100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 459, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_43 (Flatten)         (None, 45900)             0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 10)                459010    \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,139,021\n",
      "Trainable params: 5,139,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_44 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 917, 100)          80100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 458, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 45800)             0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 10)                458010    \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,158,021\n",
      "Trainable params: 5,158,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_45 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 917, 100)          80100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 458, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 45800)             0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 10)                458010    \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,158,021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 5,158,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_46 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 917, 100)          80100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 458, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 45800)             0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 10)                458010    \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,158,021\n",
      "Trainable params: 5,158,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_47 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 915, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 457, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 45700)             0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 10)                457010    \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,177,021\n",
      "Trainable params: 5,177,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_48 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 915, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 457, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_48 (Flatten)         (None, 45700)             0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 10)                457010    \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,177,021\n",
      "Trainable params: 5,177,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_49 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 915, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 457, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 45700)             0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 10)                457010    \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,177,021\n",
      "Trainable params: 5,177,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_50 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 919, 200)          120200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 459, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 91800)             0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 10)                918010    \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,658,121\n",
      "Trainable params: 5,658,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_51 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 919, 200)          120200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 459, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 91800)             0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 10)                918010    \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,658,121\n",
      "Trainable params: 5,658,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_52 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 919, 200)          120200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 459, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_52 (Flatten)         (None, 91800)             0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 10)                918010    \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,658,121\n",
      "Trainable params: 5,658,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_53 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 917, 200)          160200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 458, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_53 (Flatten)         (None, 91600)             0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 10)                916010    \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,696,121\n",
      "Trainable params: 5,696,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_54 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 917, 200)          160200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 458, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_54 (Flatten)         (None, 91600)             0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 10)                916010    \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,696,121\n",
      "Trainable params: 5,696,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_55 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 917, 200)          160200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 458, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_55 (Flatten)         (None, 91600)             0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 10)                916010    \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,696,121\n",
      "Trainable params: 5,696,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_56 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 915, 200)          200200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 457, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_56 (Flatten)         (None, 91400)             0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 10)                914010    \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,734,121\n",
      "Trainable params: 5,734,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_57 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 915, 200)          200200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 457, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_57 (Flatten)         (None, 91400)             0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 10)                914010    \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,734,121\n",
      "Trainable params: 5,734,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_58 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 915, 200)          200200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 457, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_58 (Flatten)         (None, 91400)             0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 10)                914010    \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,734,121\n",
      "Trainable params: 5,734,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_59 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 919, 100)          60100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 459, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_59 (Flatten)         (None, 45900)             0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 10)                459010    \n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_118 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,139,021\n",
      "Trainable params: 5,139,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(\n",
    "    filters=[32, 100, 200], \n",
    "    kernel_size = [6, 8, 10]\n",
    ")\n",
    "grid = GridSearchCV(\n",
    "    estimator = m3, \n",
    "    param_grid = param_grid, \n",
    "    n_jobs = 1\n",
    ")\n",
    "grid_result = grid.fit(data, df.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:39:24.706509Z",
     "start_time": "2018-11-20T20:39:24.098509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8776800000119209\n",
      "{'filters': 100, 'kernel_size': 6}\n"
     ]
    }
   ],
   "source": [
    "# Examine the best score and model params\n",
    "print(grid_result.best_score_)\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T21:20:18.976509Z",
     "start_time": "2018-11-20T21:20:18.259509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attach to the best model\n",
    "bestModel = grid_result.best_estimator_ \n",
    "\n",
    "# Save it\n",
    "bestModel.model.save('Model-06.p5.CNN-v3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T18:17:30.428509Z",
     "start_time": "2018-11-20T18:16:37.268509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Loading test data.\n",
      "\n",
      "** Reading processed test data from disk.\n",
      "\n",
      "** Creating Keras sequences.\n",
      "\n",
      "** Predicting classes.\n",
      "** First 10 predictions:\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "\n",
      "** Adding predictions to test data frame.\n",
      "\n",
      "** Writing submission CSV file.\n",
      "\n",
      "** Finished!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF = createKaggleSubmission(bestModel, writeData = False, readData = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T16:28:55.511509Z",
     "start_time": "2018-11-20T16:28:54.903509Z"
    }
   },
   "source": [
    "Kaggle score model v3.0:  0.86752\n",
    "\n",
    "Again we have improvements which is a result of the hyperparameter fine tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-08T16:48:34.065Z"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I continued to work on CNN hyperparameter fine tuning I came across a number of papers detailing transfer learning on NLP tasks.  I felt this was a more promising line of exploration, because further non-trivial improvements on the CNN were slow in materializing.  As such I decided to stop work on this write-up, and instead spend some time on utilizing pre-trained networks in a separate write-up.\n",
    "\n",
    "In this write-up we accomplished the following:\n",
    "\n",
    "\n",
    "* Createe a feature set of document vectors from the IMDb movie review text utilizing Doc2Vec\n",
    "* Utilized LSTM and CNN algorithms to create predictive models from the feature set(s) created above\n",
    "* Obtained Kaggle scores on the outputs of the models\n",
    "\n",
    "The best Kaggle score we achieved was 0.86752 on the tuned version of the CNN algorithm.  This score isn't bad for our first serious Kaggle submission; however, there is clearly room for improvement which hopefully we'll gain utilizing NLP transfer learning techniques in the next write-up.\n",
    "\n",
    "Performance metrics so far:\n",
    "\n",
    "|Model     |Kaggle Score|\n",
    "|----------|------------|\n",
    "|LST       | 0.84780    |\n",
    "|CNN       | 0.85424    |\n",
    "|Tuned CNN | 0.86752    |\n",
    "<div style=\"clear:both\"></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
