{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#IMDB-Movie-Review-Sentiment-Classification\" data-toc-modified-id=\"IMDB-Movie-Review-Sentiment-Classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>IMDB Movie Review Sentiment Classification</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Process\" data-toc-modified-id=\"Process-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Process</a></span></li><li><span><a href=\"#Configure-notebook,-import-libraries,-and-import-dataset\" data-toc-modified-id=\"Configure-notebook,-import-libraries,-and-import-dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Configure notebook, import libraries, and import dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries\" data-toc-modified-id=\"Import-libraries-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Import libraries</a></span></li><li><span><a href=\"#Define-global-variables\" data-toc-modified-id=\"Define-global-variables-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Define global variables</a></span></li></ul></li><li><span><a href=\"#Helper-Functions\" data-toc-modified-id=\"Helper-Functions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Helper Functions</a></span></li><li><span><a href=\"#Examine-the-data\" data-toc-modified-id=\"Examine-the-data-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Examine the data</a></span></li><li><span><a href=\"#Cleaning-and-preprocessing\" data-toc-modified-id=\"Cleaning-and-preprocessing-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Cleaning and preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-labeled-training-data\" data-toc-modified-id=\"Load-labeled-training-data-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Load labeled training data</a></span></li><li><span><a href=\"#Clean/Process-reviews\" data-toc-modified-id=\"Clean/Process-reviews-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Clean/Process reviews</a></span><ul class=\"toc-item\"><li><span><a href=\"#Option-A:--Read-previous-text-cleaning-work-from-disk\" data-toc-modified-id=\"Option-A:--Read-previous-text-cleaning-work-from-disk-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>Option A:  Read previous text cleaning work from disk</a></span></li><li><span><a href=\"#Option-B:--Perform-text-cleaning-work-and-write-to-disk\" data-toc-modified-id=\"Option-B:--Perform-text-cleaning-work-and-write-to-disk-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>Option B:  Perform text cleaning work and write to disk</a></span></li></ul></li></ul></li><li><span><a href=\"#Develop-LSTM-models\" data-toc-modified-id=\"Develop-LSTM-models-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Develop LSTM models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenize-the-cleaned-data\" data-toc-modified-id=\"Tokenize-the-cleaned-data-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Tokenize the cleaned data</a></span></li><li><span><a href=\"#LSTM-v1.0\" data-toc-modified-id=\"LSTM-v1.0-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>LSTM v1.0</a></span></li><li><span><a href=\"#LSTM-v2.0\" data-toc-modified-id=\"LSTM-v2.0-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>LSTM v2.0</a></span></li><li><span><a href=\"#CNN-v3.0\" data-toc-modified-id=\"CNN-v3.0-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>CNN v3.0</a></span></li><li><span><a href=\"#CNN-v4.0\" data-toc-modified-id=\"CNN-v4.0-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>CNN v4.0</a></span></li><li><span><a href=\"#Comments\" data-toc-modified-id=\"Comments-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;</span>Comments</a></span></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>IMDB Movie Review Sentiment Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left; margin-right: 15px; width: 30%; height: 30%;\" src=\"images/imdb.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The overall goal of this set of write-ups is to explore a number of machine learning algorithms utilizing natural language processing (NLP) to classify sentiment IMDB movie reviews.\n",
    "\n",
    "The specific goals of this write-up include:\n",
    "1. Create a set of document vectors from the IMDb movie review text utilizing [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "2. Tune and train a number of Doc2Vec models on the movie review corpus \n",
    "2. Run the models from the [first write-up](./Model-06.ipynb) against the Doc2Vec feature set outputs\n",
    "3. Determine if utilizing Doc2Vec improves our ability to correctly classify movie review sentiment\n",
    "\n",
    "This series of write-ups is inspired by the Kaggle [\n",
    "Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial) competition.\n",
    "\n",
    "References:\n",
    "* [Gensim Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "* [Original paper](https://arxiv.org/abs/1405.4053) by Mikilov and Le\n",
    "\n",
    "Dataset source:  [IMDB Movie Reviews](https://www.kaggle.com/c/word2vec-nlp-tutorial/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "\n",
    "Previously covered [here](./Model-06.ipynb#Process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure notebook, import libraries, and import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T15:16:22.416553Z",
     "start_time": "2018-10-16T15:16:22.413553Z"
    }
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T23:31:17.760226Z",
     "start_time": "2018-11-20T23:31:16.215777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import set_option\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# http://www.nltk.org/index.html\n",
    "# pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# Creating function implementing punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "\n",
    "# Only need this the first time...\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "# pip install BeautifulSoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# https://pypi.org/project/gensim/\n",
    "# pip install gensim\n",
    "import gensim.models.doc2vec\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "#import multiprocessing\n",
    "\n",
    "#cores = multiprocessing.cpu_count()\n",
    "#assert(gensim.models.doc2vec.FAST_VERSION > -1, \"Going to be slow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:38:09.409916Z",
     "start_time": "2018-11-19T20:38:08.815916Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Opens a GUI that allows us to download the NLTK data\n",
    "# nltk.download()\n",
    "\n",
    "dataPath = os.path.join('.', 'datasets', 'imdb_movie_reviews')\n",
    "labeledTrainData = os.path.join(dataPath, 'labeledTrainData.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:38:08.209916Z",
     "start_time": "2018-11-19T20:38:07.523916Z"
    }
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "contractionsObj = re.compile('(%s)' % '|'.join(contractions.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T18:22:27.645509Z",
     "start_time": "2018-11-20T18:22:27.027509Z"
    }
   },
   "outputs": [],
   "source": [
    "def expandContractions(txt, contractions = contractions):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractionsObj.sub(replace, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T18:18:59.117509Z",
     "start_time": "2018-11-20T18:18:58.502509Z"
    }
   },
   "outputs": [],
   "source": [
    "def createKaggleSubmission(model, writeData = False, readData = False):\n",
    "    \n",
    "    # Pull in the labeled data\n",
    "    print(\"** Loading test data.\")\n",
    "    dataPath = os.path.join('.', 'datasets', 'imdb_movie_reviews')\n",
    "    testData = os.path.join(dataPath, 'testData.tsv')\n",
    "\n",
    "    testDF = pd.read_csv(testData, sep = '\\t', header = 0, quoting = 3)\n",
    "    testDF['id'] = testDF['id'].str.replace('\"', '')\n",
    "        \n",
    "    if readData:\n",
    "        print(\"\\n** Reading processed test data from disk.\")\n",
    "        with open('Model-06.p5.finalTest.pkl','rb') as f:\n",
    "            finalTest =  pickle.load(f) \n",
    "    \n",
    "    else:\n",
    "        # Sanity check\n",
    "        print(\"** Test data loaded into dataframe.\")\n",
    "        print('testDF.shape :', testDF.shape)\n",
    "        print(\"\")\n",
    "        print(testDF.head())\n",
    "\n",
    "        # Clean the test data\n",
    "        cleanTest = []\n",
    "\n",
    "        # Clean the reviews\n",
    "        print(\"\\n** Cleaning the test data.\")\n",
    "        for i, s in tqdm(enumerate(testDF.iloc[:,1])):\n",
    "            cleanTest.append(cleanReview(s, True))\n",
    "\n",
    "        # Examine a portion of the first clean review\n",
    "        print(\"Examine a portion of the first clean review:\\n\")\n",
    "        print(cleanTest[0][:10])\n",
    "\n",
    "\n",
    "        print(\"\\n** Processing the clean test data.\")\n",
    "        finalTest = []\n",
    "        for doc in tqdm(cleanTest):\n",
    "            finalTest.append(processCleanReview(doc, vocab))\n",
    "\n",
    "        print(\"** Processing complete.\")\n",
    "        print('Records:', len(finalTest))\n",
    "        print(\"First record:\")\n",
    "        print(finalTest[0])\n",
    "\n",
    "        if writeData:\n",
    "            print(\"\\n** Writing processed test data to disk.\")\n",
    "            with open('Model-06.p5.finalTest.pkl','wb') as f:\n",
    "                pickle.dump(finalTest, f)\n",
    "\n",
    "\n",
    "    # Create test data sequences for use by the Keras CNN model\n",
    "    print(\"\\n** Creating Keras sequences.\")\n",
    "    sequences = tokenizer.texts_to_sequences(finalTest)\n",
    "    data = pad_sequences(sequences, maxlen = seqLength, padding = \"post\")\n",
    "\n",
    "    print(\"\\n** Predicting classes.\")\n",
    "    yHat = model.predict(data)\n",
    "    yHat = np.round(yHat).astype(np.int)\n",
    "    print(\"** First 10 predictions:\")\n",
    "    print(yHat[:10])\n",
    "\n",
    "\n",
    "    # Add the predictions to the test data frame\n",
    "    print(\"\\n** Adding predictions to test data frame.\")\n",
    "    testDF['sentiment'] = yHat\n",
    "    testDF.head()\n",
    "\n",
    "\n",
    "    # Create the Kaggle submission file\n",
    "    print(\"\\n** Writing submission CSV file.\")\n",
    "    import csv\n",
    "    header = ['id', 'sentiment']\n",
    "    testDF.to_csv('submission.csv', columns = header, index = False, quoting = csv.QUOTE_NONE)\n",
    "    \n",
    "    print(\"\\n** Finished!\\n\")\n",
    "    \n",
    "    return testDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the data\n",
    "\n",
    "Previously covered [here](./Model-06.ipynb#Examine-the-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labeled training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Previous process justification and methodology also previously covered [here](./Model-06.ipynb#Cleaning-and-preprocessing).)\n",
    "\n",
    "We need to load the labeled training data exactly as we've done in previous write-ups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:38:16.486916Z",
     "start_time": "2018-11-19T20:38:15.529916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape : (25000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Pull in the labeled data\n",
    "df = pd.read_csv(labeledTrainData, sep = '\\t', header = 0, quoting = 3)\n",
    "\n",
    "# Sanity check\n",
    "print('df.shape :', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean/Process reviews\n",
    "\n",
    "Take a given sentence and process/clean it (i.e. remove HTML and other cruft, lower case the text, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:38:21.505916Z",
     "start_time": "2018-11-19T20:38:20.922916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Update stop word helper function to output a list of words\n",
    "\n",
    "# Clean IMDB review text\n",
    "def cleanReview(review, removeStopWords = False, applyStemming = False):\n",
    "    \n",
    "    # Convert the stop words to a set\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Remove HTML\n",
    "    clean = BeautifulSoup(review).get_text()\n",
    "    \n",
    "    # Expand contractions (i.e. wasn't => was not)\n",
    "    clean = expandContractions(clean)\n",
    "    \n",
    "    # Remove non-alpha chars\n",
    "    clean = re.sub(\"[^a-zA-Z]\", ' ', clean)\n",
    "    \n",
    "    # Convert to lower case and \"tokenize\"\n",
    "    clean = clean.lower().split()\n",
    "       \n",
    "    # Remove stop words and add to global vocab\n",
    "    if removeStopWords:\n",
    "        clean = [x for x in clean if not x in stopWords]\n",
    "        \n",
    "    # Stemming\n",
    "    if applyStemming:\n",
    "        clean = [PorterStemmer().stem(x) for x in clean]\n",
    "    \n",
    "    # Return results\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T16:41:06.986509Z",
     "start_time": "2018-11-20T16:41:06.380509Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Remove any words from the cleaned review not found in the vocab tokens list\n",
    "# 2. Remove any words with length less than 2\n",
    "def processCleanReview(review, vocab):\n",
    "    results = []\n",
    "    \n",
    "    for x in review:\n",
    "        if (x in vocab and len(x) > 1):\n",
    "            results.append(x)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A:  Read previous text cleaning work from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:38:34.891916Z",
     "start_time": "2018-11-19T20:38:33.867916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run the block below if you've already processed the data and saved the vocab to disk\n",
    "with open('Model-06.p5.finalDocs.pkl','rb') as f:\n",
    "    finalDocs =  pickle.load(f)\n",
    "    \n",
    "with open('Model-06.p5.vocab.pkl','rb') as f:\n",
    "    vocab =  pickle.load(f)\n",
    "\n",
    "with open('Model-06.p5.counts.pkl','wb') as f:\n",
    "    counts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T16:38:37.122509Z",
     "start_time": "2018-11-20T16:38:36.506509Z"
    }
   },
   "source": [
    "### Option B:  Perform text cleaning work and write to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick examination of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:38:25.069916Z",
     "start_time": "2018-11-19T20:38:24.452916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['look',\n",
       " 'quo',\n",
       " 'vadi',\n",
       " 'local',\n",
       " 'video',\n",
       " 'store',\n",
       " 'found',\n",
       " 'version',\n",
       " 'look',\n",
       " 'interest',\n",
       " 'wow',\n",
       " 'amaz']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine\n",
    "cleanReview(df.iloc[25,2], True, True)[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T19:03:00.381341Z",
     "start_time": "2018-11-19T19:02:08.571916Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000it [00:51, 488.16it/s]\n"
     ]
    }
   ],
   "source": [
    "cleanDocs = []\n",
    "vocab = Counter()\n",
    "\n",
    "# Clean the reviews\n",
    "for i, s in tqdm(enumerate(df.iloc[:,2])):\n",
    "    _ = cleanReview(s, True)\n",
    "    cleanDocs.append(_)\n",
    "    vocab.update(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T16:36:00.115509Z",
     "start_time": "2018-11-20T16:35:59.519509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46199\n",
      "['stuff', 'going', 'moment', 'mj', 'started']\n"
     ]
    }
   ],
   "source": [
    "# Examine some vocab metrics\n",
    "print(len(vocab))\n",
    "print(vocab[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T19:03:01.616217Z",
     "start_time": "2018-11-19T19:03:00.996279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46199\n"
     ]
    }
   ],
   "source": [
    "# How big is the vocab if we only consider words appearing at least N times?\n",
    "ocurring = 2\n",
    "vocab = [k for k,c in vocab.items() if c >= ocurring]\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check test\n",
    "_ = cleanReview(df.iloc[25,2], True, True)\n",
    "processCleanReview(_, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to do final processing of the cleaned reviews\n",
    "\n",
    "finalDocs = []\n",
    "\n",
    "for doc in tqdm(cleanDocs):\n",
    "    finalDocs.append(processCleanReview(doc, vocab))\n",
    "    \n",
    "print(len(finalDocs))\n",
    "print(finalDocs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:39:12.710916Z",
     "start_time": "2018-11-19T20:39:12.099916Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000it [00:00, 1666655.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now create a collection of review sizes in words\n",
    "counts = []\n",
    "\n",
    "for i, d in tqdm(enumerate(finalDocs)):\n",
    "    counts.append(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:39:14.950916Z",
     "start_time": "2018-11-19T20:39:14.361916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max review size in words: 1418\n",
      "Min review size in words: 4\n"
     ]
    }
   ],
   "source": [
    "# What are the max and min review sizes in words?\n",
    "print(\"Max review size in words:\", max(counts))\n",
    "print(\"Min review size in words:\", min(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:39:32.943916Z",
     "start_time": "2018-11-19T20:39:32.360916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'match tag team table match bubba ray spike dudley vs eddie guerrero chris benoit bubba ray spike dudley started things tag team table match eddie guerrero chris benoit according rules match opponents go tables order get win benoit guerrero heated early taking turns hammering first spike bubba ray german suplex benoit bubba took wind dudley brother spike tried help brother referee restrained benoit guerrero ganged corner benoit stomping away bubba guerrero set table outside spike dashed ring somersaulted top rope onto guerrero outside recovering taking care spike guerrero slipped table ring helped wolverine set tandem set double superplex middle rope'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visually inspect the max review\n",
    "\" \".join(finalDocs[counts.index(max(counts))][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:39:33.539916Z",
     "start_time": "2018-11-19T20:39:32.945916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie terrible good effects'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visually inspect the min review\n",
    "\" \".join(finalDocs[counts.index(min(counts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:39:35.434916Z",
     "start_time": "2018-11-19T20:39:34.840916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 largest reviews in words: [700, 707, 735, 796, 797, 812, 874, 913, 924, 1418]\n",
      "10 smallest reviews in words: [4, 6, 6, 7, 7, 8, 8, 8, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "# View the 10 largest and 10 smallest reviews in words\n",
    "counts.sort()\n",
    "print(\"10 largest reviews in words:\", counts[-10:])\n",
    "print(\"10 smallest reviews in words:\", counts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T19:08:31.135916Z",
     "start_time": "2018-11-19T19:08:29.191916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickle the cleanDocs vocab, and counts to save time if/when we run this again\n",
    "with open('Model-06.p5.finalDocs.pkl','wb') as f:\n",
    "    pickle.dump(finalDocs, f)\n",
    "    \n",
    "with open('Model-06.p5.vocab.pkl','wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "    \n",
    "with open('Model-06.p5.counts.pkl','wb') as f:\n",
    "    pickle.dump(counts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop LSTM models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest review in words frankly looks like junk.  We are going to ignore it and make the max seq. length the 2nd largest review size in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T19:46:02.685509Z",
     "start_time": "2018-11-20T19:46:02.011509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 46199\n",
      "seqLength: 924\n"
     ]
    }
   ],
   "source": [
    "vocabSize = len(vocab)\n",
    "seqLength = counts[-2]\n",
    "\n",
    "print(\"Vocab size:\", vocabSize)\n",
    "print(\"seqLength:\", seqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T19:46:43.316509Z",
     "start_time": "2018-11-20T19:46:40.328509Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocabSize)\n",
    "tokenizer.fit_on_texts(vocab)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(finalDocs)\n",
    "data = pad_sequences(sequences, maxlen = seqLength, padding = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T21:25:44.360509Z",
     "start_time": "2018-11-20T21:25:43.731509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46199\n",
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "print(len(sequences))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T19:52:54.891509Z",
     "start_time": "2018-11-20T19:52:54.269509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's confirm Keras was smart enough to understand we already tokenized the review text\n",
    "assert(len(sequences) == len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T19:52:55.512509Z",
     "start_time": "2018-11-20T19:52:54.893509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     2,     3, ...,     0,     0,     0],\n",
       "       [  167,   168,   169, ...,     0,     0,     0],\n",
       "       [   32,    76,   225, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [   19,  2353,    16, ...,     0,     0,     0],\n",
       "       [ 1235,    10, 19681, ...,     0,     0,     0],\n",
       "       [ 1063,    66,  3099, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's also examine what the final product looks like\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T18:26:28.198509Z",
     "start_time": "2018-11-20T18:26:27.517509Z"
    }
   },
   "source": [
    "## LSTM v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T19:54:07.604509Z",
     "start_time": "2018-11-20T19:54:06.095509Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, 100, input_length = seqLength))\n",
    "model.add(LSTM(100, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:08:30.137509Z",
     "start_time": "2018-11-20T19:54:07.606509Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f02b5df40f4d2baa6ae6da26df7590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=3, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c01dcba039449c0bada37e6e438bc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=15000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897ecf24be4e44cba788eecee44ca5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=15000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b620296663834fecb4a08dbc74e45542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=15000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b082cbe0>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    data, \n",
    "    df.iloc[:, 1], \n",
    "    validation_split = 0.4, \n",
    "    epochs = 3, \n",
    "    verbose = 0, \n",
    "    callbacks = [TQDMNotebookCallback(leave_inner = True, leave_outer = True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final epoch reported loss: 0.693, acc: 0.495, val_loss: 0.693, val_acc: 0.501.\n",
    "\n",
    "Pretty poor results...   Let's try another, deeper model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:13:02.501509Z",
     "start_time": "2018-11-20T20:13:01.883509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create, compile, and return the CNN model\n",
    "def createModelv2(vocabSize, seqLength):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabSize, 100, input_length = seqLength))\n",
    "    model.add(Conv1D(filters = 32, kernel_size = 8, activation = 'relu'))\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:13:03.321509Z",
     "start_time": "2018-11-20T20:13:02.503509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_66 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 917, 32)           25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 458, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_64 (Flatten)         (None, 14656)             0         \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 10)                146570    \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,792,113\n",
      "Trainable params: 4,792,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m2 = createModelv2(vocabSize, seqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:20:38.103509Z",
     "start_time": "2018-11-20T20:13:03.322509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd099c14b40451baaf860cfd705a97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=3, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47277c551a945968f3669066494a3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=25000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfe13ef05dc49a880bbb3a5490b362b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=25000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c56a873fec64cccbd48f6e8384b58b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=25000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b483aef0>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.fit(\n",
    "    data, \n",
    "    df.iloc[:, 1], \n",
    "    epochs = 3, \n",
    "    verbose = 0, \n",
    "    callbacks = [TQDMNotebookCallback(leave_inner = True, leave_outer = True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:20:41.491509Z",
     "start_time": "2018-11-20T20:20:38.105509Z"
    }
   },
   "outputs": [],
   "source": [
    "m2.save('Model-06.p5.CNN-v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:35:25.213509Z",
     "start_time": "2018-11-20T20:34:43.333509Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Loading test data.\n",
      "\n",
      "** Reading processed test data from disk.\n",
      "\n",
      "** Creating Keras sequences.\n",
      "\n",
      "** Predicting classes.\n",
      "** First 10 predictions:\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "** Adding predictions to test data frame.\n",
      "\n",
      "** Writing submission CSV file.\n",
      "\n",
      "** Finished!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m2DF = createKaggleSubmission(m2, writeData = False, readData = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle score model v1.0:  0.84780\n",
    "\n",
    "Kaggle score model v2.0:  0.85424\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN v3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T21:50:07.472916Z",
     "start_time": "2018-11-19T21:50:06.850916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's do some gridsearch and see if we can reduce the variance issues (i.e. over fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T23:44:24.344598Z",
     "start_time": "2018-11-19T23:44:23.724617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create, compile, and return the CNN model\n",
    "def createModelv3(vocabSize, seqLength, filters, kernelSize, debug = False):    \n",
    "    if debug:\n",
    "        print(\"\\n*****\")\n",
    "        print(\"filters:\", filters)\n",
    "        print(\"kernel_size:\", kernel_size)\n",
    "        print(\"*****\\n\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabSize, 100, input_length = seqLength))\n",
    "    model.add(Conv1D(filters = filters, kernel_size = kernelSize, activation = 'relu'))\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    if debug:\n",
    "        model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T23:45:15.810121Z",
     "start_time": "2018-11-19T23:45:15.207138Z"
    }
   },
   "outputs": [],
   "source": [
    "m3 = KerasClassifier(\n",
    "    build_fn = define_model2, \n",
    "    vocab_size = vocabSize, \n",
    "    max_length = seqLength, \n",
    "    verbose = 0,\n",
    "    epochs = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T03:33:25.908909Z",
     "start_time": "2018-11-19T23:45:23.365903Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_32 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 919, 32)           19232     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 459, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 14688)             0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 10)                146890    \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,786,033\n",
      "Trainable params: 4,786,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_33 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 919, 32)           19232     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 459, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 14688)             0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 10)                146890    \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,786,033\n",
      "Trainable params: 4,786,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_34 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 919, 32)           19232     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 459, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 14688)             0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 10)                146890    \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,786,033\n",
      "Trainable params: 4,786,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_35 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 917, 32)           25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 458, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 14656)             0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 10)                146570    \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,792,113\n",
      "Trainable params: 4,792,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_36 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 917, 32)           25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 458, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_36 (Flatten)         (None, 14656)             0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 10)                146570    \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,792,113\n",
      "Trainable params: 4,792,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_37 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 917, 32)           25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 458, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_37 (Flatten)         (None, 14656)             0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 10)                146570    \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,792,113\n",
      "Trainable params: 4,792,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_38 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 915, 32)           32032     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 457, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_38 (Flatten)         (None, 14624)             0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 10)                146250    \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,798,193\n",
      "Trainable params: 4,798,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_39 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 915, 32)           32032     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 457, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_39 (Flatten)         (None, 14624)             0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 10)                146250    \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,798,193\n",
      "Trainable params: 4,798,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 32\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_40 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 915, 32)           32032     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 457, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 14624)             0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 10)                146250    \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,798,193\n",
      "Trainable params: 4,798,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_41 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 919, 100)          60100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 459, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         (None, 45900)             0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 10)                459010    \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,139,021\n",
      "Trainable params: 5,139,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_42 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 919, 100)          60100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 459, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_42 (Flatten)         (None, 45900)             0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 10)                459010    \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,139,021\n",
      "Trainable params: 5,139,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_43 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 919, 100)          60100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 459, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_43 (Flatten)         (None, 45900)             0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 10)                459010    \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,139,021\n",
      "Trainable params: 5,139,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_44 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 917, 100)          80100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 458, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 45800)             0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 10)                458010    \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,158,021\n",
      "Trainable params: 5,158,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_45 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 917, 100)          80100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 458, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 45800)             0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 10)                458010    \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,158,021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 5,158,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_46 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 917, 100)          80100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 458, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 45800)             0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 10)                458010    \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,158,021\n",
      "Trainable params: 5,158,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_47 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 915, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 457, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 45700)             0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 10)                457010    \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,177,021\n",
      "Trainable params: 5,177,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_48 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 915, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 457, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_48 (Flatten)         (None, 45700)             0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 10)                457010    \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,177,021\n",
      "Trainable params: 5,177,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_49 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 915, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 457, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 45700)             0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 10)                457010    \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,177,021\n",
      "Trainable params: 5,177,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_50 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 919, 200)          120200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 459, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 91800)             0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 10)                918010    \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,658,121\n",
      "Trainable params: 5,658,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_51 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 919, 200)          120200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 459, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 91800)             0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 10)                918010    \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,658,121\n",
      "Trainable params: 5,658,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_52 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 919, 200)          120200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 459, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_52 (Flatten)         (None, 91800)             0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 10)                918010    \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,658,121\n",
      "Trainable params: 5,658,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_53 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 917, 200)          160200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 458, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_53 (Flatten)         (None, 91600)             0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 10)                916010    \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,696,121\n",
      "Trainable params: 5,696,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_54 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 917, 200)          160200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 458, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_54 (Flatten)         (None, 91600)             0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 10)                916010    \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,696,121\n",
      "Trainable params: 5,696,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 8\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_55 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 917, 200)          160200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 458, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_55 (Flatten)         (None, 91600)             0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 10)                916010    \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,696,121\n",
      "Trainable params: 5,696,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_56 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 915, 200)          200200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 457, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_56 (Flatten)         (None, 91400)             0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 10)                914010    \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,734,121\n",
      "Trainable params: 5,734,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_57 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 915, 200)          200200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 457, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_57 (Flatten)         (None, 91400)             0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 10)                914010    \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,734,121\n",
      "Trainable params: 5,734,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 200\n",
      "kernel_size: 10\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_58 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 915, 200)          200200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 457, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_58 (Flatten)         (None, 91400)             0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 10)                914010    \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,734,121\n",
      "Trainable params: 5,734,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "*****\n",
      "filters: 100\n",
      "kernel_size: 6\n",
      "*****\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_59 (Embedding)     (None, 924, 100)          4619900   \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 919, 100)          60100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 459, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_59 (Flatten)         (None, 45900)             0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 10)                459010    \n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_118 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,139,021\n",
      "Trainable params: 5,139,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(\n",
    "    filters=[32, 100, 200], \n",
    "    kernel_size = [6, 8, 10]\n",
    ")\n",
    "grid = GridSearchCV(\n",
    "    estimator = m3, \n",
    "    param_grid = param_grid, \n",
    "    n_jobs = 1\n",
    ")\n",
    "grid_result = grid.fit(data, df.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:39:24.706509Z",
     "start_time": "2018-11-20T20:39:24.098509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8776800000119209\n",
      "{'filters': 100, 'kernel_size': 6}\n"
     ]
    }
   ],
   "source": [
    "# Examine the best score and model params\n",
    "print(grid_result.best_score_)\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T21:20:18.976509Z",
     "start_time": "2018-11-20T21:20:18.259509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attach to the best model\n",
    "bestModel = grid_result.best_estimator_ \n",
    "\n",
    "# Save it\n",
    "bestModel.model.save('Model-06.p5.CNN-v3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T18:17:30.428509Z",
     "start_time": "2018-11-20T18:16:37.268509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Loading test data.\n",
      "\n",
      "** Reading processed test data from disk.\n",
      "\n",
      "** Creating Keras sequences.\n",
      "\n",
      "** Predicting classes.\n",
      "** First 10 predictions:\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "\n",
      "** Adding predictions to test data frame.\n",
      "\n",
      "** Writing submission CSV file.\n",
      "\n",
      "** Finished!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF = createKaggleSubmission(bestModel, writeData = False, readData = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T18:17:31.057509Z",
     "start_time": "2018-11-20T18:17:30.429509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review  sentiment\n",
       "0  12311_10  \"Naturally in a film who's main themes are of ...          1\n",
       "1    8348_2  \"This movie is a disaster within a disaster fi...          0\n",
       "2    5828_4  \"All in all, this is a movie for kids. We saw ...          0\n",
       "3    7186_2  \"Afraid of the Dark left me with the impressio...          0\n",
       "4   12128_7  \"A very accurate depiction of small time mob l...          1"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T16:28:55.511509Z",
     "start_time": "2018-11-20T16:28:54.903509Z"
    }
   },
   "source": [
    "Kaggle score:  0.84780\n",
    "\n",
    "Kaggle score w/ contractions and no short words:  0.85424\n",
    "    \n",
    "Kaggle score on tuned CNN:  0.86752"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN v4.0\n",
    "\n",
    "Initialize the weights of the embedding layer from a pre-trained model (i.e. GloVe)\n",
    "\n",
    "GloVe site:  https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "glove.6B.zip URL:  http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T21:41:01.405446Z",
     "start_time": "2018-11-20T21:40:46.740579Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:13, 29032.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load the GloVe embedding\n",
    "embeddingsIndex = dict()\n",
    "\n",
    "glovePath = os.path.join('.', 'datasets', 'glove.6B')\n",
    "gloveData = os.path.join(glovePath, 'glove.6B.100d.txt')\n",
    "\n",
    "f = open(gloveData, encoding=\"utf8\")\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddingsIndex[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddingsIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T21:47:31.397264Z",
     "start_time": "2018-11-20T21:47:30.680264Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46199/46199 [00:00<00:00, 491479.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(embeddingMatrix): 46199\n"
     ]
    }
   ],
   "source": [
    "# Filter to only weight matrix for words in vocab\n",
    "embeddingMatrix = np.zeros((vocabSize, 100))\n",
    "\n",
    "for word, i in tqdm(tokenizer.word_index.items()):\n",
    "    embeddingVector = embeddingsIndex.get(word)\n",
    "    if embeddingVector is not None:\n",
    "        embeddingMatrix[i-1] = embeddingVector\n",
    "        \n",
    "print(\"len(embeddingMatrix):\", len(embeddingMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T22:35:51.789485Z",
     "start_time": "2018-11-20T22:35:51.173491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46199"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddingMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T23:32:11.669909Z",
     "start_time": "2018-11-20T23:32:11.067734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create, compile, and return the CNN model\n",
    "def createModelv4(vocabSize, seqLength, embeddingWeights, filters, kernelSize, debug = False):    \n",
    "    if debug:\n",
    "        print(\"\\n*****\")\n",
    "        print(\"filters:\", filters)\n",
    "        print(\"kernel_size:\", kernel_size)\n",
    "        print(\"*****\\n\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabSize, 100, weights = [embeddingWeights], input_length = seqLength, trainable = False))\n",
    "    \n",
    "    model.add(Conv1D(filters = filters, kernel_size = kernelSize, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #model.add(Conv1D(filters = int(filters/2), kernel_size = int(kernelSize/2), activation = 'relu'))\n",
    "    #model.add(MaxPooling1D(pool_size = 2))\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    if debug:\n",
    "        model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T23:32:19.414160Z",
     "start_time": "2018-11-20T23:32:11.669909Z"
    }
   },
   "outputs": [],
   "source": [
    "m4 = createModelv4(vocabSize, seqLength, embeddingMatrix, 128, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T23:42:03.274199Z",
     "start_time": "2018-11-20T23:32:19.414160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f15724947e84bd9af92fe0103a322f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=5, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70b13e4d49c40118ff4405e10e47a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=20000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156c5ed4e82948c6824f0fdce3b034f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=20000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89f4c79ff1a4891abf4c4539fc4abc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=20000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e532105b6049f8aaac828d23ffbe1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=20000, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-281-b47c68976083>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mTQDMNotebookCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleave_inner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave_outer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nzrasch\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m4.fit(\n",
    "    data, \n",
    "    df.iloc[:, 1], \n",
    "    epochs = 5, \n",
    "    verbose = 0, \n",
    "    callbacks = [TQDMNotebookCallback(leave_inner = True, leave_outer = True)],\n",
    "    batch_size = None,\n",
    "    validation_split = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T23:23:04.754332Z",
     "start_time": "2018-11-20T23:15:23.865Z"
    }
   },
   "outputs": [],
   "source": [
    "m4.save('Model-06.p5.CNN-v4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T23:04:57.311789Z",
     "start_time": "2018-11-20T23:01:15.850Z"
    }
   },
   "outputs": [],
   "source": [
    "m4DF = createKaggleSubmission(m4, writeData = False, readData = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "So far Doc2Vec with combined models and manual training has given us the best results with a 89.53% on the training data.  This is 3 percentage points over the baseline and the Doc2Vec centroid models, and 5 percentage points better than the initial, untuned Doc2Ved model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-08T16:48:34.065Z"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this write-up we accomplished the following:\n",
    "\n",
    "1. Created a set of document vectors from the IMDb movie review text utilizing [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "2. Tuned and trained a number of Doc2Vec models on the movie review corpus \n",
    "2. Ran the models from the [first write-up](./Model-06.ipynb) against the Doc2Vec feature set outputs\n",
    "3. Evaluated if utilizing Doc2Vec improved our ability to correctly classify movie review sentiment\n",
    "\n",
    "\n",
    "Performance metrics so far:\n",
    "\n",
    "|Model|Accuracy|Best Params                                      |\n",
    "|--------------------------|--------|-----------------------------------|\n",
    "|LR (baseline)             |86.35%  |{'LR__C': 0.1, 'LR__penalty': 'l1'}|\n",
    "|SVM centroid              |86.36%  |Scikit-learn defaults              |\n",
    "|SVM Doc2Vec               |84.48%  |Scikit-learn defaults              |\n",
    "|SVM Doc2Vec Init tuning   |88.45%  |dm0, vs100, ng5, hs0, mc2, sm0, e20|\n",
    "|LR manual/combined        |89.53%  |model1, model2, model3             |\n",
    "<div style=\"clear:both\"></div>\n",
    "\n",
    "\n",
    "Utilizing Doc2Vec with manual training and combining model outputs has given us the best classification results to date.  We were able to gain over 3 percentage points in performance from the LR baseline model.\n",
    "\n",
    "If we were to continue this write-up it would be interesting to explore adding many models together and seeing how that affected the output  similar to bagging.  We could also likely spend a lot of time with further tuning, because both the Doc2Vec and Scikit-learn models have a large number of tunable parameters we could leverage.  The best strategy would likely be to start with a randomized grid search due to the large number of parameters, and then focus in on a more narrow set once the more performant combinations started to emerge.\n",
    "\n",
    "And lastly, I'd also like to try taking the combined model feature set and feeding it to a neural network or LSTM for final classification.  It would be interesting to see how one of these more complex algorithms compared against the Scikit-learn linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
