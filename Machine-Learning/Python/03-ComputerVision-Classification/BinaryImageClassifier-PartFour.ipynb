{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Image-Classifiers---Part-Four\" data-toc-modified-id=\"Image-Classifiers---Part-Four-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Image Classifiers - Part Four</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li></ul></li><li><span><a href=\"#ReLu-activations-and-&quot;He&quot;-initialization\" data-toc-modified-id=\"ReLu-activations-and-&quot;He&quot;-initialization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>ReLu activations and \"He\" initialization</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-it?\" data-toc-modified-id=\"What-is-it?-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>What is it?</a></span></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"#&quot;He-Initialization&quot;\" data-toc-modified-id=\"&quot;He-Initialization&quot;-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>\"He Initialization\"</a></span></li><li><span><a href=\"#ReLU\" data-toc-modified-id=\"ReLU-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>ReLU</a></span></li></ul></li></ul></li><li><span><a href=\"#Drop-out-regularization\" data-toc-modified-id=\"Drop-out-regularization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Drop out regularization</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-it?\" data-toc-modified-id=\"What-is-it?-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>What is it?</a></span></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Implementation</a></span></li></ul></li><li><span><a href=\"#Mini-batch-gradient-descent\" data-toc-modified-id=\"Mini-batch-gradient-descent-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Mini-batch gradient descent</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-it?\" data-toc-modified-id=\"What-is-it?-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>What is it?</a></span></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Implementation</a></span></li></ul></li><li><span><a href=\"#Testing-what-we-have-so-far\" data-toc-modified-id=\"Testing-what-we-have-so-far-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Testing what we have so far</a></span></li><li><span><a href=\"#Adam-optimization\" data-toc-modified-id=\"Adam-optimization-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Adam optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-it?\" data-toc-modified-id=\"What-is-it?-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>What is it?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Adam\" data-toc-modified-id=\"Adam-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Adam</a></span></li><li><span><a href=\"#Momentum\" data-toc-modified-id=\"Momentum-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Momentum</a></span></li><li><span><a href=\"#RMSprop\" data-toc-modified-id=\"RMSprop-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>RMSprop</a></span></li><li><span><a href=\"#Combined-summary-resource\" data-toc-modified-id=\"Combined-summary-resource-6.1.4\"><span class=\"toc-item-num\">6.1.4&nbsp;&nbsp;</span>Combined summary resource</a></span></li></ul></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Implementation</a></span></li></ul></li><li><span><a href=\"#Testing-it-out\" data-toc-modified-id=\"Testing-it-out-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Testing it out</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comments\" data-toc-modified-id=\"Comments-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Comments</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Image Classifiers - Part Four</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<img style=\"float: left; margin-right: 25px;\" src=\"images/catMonocle.jpg\" />\n",
    "\n",
    "The overall goal of this series of write-ups is to explore a number of models performing binary classification on a given set of images.  In this fourth write-up we want to add optimization and regularization into the deep nueral network model code we wrote previously.  \n",
    "\n",
    "In the [last write-up] we summarized with the following observation:\n",
    "\n",
    "_What is interesting is that we are definitely suffering from a variance problem (i.e. over fitting) the way things stand. The models consistently reach training accuracy rates of 99%, but the test accuracy isn't much more than what we achieved with a shallow neural network. Hopefully as we move forward with additional regularization techniques in later write ups this will improve, and we'll be able to increase the training accuracy rates._\n",
    "\n",
    "Thus the goals of this write-up are to being adding elements to the model to combat variance as well as optimize how quickly and accurately we can train our neural networks.  This write-up will cover:\n",
    "\n",
    "* He initialization\n",
    "* Switching over to the ReLu activation function\n",
    "* Drop out regularization\n",
    "* Mini-batch gradient descent\n",
    "* Adam optimization\n",
    "\n",
    "\n",
    "For reference here are links to the previous entries in this series:\n",
    "* [The logistics of acquiring and developing an image dataset](https://github.com/nrasch/Portfolio/tree/master/Machine-Learning/Python/02-ComputerVision-Dataset-Creation)\n",
    "* [Binary image classification utilizing logistic regression with gradient descent](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/03-ComputerVision-Classification/BinaryImageClassifier-PartOne.ipynb)\n",
    "* [Binary image classification utilizing a shallow neural network](https://nbviewer.jupyter.org/github/nrasch/Portfolio/blob/master/Machine-Learning/Python/03-ComputerVision-Classification/BinaryImageClassifier-PartTwo.ipynb)\n",
    "\n",
    "So, let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLu activations and \"He\" initialization\n",
    "\n",
    "## What is it?\n",
    "\n",
    "First, for those interested the paper can be found here describing these topics:\n",
    "* https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf\n",
    "\n",
    "\n",
    "The rectified linear unit activation (ReLu) function defines the output of a node given an input or set of inputs. For example in the image below the calcuation \n",
    "\n",
    "\\begin{equation}z^{(i)} = w^T x^{(i)} + b\\end{equation} \n",
    "\n",
    "is fed into the activation function \\begin{equation}a^{(i)} = sigma(z^{(i)})\\end{equation}\n",
    "\n",
    "\n",
    "And below is a diagram of a neural network using a ReLu activation in the cells, and then a final sigmoid activation to output the model's y-hat values:\n",
    "\n",
    "<img src=\"images/relu_activation.png\" align=\"left\" height=\"30%\" width=\"30%\" padding-left=\"100px\" />\n",
    "<p style=\"clear: both;\">&nbsp;</p>\n",
    "\n",
    "Benfits of utilizing ReLu include:\n",
    "* Mitigate exploding or vanishing gradient problems\n",
    "* Faster learning\n",
    "* Avoids computing exponents\n",
    "* Outputs sparse representations with true zeros\n",
    "\n",
    "Note that if you wish to read more about rectified linear units a good article can be found [here](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning).\n",
    "\n",
    "The \"He\" initialization is a method to initialize the parameters in a neural network utilizing ReLu functions.  In short the formula is:\n",
    "$\\sqrt{\\frac{2}{\\text{dim(L - 1)}}}$  where L is a layer in the neural network.  \n",
    "\n",
    "The paper cited above summarizes with the following:\n",
    "\n",
    "_Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures.  Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% )._\n",
    "\n",
    "And finally, [here](https://en.wikipedia.org/wiki/Activation_function) is a good summary of the various activation functions and their graphs.\n",
    "\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"He Initialization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize model params (i.e. W and b)\n",
    "def initilizeParameters(dimensionDict):\n",
    "\n",
    "    params = {}\n",
    "    lastDimSize = dimensionDict[\"numberInputs\"]\n",
    "    \n",
    "    for index, size in enumerate(dimensionDict['hiddenLayerSizes'], start=1):\n",
    "        wName = \"w\" + str(index)\n",
    "        bName = \"b\" + str(index)\n",
    "        \n",
    "        # Initialize utilizing \"He Initialization\"\n",
    "        np.random.seed(10)  # Yes, this has to be done every time...  :(\n",
    "        params[wName] = np.random.randn(size, lastDimSize) * np.sqrt(2/lastDimSize)\n",
    "        params[bName] = np.zeros((size, 1))\n",
    "        lastDimSize = size\n",
    "   \n",
    "    # add final output layer\n",
    "    wName = \"w\" + str(len(dimensionDict['hiddenLayerSizes']) + 1)\n",
    "    bName = \"b\" + str(len(dimensionDict['hiddenLayerSizes']) + 1)\n",
    "        \n",
    "    # Initialize utilizing \"He Initialization\"\n",
    "    np.random.seed(10)  # Yes, this has to be done every time...  :(\n",
    "    params[wName] = np.random.randn(dimensionDict[\"numberOutputs\"], lastDimSize) * np.sqrt(2/lastDimSize)\n",
    "    \n",
    "    params[bName] = np.zeros((dimensionDict[\"numberOutputs\"], 1))\n",
    "      \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ReLu activation\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "# Define ReLu derivative\n",
    "def dRelu(x):\n",
    "    return 1. * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward propogation\n",
    "def forwardPropagation(data, params):\n",
    "    \n",
    "    # Init vars\n",
    "    numLayers = (len(params))//2 \n",
    "    cache = {}\n",
    "    cache['a0'] = a = data\n",
    "\n",
    "    # Process each layer of the NN\n",
    "    for i in range(1, numLayers + 1):\n",
    "\n",
    "        # Made the code below easier to read\n",
    "        aPrev = cache['a' + str(i-1)]\n",
    "        w = params['w' + str(i)]\n",
    "        b = params['b' + str(i)]\n",
    "       \n",
    "        # Perform linear calculations & sanity check\n",
    "        z = np.dot(w, aPrev) + b\n",
    "        assert(z.shape == (w.shape[0], aPrev.shape[1]))\n",
    "        \n",
    "        # Perform sigmoid or ReLu activation\n",
    "        if (i == numLayers):\n",
    "            # last layer; sigmoid activation\n",
    "            cache['a' + str(i)] = 1 / (1 + np.exp(-(z)))\n",
    "            assert(cache['a' + str(i)].shape == z.shape)\n",
    "        else:\n",
    "            # Hidden layer; ReLu activation\n",
    "            cache['a' + str(i)] = relu(z)\n",
    "            assert(cache['a' + str(i)].shape == z.shape)\n",
    "        \n",
    "    # Final sanity check\n",
    "    assert(cache['a' + str((len(params))//2)].shape == (1, data.shape[1]))\n",
    "        \n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform backward propogation\n",
    "def backwardPropagation(labels, cache, params, lamb):\n",
    "    \n",
    "    # Init variables\n",
    "    grads = {}\n",
    "    layers = (len(params)//2)\n",
    "    \n",
    "    # Make the code below easier to read\n",
    "    aL = cache['a' + str(layers)]\n",
    "    dw = \"dw\" + str(layers)\n",
    "    db = \"db\" + str(layers)\n",
    "    aPrev = 'a' + str(layers-1)\n",
    "    w = 'w' + str(layers)\n",
    "    b = 'b' + str(layers)\n",
    "    m = aL.shape[1]\n",
    "    \n",
    "\n",
    "    # Initialize backprop:  Calc dz, dw, and db for layer L\n",
    "    dz = aL - labels\n",
    "    grads[dw], grads[db] = linearBackProp(dz, cache[aPrev], params[w], params[b], lamb)\n",
    "       \n",
    "    # Backprop for the hidden layers\n",
    "    for i in reversed(range(1, layers)):\n",
    "        \n",
    "        # Make the code below easier to read\n",
    "        dw = \"dw\" + str(i)\n",
    "        db = \"db\" + str(i)\n",
    "        a = 'a' + str(i)\n",
    "        aPrev = 'a' + str(i-1)\n",
    "        dzBefore = dz\n",
    "        w = 'w' + str(i)\n",
    "        wBefore = 'w' + str(i+1)\n",
    "        b = 'b' + str(i)\n",
    "        m = aL.shape[1]\n",
    "        \n",
    "        dz = np.dot(params[wBefore].T, dzBefore) * dRelu(cache[a])\n",
    "        grads[dw], grads[db] = linearBackProp(dz, cache[aPrev], params[w], params[b], lamb)\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop out regularization\n",
    "## What is it?\n",
    "Drop out regularization is a method to combat over fitting on the training data.  This is done by randomly \"dropping out\" nodes in the neural network by setting all the values for that node to zero.  This in theory prevents the model from \"fixating\" too heavily on the training data and should reduce variance issues.\n",
    "\n",
    "A quick visual:\n",
    "\n",
    "<img src=\"images/dropout.png\" align=\"left\" height=\"40%\" width=\"40%\" padding-left=\"100px\" />\n",
    "<p style=\"clear: both;\">&nbsp;</p>\n",
    "\n",
    "You can read more about drop out regularization [here](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5).\n",
    "\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward propogation\n",
    "def forwardPropagation(data, params, dropProb):\n",
    "    \n",
    "    # Init vars\n",
    "    numLayers = (len(params))//2 \n",
    "    cache = {}\n",
    "    cache['a0'] = a = data\n",
    "\n",
    "    # Process each layer of the NN\n",
    "    for i in range(1, numLayers + 1):\n",
    "\n",
    "        # Made the code below easier to read\n",
    "        aPrev = cache['a' + str(i-1)]\n",
    "        w = params['w' + str(i)]\n",
    "        b = params['b' + str(i)]\n",
    "       \n",
    "        # Perform linear calculations & sanity check\n",
    "        z = np.dot(w, aPrev) + b\n",
    "        assert(z.shape == (w.shape[0], aPrev.shape[1]))\n",
    "        \n",
    "        # Perform sigmoid or ReLu activation\n",
    "        if (i == numLayers):\n",
    "            # last layer; sigmoid activation\n",
    "            cache['a' + str(i)] = 1 / (1 + np.exp(-(z)))\n",
    "            assert(cache['a' + str(i)].shape == z.shape)\n",
    "        else:\n",
    "            # Hidden layer; ReLu activation\n",
    "            cache['a' + str(i)] = relu(z)\n",
    "            assert(cache['a' + str(i)].shape == z.shape)\n",
    "            \n",
    "            # Dropout regularization\n",
    "            np.random.seed(10)  # Yes, this has to be done every time...  :(\n",
    "            cache['dropMask' + str(i)] = np.random.rand(cache['a' + str(i)].shape[0], cache['a' + str(i)].shape[1])\n",
    "            cache['dropMask' + str(i)] = cache['dropMask' + str(i)] < dropProb\n",
    "            cache['a' + str(i)] = cache['a' + str(i)] * cache['dropMask' + str(i)]\n",
    "            cache['a' + str(i)] = cache['a' + str(i)] / dropProb\n",
    "        \n",
    "    # Final sanity check\n",
    "    assert(cache['a' + str((len(params))//2)].shape == (1, data.shape[1]))\n",
    "        \n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform backward propogation\n",
    "def backwardPropagation(labels, cache, params, lamb, dropProb):\n",
    "    \n",
    "    # Init variables\n",
    "    grads = {}\n",
    "    layers = (len(params)//2)\n",
    "    \n",
    "    # Make the code below easier to read\n",
    "    aL = cache['a' + str(layers)]\n",
    "    dw = \"dw\" + str(layers)\n",
    "    db = \"db\" + str(layers)\n",
    "    aPrev = 'a' + str(layers-1)\n",
    "    w = 'w' + str(layers)\n",
    "    b = 'b' + str(layers)\n",
    "    m = aL.shape[1]\n",
    "    \n",
    "\n",
    "    # Initialize backprop:  Calc dz, dw, and db for layer L\n",
    "    dz = aL - labels\n",
    "    grads[dw], grads[db] = linearBackProp(dz, cache[aPrev], params[w], params[b], lamb)\n",
    "       \n",
    "    # Backprop for the hidden layers\n",
    "    for i in reversed(range(1, layers)):\n",
    "        \n",
    "        # Make the code below easier to read\n",
    "        dw = \"dw\" + str(i)\n",
    "        db = \"db\" + str(i)\n",
    "        a = 'a' + str(i)\n",
    "        aPrev = 'a' + str(i-1)\n",
    "        dzBefore = dz\n",
    "        w = 'w' + str(i)\n",
    "        wBefore = 'w' + str(i+1)\n",
    "        b = 'b' + str(i)\n",
    "        m = aL.shape[1]\n",
    "        dropMask = 'dropMask' + str(i)\n",
    "        \n",
    "        # Apply backprop with dropout regularization and calc grads; breaking up the calc steps for readability\n",
    "        # dz = np.dot(params[wBefore].T, dzBefore) * dRelu(cache[a])\n",
    "        dz = np.dot(params[wBefore].T, dzBefore)\n",
    "        dz = dz * cache[dropMask]\n",
    "        dz = dz / dropProb\n",
    "        dz = dz * dRelu(cache[a])\n",
    "        grads[dw], grads[db] = linearBackProp(dz, cache[aPrev], params[w], params[b], lamb)\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch gradient descent\n",
    "## What is it?\n",
    "\n",
    "Mini-batch gradient descent is another way to combat variance issues.  In essence the training examples are randomly divided up into N number of groups, and then each of these groups is sent through the model.  As the groups are sent through the model forward and backward propagation occurs, the parameters are updated, and so forth.  However, all this happens for only one batch at a time, and similar to drop out regularization this prevents the model from \"fixating\" too heavily on one set of training examples.\n",
    "\n",
    "<img src=\"images/mini_batch_process.png\" align=\"left\" height=\"60%\" width=\"60%\" padding-left=\"100px\" />\n",
    "<p style=\"clear: both;\">&nbsp;</p>\n",
    "\n",
    "In code it looks something like this, which I think helps illustrate the mechanics:\n",
    "\n",
    "```python\n",
    "# For each training iteration\n",
    "for i in range(0, numIterations):\n",
    "    \n",
    "    # Create the mini batches - This is where we split up the training examples into N number of groups or \"batches\"\n",
    "    miniBatches = createMiniBatches(data, labels, batchSize, seed)\n",
    "    \n",
    "    # Now we feed the batches through the model\n",
    "    for batch in miniBatches:\n",
    "\n",
    "        # Perform forward and backward prop, update params, etc. using the batch of training examples\n",
    "```\n",
    "\n",
    "You can read more about mini-batch and other gradient descent algorithms [here](http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants), as well as an article dedicated strictly to mini-batch gradient descent [here](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/).\n",
    "\n",
    "## Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMiniBatches(data, labels, batchSize, seed):  \n",
    "    m = data.shape[1]\n",
    "    miniBatches = []\n",
    "    \n",
    "    # Shuffle the data and labels\n",
    "    np.random.seed(seed)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    \n",
    "    shuffledData = data[:, permutation]\n",
    "    shuffledLabels = labels[:, permutation].reshape((1,m))\n",
    "    \n",
    "    # Create the mini batches\n",
    "    # First take all the groupings that fit into the \"batchSize\" bucket\n",
    "    batches = math.floor(m / batchSize)\n",
    "    for i in range(0, batches):\n",
    "        batchData = shuffledData[:, i * batchSize : (i + 1) * batchSize]\n",
    "        batchLabels = shuffledLabels[:, i * batchSize : (i + 1) * batchSize]\n",
    "        miniBatch = (batchData, batchLabels)\n",
    "        miniBatches.append(miniBatch)\n",
    "        \n",
    "    # Next take the final grouping of records that are left over\n",
    "    if m % batchSize != 0:\n",
    "        batchData = shuffledData[:, batches * batchSize : m]\n",
    "        batchLabels = shuffledLabels[:, batches * batchSize : m]\n",
    "        miniBatch = (batchData, batchLabels)\n",
    "        miniBatches.append(miniBatch)\n",
    "        \n",
    "    return miniBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the actual neural network classification model\n",
    "def model(data, labels, dims, numIterations, learningRate, lamb, batchSize, dropProb, printCost = False, showGraph = False):\n",
    "    \n",
    "    # Init vars\n",
    "    params = initilizeParameters(dims)\n",
    "    seed = 10\n",
    "    \n",
    "    #return params;\n",
    "    \n",
    "    costs = []\n",
    "    descendingGraph = True\n",
    "    \n",
    "    # For each training iteration\n",
    "    for i in range(0, numIterations + 1):\n",
    "         \n",
    "        # Create the mini batches\n",
    "        seed = seed + 1   # assure we get a different batch composition each time through\n",
    "        miniBatches = createMiniBatches(data, labels, batchSize, seed)\n",
    "        \n",
    "        for batch in miniBatches:\n",
    "        \n",
    "            # Get a set of data and lable records\n",
    "            (batchData, batchLabels) = batch\n",
    "            \n",
    "            # Forward propagation\n",
    "            cache = forwardPropagation(batchData, params, dropProb)\n",
    "\n",
    "            # Cost function\n",
    "            cost = calculateCost(batchLabels, params, cache, lamb)\n",
    "\n",
    "            # Backward  propagation\n",
    "            grads = backwardPropagation(batchLabels, cache, params, lamb, dropProb)\n",
    "\n",
    "            # Gradient descent parameter update\n",
    "            params = updateParams(params, grads, learningRate)\n",
    "        \n",
    "        # Print the cost every N number of iterations\n",
    "        if printCost and i % 500 == 0:\n",
    "            print (\"Cost after iteration\", str(i), \"is\", str(cost))\n",
    "        \n",
    "        # Record the cost every N number of iterations\n",
    "        if i % 50 == 0:\n",
    "            if (len(costs) != 0) and (cost > costs[-1]):\n",
    "                descendingGraph = False\n",
    "            costs.append(cost)\n",
    "      \n",
    "    # Print the model training cost graph\n",
    "    if showGraph:\n",
    "        _costs = np.squeeze(costs)\n",
    "        plt.plot(_costs)\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Iterations (every 50)')\n",
    "        plt.title(\"Learning rate =\" + str(learningRate))\n",
    "        plt.show()\n",
    "\n",
    "    return params, costs, descendingGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing what we have so far\n",
    "\n",
    "Ok, let's test what we have so far, and ensure everything works as we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "import random \n",
    "random.seed(10)\n",
    "\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib import pyplot as plt\n",
    "import inspect\n",
    "import time\n",
    "import copy\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "from utils_v1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, shape, and normalize the data used for training the model\n",
    "imageData = path.join(\"datasets\", \"imageData500_64pixels.hdf5\")\n",
    "\n",
    "with h5py.File(imageData, \"r\") as archive:   \n",
    "    trainingData = np.squeeze(archive[\"trainData\"][:])\n",
    "    testData = np.squeeze(archive[\"testData\"][:])\n",
    "    trainingLabels = np.array(archive[\"trainLabels\"][:])\n",
    "    testLabels = np.array(archive[\"testLabels\"][:])\n",
    "    archive.close()\n",
    "\n",
    "# Reshape the training and test data and label matrices\n",
    "trainingData = trainingData.reshape(trainingData.shape[0], -1).T\n",
    "testData = testData.reshape(testData.shape[0], -1).T\n",
    "\n",
    "# Normalization\n",
    "trainingData = trainingData/255.\n",
    "testData = testData/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0 is 0.901112309065816\n",
      "Cost after iteration 500 is 0.4853297051889699\n",
      "Cost after iteration 1000 is 0.3595267318773825\n",
      "Cost after iteration 1500 is 0.33043265250081566\n",
      "Cost after iteration 2000 is 0.30033542870505747\n",
      "Cost after iteration 2500 is 0.2721246728216369\n",
      "Cost after iteration 3000 is 0.2850309686729501\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.5\n"
     ]
    }
   ],
   "source": [
    "# model(data, labels, dims, numIterations, learningRate, lamb, batchSize, dropProb, printCost = False, showGraph = False):\n",
    "testParams, testCosts, testGraph = model(\n",
    "    data= trainingData, \n",
    "    labels = trainingLabels, \n",
    "    dims = defineDimensions(trainingData, trainingLabels, (2, 4, 4)),\n",
    "    numIterations = 3000,\n",
    "    learningRate = 0.0005,\n",
    "    lamb = 0.9, \n",
    "    batchSize = 32,\n",
    "    dropProb = 0.85, \n",
    "    printCost = True, \n",
    "    showGraph = True\n",
    ")\n",
    "\n",
    "testPreds = predict(testData, testParams, testLabels)\n",
    "print(testPreds['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that doesn't look half bad for a first try, and it certainly gives us confidence we don't have any bugs or other issues with the changes we've made so far.  Let's go ahead and add in the last optimization, and then we'll work on training the model.\n",
    "\n",
    "Also note that now that we are utilizing mini-batch gradient descent the cost/convergence graph is no longer smooth although it does have an overall downward slope as the error rate of the model decreases during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam optimization\n",
    "\n",
    "## What is it?\n",
    "\n",
    "### Adam\n",
    "\n",
    "For those interested the paper on Adam can be found [here](https://arxiv.org/abs/1412.6980).\n",
    "\n",
    "In a nutshell Adam combines Momentum and RMSprop together to get the best of both worlds.  Adam attempts to accelerate convergence while dampening the oscillations that occur during training with methods such as mini-batch gradient descent.\n",
    "\n",
    "An excerpt from [Andrew Ng's lecture on Coursera](https://www.coursera.org/lecture/deep-neural-network/adam-optimization-algorithm-w9VCZ) fleshes things out still further:\n",
    "\n",
    "\n",
    "**How does Adam work?**\n",
    "1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n",
    "2. It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n",
    "3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n",
    "\n",
    "The update rule is, for $l = 1, ..., L$: \n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
    "\\end{cases}$$\n",
    "where:\n",
    "- t counts the number of steps taken of Adam \n",
    "- L is the number of layers\n",
    "- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\varepsilon$ is a very small number to avoid dividing by zero\n",
    "\n",
    "\n",
    "\n",
    "And since Adam combines aspects of Momentum and RMSprop we'll say a brief word about them both below.\n",
    "\n",
    "### Momentum\n",
    "\n",
    "A fantastic write-up on Momentum can be found [here](https://distill.pub/2017/momentum/).  The interactive graph at the top that models the impact of the various values of alpha and beta is worth visiting the page for alone.\n",
    "\n",
    "In summary, _[i]nstead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go.  [1](https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/)._\n",
    "\n",
    "A very common diagram that often appears when discussing Momentum:\n",
    "\n",
    "<img src=\"images/momentum.png\" align=\"left\" height=\"60%\" width=\"60%\" padding-left=\"100px\" />\n",
    "<p style=\"clear: both;\">&nbsp;</p>\n",
    "\n",
    "The goal is to reduce the movement of the blue line up and down (i.e the oscillations) while accelerating if possible the movement toward the optima (i.e. the descent).\n",
    "\n",
    "### RMSprop\n",
    "\n",
    "How it works:\n",
    "\n",
    "_Rather than having just a global, scalar learning rate, we have a vector of learning rates for each trainable parameter. It is iteratively updated with a running average of magnitudes of squares of previous gradients. Changes to the weights during training are now not purely in the direction of the gradient, but rather in the direction of the elementwise division of the gradient by this vector you are maintaining.  [2](https://www.quora.com/What-is-an-intuitive-explanation-of-RMSProp)_\n",
    "\n",
    "And here are the [equations](https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b) for Momentum and RMSprop, which you can see are very similar:\n",
    "\n",
    "<img src=\"images/momentum_rmsprop_equations.png\" align=\"left\" height=\"60%\" width=\"60%\" padding-left=\"100px\" />\n",
    "<p style=\"clear: both;\">&nbsp;</p>\n",
    "\n",
    "### Combined summary resource\n",
    "\n",
    "And finally, another good resource discussing Adam, Momentum, and RMSprop can be found [here](https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/).\n",
    "\n",
    "## Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize Adam model params (i.e. v and s)\n",
    "#  v - exponentially weighted average of the gradient\n",
    "#  s - exponentially weighted average of the squared gradient\n",
    "def initilizeAdamParameters(params):\n",
    "\n",
    "    # 'Params' contains the W and b value matrices for each layer of the NN\n",
    "    # So we divide by two to get the number of NN layers which we'll label 'm' similiar to other code blocks\n",
    "    m = len(params) // 2\n",
    "    \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        v[\"dw\" + str(i + 1)] = np.zeros((params[\"w\" + str(i + 1)].shape))\n",
    "        v[\"db\" + str(i + 1)] = np.zeros((params[\"b\" + str(i + 1)].shape))\n",
    "        s[\"dw\" + str(i + 1)] = np.zeros((params[\"w\" + str(i + 1)].shape))\n",
    "        s[\"db\" + str(i + 1)] = np.zeros((params[\"b\" + str(i + 1)].shape))\n",
    "             \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model params using Adam\n",
    "def updateParamsAdam(params, grads, v, s, t, learningRate, beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):    \n",
    "   \n",
    "    # Init vars\n",
    "    layers = (len(params)//2)\n",
    "    # Adam moment estimate containers\n",
    "    vCorrected = {}                  \n",
    "    sCorrected = {}\n",
    "    \n",
    "    # Perform the param updates utilizing Adam\n",
    "    for l in range(layers):\n",
    "        # Moving average of the gradients\n",
    "        v[\"dw\" + str(l+1)] = beta1 * v['dw' + str(l+1)] + (1 - beta1) * grads['dw' + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1 * v['db' + str(l+1)] + (1 - beta1) * grads['db' + str(l+1)]\n",
    "\n",
    "        # Compute bias-corrected first moment estimate\n",
    "        vCorrected[\"dw\" + str(l+1)] = v[\"dw\" + str(l+1)] / (1 - beta1**t)\n",
    "        vCorrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1**t)\n",
    "\n",
    "        # Moving average of the squared gradients\n",
    "        s[\"dw\" + str(l+1)] = beta2 * s['dw' + str(l+1)] + (1 - beta2) * (grads['dw' + str(l+1)]**2)\n",
    "        s[\"db\" + str(l+1)] = beta2 * s['db' + str(l+1)] + (1 - beta2) * (grads['db' + str(l+1)]**2)\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate\n",
    "        sCorrected[\"dw\" + str(l+1)] = s[\"dw\" + str(l+1)] / (1 - beta2**t)\n",
    "        sCorrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2**t)\n",
    "\n",
    "        # Update params\n",
    "        params[\"w\" + str(l+1)] = params[\"w\" + str(l+1)] - learningRate * (vCorrected[\"dw\" + str(l+1)] / np.sqrt(sCorrected[\"dw\" + str(l+1)] + epsilon))\n",
    "        params[\"b\" + str(l+1)] = params[\"b\" + str(l+1)] - learningRate * (vCorrected[\"db\" + str(l+1)] / np.sqrt(sCorrected[\"db\" + str(l+1)] + epsilon))\n",
    "\n",
    "    return params, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the actual neural network classification model\n",
    "def model(data, labels, dims, numIterations, learningRate, lamb, batchSize, dropProb, \n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, printCost = False, showGraph = False):\n",
    "    \n",
    "    # Init vars\n",
    "    params = initilizeParameters(dims)\n",
    "    seed = 10    # mini-batch seed\n",
    "    t = 0        # Adam counter\n",
    "    v, s = initilizeAdamParameters(params)\n",
    "    \n",
    "    #return params;\n",
    "    \n",
    "    costs = []\n",
    "    descendingGraph = True\n",
    "    \n",
    "    # For each training iteration\n",
    "    for i in range(0, numIterations + 1):\n",
    "         \n",
    "        # Create the mini batches\n",
    "        seed = seed + 1   # assure we get a different batch composition each time through\n",
    "        miniBatches = createMiniBatches(data, labels, batchSize, seed)\n",
    "        \n",
    "        for batch in miniBatches:\n",
    "        \n",
    "            # Get a set of data and lable records\n",
    "            (batchData, batchLabels) = batch\n",
    "            \n",
    "            # Forward propagation\n",
    "            cache = forwardPropagation(batchData, params, dropProb)\n",
    "\n",
    "            # Cost function\n",
    "            cost = calculateCost(batchLabels, params, cache, lamb)\n",
    "\n",
    "            # Backward  propagation\n",
    "            grads = backwardPropagation(batchLabels, cache, params, lamb, dropProb)\n",
    "\n",
    "            # Gradient descent parameter update with Adam\n",
    "            t = t + 1 # Update Adam counter\n",
    "            params, v, s = updateParamsAdam(params, grads, v, s, t, learningRate, beta1, beta2, epsilon)    \n",
    "        \n",
    "        # Print the cost every N number of iterations\n",
    "        if printCost and i % 500 == 0:\n",
    "            print (\"Cost after iteration\", str(i), \"is\", str(cost))\n",
    "        \n",
    "        # Record the cost every N number of iterations\n",
    "        if i % 50 == 0:\n",
    "            if (len(costs) != 0) and (cost > costs[-1]):\n",
    "                descendingGraph = False\n",
    "            costs.append(cost)\n",
    "      \n",
    "    # Print the model training cost graph\n",
    "    if showGraph:\n",
    "        _costs = np.squeeze(costs)\n",
    "        plt.plot(_costs)\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Iterations (every 50)')\n",
    "        plt.title(\"Learning rate =\" + str(learningRate))\n",
    "        plt.show()\n",
    "\n",
    "    return params, costs, descendingGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save the code we just wrote above into 'utils_v2.py' and then import it for our testing below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing it out\n",
    "\n",
    "OK, let's give our updated model a test, and then we'll do some training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "import random \n",
    "random.seed(10)\n",
    "\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib import pyplot as plt\n",
    "import inspect\n",
    "import time\n",
    "import copy\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "# Contains our Adam updates...\n",
    "from utils_v2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, shape, and normalize the data used for training the model\n",
    "imageData = path.join(\"datasets\", \"imageData500_64pixels.hdf5\")\n",
    "\n",
    "with h5py.File(imageData, \"r\") as archive:   \n",
    "    trainingData = np.squeeze(archive[\"trainData\"][:])\n",
    "    testData = np.squeeze(archive[\"testData\"][:])\n",
    "    trainingLabels = np.array(archive[\"trainLabels\"][:])\n",
    "    testLabels = np.array(archive[\"testLabels\"][:])\n",
    "    archive.close()\n",
    "\n",
    "# Reshape the training and test data and label matrices\n",
    "trainingData = trainingData.reshape(trainingData.shape[0], -1).T\n",
    "testData = testData.reshape(testData.shape[0], -1).T\n",
    "\n",
    "# Normalization\n",
    "trainingData = trainingData/255.\n",
    "testData = testData/255.\n",
    "\n",
    "data = [trainingData, trainingLabels, testData, testLabels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0 is 0.7058513094395172\n",
      "Cost after iteration 500 is 0.004819282337345424\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8XHW9//HXJ5OtSdM9baBbSulCAwUkrBUsF2iLIsgVpPxcUFTcuCB4RXBBLooiqIDKvZdFVFQoCCi9UCmLlLWFBiiFblC60FDapvuSNuvn98c5CUOYLKQ5PcnM+/l4zCNzzvnOmc9J2nnP2b5fc3dEREQAsuIuQEREug+FgoiINFMoiIhIM4WCiIg0UyiIiEgzhYKIiDRTKEhaMLN/mtl5cdch0tMpFGSvmNkqMzs57jrc/VR3/1PcdQCY2Rwz+8o+eJ88M7vDzLab2Tozu7Sd9peE7baFr8tLWlZqZk+aWbWZLW35N23ntT8xs9fMrN7MruryDZV9SqEg3Z6ZZcddQ5PuVAtwFTAGGAmcCFxmZtNSNTSzqcDlwElAKXAA8F9JTe4GXgEGAj8A7jOz4g6+djlwGfBwl2yVxMvd9dCj0w9gFXByK8tOAxYAW4HngYlJyy4H3gJ2AIuBM5OWfRF4DrgB2Az8NJz3LPBLYAuwEjg16TVzgK8kvb6ttqOAp8P3fhy4GfhLK9swGagEvgesA/4M9AceAqrC9T8EDAvbXwM0AHuAncDvwvnjgcfC7VkGfKYLfvfvAFOSpn8CzGil7V3Az5KmTwLWhc/HAjVAUdLyZ4Cvt/faFu/xF+CquP9N6rF3D+0pSCTM7CPAHcDXCL593gLMTDrs8BZwPNCX4FvnX8xsv6RVHA2sAAYTfNA2zVsGDAKuA35vZtZKCW21vQt4MazrKuDz7WxOCTCA4Bv5BQR72H8Ip0cAu4HfAbj7Dwg+UC90997ufqGZFRIEwl3h9pwL/LeZlaV6MzP7bzPb2spjYdimP7A/8GrSS18FUq4znN+y7RAzGxguW+HuO1pZV1uvlTSjUJCofBW4xd1fcPcGD4731wDHALj739x9rbs3uvs9wJvAUUmvX+vuv3X3enffHc5b7e63uXsD8CdgP2BIK++fsq2ZjQCOBK5091p3fxaY2c62NAI/dvcad9/t7pvc/X53rw4/SK8BPtbG608DVrn7H8LteRm4HzgrVWN3/6a792vlMTFs1jv8uS3ppduAolZq6J2iLWH7lstarqut10qaUShIVEYC30n+lgsMJ/h2i5l9wcwWJC07mOBbfZM1Kda5rumJu1eHT3unaNdW2/2BzUnzWnuvZFXuvqdpwswKzOwWM1ttZtsJDkX1M7NEK68fCRzd4nfxWYI9kM7aGf7skzSvD8Ehsdbat2xL2L7lspbrauu1kmYUChKVNcA1Lb7lFrj73WY2ErgNuBAY6O79gNeB5ENBUXXf+y4wwMwKkuYNb+c1LWv5DjAOONrd+wAnhPOtlfZrgKda/C56u/s3Ur2Zmf2vme1s5bEIwN23hNtyaNJLDwUWtbINi1K0Xe/um8JlB5hZUYvlizrwWkkzCgXpCjlmlp/0yCb40P+6mR1tgUIz+0T4wVNI8MFZBWBmXyLYU4icu68GKoCrzCzXzI4FPvkhV1NEcB5hq5kNAH7cYvl6git0mjwEjDWzz5tZTvg40swOaqXGr4ehkeqRfM7gTuCHZtbfzMYTHLL7Yys13wl82cwmhOcjftjU1t3fILgg4Mfh3+9MYCLBIa42XwsQbk8+wedJdriO1vaapJtTKEhXmEXwIdn0uMrdKwg+pH5HcIXOcoKrgnD3xcCvgLkEH6CHEFxttK98FjgW2ERwZdM9BOc7OupGoBewEZgHPNJi+U3AWWa2xcx+E553mAJMB9YSHNr6BZDH3vkxwQn71cBTwPXu/giAmY0I9yxGAITzrwOeDNuv5v1hNh0oJ/hbXQuc5e5VHXztbQR/93MJLmfdTfsn76WbMncNsiOZzczuAZa6e8tv/CIZR3sKknHCQzejzSwrvNnrDOAfcdcl0h10p7szRfaVEuABgvsUKoFvuPsr8ZYk0j3o8JGIiDTT4SMREWnW4w4fDRo0yEtLS+MuQ0SkR3nppZc2untxe+16XCiUlpZSUVERdxkiIj2Kma3uSDsdPhIRkWYKBRERaaZQEBGRZgoFERFpplAQEZFmCgUREWmmUBARkWaRhoKZTTOzZWa23MwuT7H8hnD0rQVm9kY4IlUkXn57C794ZGlUqxcRSQuRhUI4yMbNwKnABOBcM5uQ3MbdL3H3w9z9MOC3BJ2UReL1d7bxP3PeYvkGjSAoItKaKPcUjgKWu/sKd68FZhB0Udyac4G7oypmyoRgONzZi9ZH9RYiIj1elKEwlPcPiF4ZzvuAcMzeUcC/Wll+gZlVmFlFVVVVp4op6ZvPocP7MXvRuvYbi4hkqChDwVLMa62f7unAfe7ekGqhu9/q7uXuXl5c3G5/Tq2aWjaEhZXbWLt1d6fXISKSzqIMhUpgeNL0MILxaVOZToSHjppMLQsOIT2qvQURkZSiDIX5wBgzG2VmuQQf/DNbNjKzcUB/gkHcIzW6uDcHDu6t8woiIq2ILBTcvR64EJgNLAHudfdFZna1mZ2e1PRcYIbvoyHgppYN4cVVm9myq3ZfvJ2ISI8S6X0K7j7L3ce6+2h3vyacd6W7z0xqc5W7f+AehqhMLSuhodF5fIn2FkREWsq4O5oPGdqX/fvm6xCSiEgKGRcKZsaUshKeebOK6tr6uMsREelWMi4UAKaUDaGmvpGnlnXungcRkXSVkaFwVOkA+hfk6EY2EZEWMjIUshNZnHTQEJ5YuoHa+sa4yxER6TYyMhQguAppx5565q3YFHcpIiLdRsaGwvFjBlGQm9AhJBGRJBkbCvk5CT42tpjHFq+nsXGf3DcnItLtZWwoQHAIacOOGl5ZE9nYPiIiPUpGh8KJ4weTnWU8uliHkEREIMNDoW+vHI4dPZBHF61nH3W9JCLSrWV0KEBwCGnlxl28uWFn3KWIiMQu40NhyoQhmMHs13UISUQk40NhcJ98Dh/ej9k6ryAiolCA4BDS6+9sp3JLddyliIjESqFA8jCd6k5bRDKbQgEoHVTIuCFFurtZRDKeQiE0tWwI81dtZtPOmrhLERGJjUIhNKWshEaHJ5ZsiLsUEZHYRBoKZjbNzJaZ2XIzSzkOs5l9xswWm9kiM7srynraUrZ/H4b266VDSCKS0bKjWrGZJYCbgVOASmC+mc1098VJbcYAVwCT3H2LmQ2Oqp72BMN0DuGvL7zNzpp6eudF9qsREem2otxTOApY7u4r3L0WmAGc0aLNV4Gb3X0LgLvHeuxmalkJtRqmU0QyWJShMBRYkzRdGc5LNhYYa2bPmdk8M5uWakVmdoGZVZhZRVVVdB/YR5YOYEBhrg4hiUjGijIULMW8lr3OZQNjgMnAucDtZtbvAy9yv9Xdy929vLi4uMsLbZLIMk4+aDBPaphOEclQUYZCJTA8aXoYsDZFmwfdvc7dVwLLCEIiNlPLSthRU8/zb22MswwRkVhEGQrzgTFmNsrMcoHpwMwWbf4BnAhgZoMIDietiLCmdk06cBCFuQlm6+5mEclAkYWCu9cDFwKzgSXAve6+yMyuNrPTw2azgU1mthh4Eviuu2+KqqaOyM9JMHncYB5bvJ4GDdMpIhkm0usu3X0WMKvFvCuTnjtwafjoNqaUDeHh197llbe3UF46IO5yRET2Gd3RnMKJ4weTkzBdhSQiGUehkEKf/ByOGz2I2RqmU0QyjEKhFVPLSnh7czVL1+2IuxQRkX1GodCKU5qG6dQhJBHJIAqFVhQX5XHEiP66NFVEMopCoQ1Ty0pY8u521mzWMJ0ikhkUCm1oGqZTh5BEJFMoFNowYmAB40s0TKeIZA6FQjumlpVQsXoLVTs0TKeIpD+FQjumlpXgDo8v0QlnEUl/CoV2HLRfEcMH9OJRHUISkQygUGiHmTF1QgnPLd/Ejj11cZcjIhIphUIHTD24hNqGRuZomE4RSXMKhQ74yIj+DOqtYTpFJP0pFDogkWWcMmEIc5ZVUVPfEHc5IiKRUSh00JSyEnbW1PP88ljHABIRiZRCoYOOGz2Q3nnZOoQkImlNodBBedkJThyvYTpFJL0pFD6EqWVD2LSrlpdWb4m7FBGRSEQaCmY2zcyWmdlyM7s8xfIvmlmVmS0IH1+Jsp69NXncYHKzs3QISUTSVmShYGYJ4GbgVGACcK6ZTUjR9B53Pyx83B5VPV2hd142Hz1wELMXrdMwnSKSlqLcUzgKWO7uK9y9FpgBnBHh++0TU8uGULllN4vf3R53KSIiXS7KUBgKrEmargzntfRpM1toZveZ2fBUKzKzC8yswswqqqrivav4pIOGkGVoRDYRSUtRhoKlmNfymMv/AaXuPhF4HPhTqhW5+63uXu7u5cXFxV1c5oczqHce5SMHqIM8EUlLUYZCJZD8zX8YsDa5gbtvcvemgQpuA46IsJ4uM6VsCEvX7WD1pl1xlyIi0qWiDIX5wBgzG2VmucB0YGZyAzPbL2nydGBJhPV0GQ3TKSLpKrJQcPd64EJgNsGH/b3uvsjMrjaz08NmF5nZIjN7FbgI+GJU9XSl4QMKmLBfH51XEJG0kx3lyt19FjCrxbwrk55fAVwRZQ1RmVpWwo1PvMGGHXsYXJQfdzkiIl1CdzR30tSDh+AOjy3W3oKIpA+FQieNG1LEyIEFOoQkImlFodBJZsbUshLmvrWR7RqmU0TShEJhL0wtG0Jdg/Pk0g1xlyIi0iUUCnvh8OH9KS7K06WpIpI2FAp7IStpmM49dRqmU0R6PoXCXppaVkJ1bQPPvrkx7lJERPaaQmEvHXvAQIryNUyniKQHhcJeys3O4t/GD+bxJeupb2iMuxwRkb2iUOgCU8tK2FJdx/xVGqZTRHo2hUIX+NjYYg3TKSJpQaHQBQrzsjlhzCAeW7xew3SKSI+mUOgiU8pKeGfrbhat1TCdItJzKRS6yMnNw3TqEJKI9FwKhS4yoDCXo0YNUCiISI+mUOhCU8tKeGP9TlZu1DCdItIzKRS60NSyErIMbntmRdyliIh0ikKhC+3frxfnTxrFXS+8zbwVm+IuR0TkQ1ModLFLp4xlxIACLr9/oTrJE5EeJ9JQMLNpZrbMzJab2eVttDvLzNzMyqOsZ18oyM3m2n8/hFWbqrnh8TfiLkdE5EOJLBTMLAHcDJwKTADONbMJKdoVARcBL0RVy7523IGDmH7kcG57egULK7fGXY6ISIdFuadwFLDc3Ve4ey0wAzgjRbufANcBeyKsZZ+74uMHMah3Hpfdt5A6dZQnIj1ElKEwFFiTNF0ZzmtmZocDw939obZWZGYXmFmFmVVUVVV1faUR6Nsrh59+6mCWrtvBLU+9FXc5IiIdEmUoWIp5zR0DmVkWcAPwnfZW5O63unu5u5cXFxd3YYnRmlJWwicm7sdvnljO8g074i5HRKRdUYZCJTA8aXoYsDZpugg4GJhjZquAY4CZ6XCyOdlVnyyjIC/BZfctpKFRneWJSPcWZSjMB8aY2SgzywWmAzObFrr7Nncf5O6l7l4KzANOd/eKCGva54qL8rjytAm8/PZW/jx3VdzliIi0KbJQcPd64EJgNrAEuNfdF5nZ1WZ2elTv2x2defhQPja2mOtmL2PN5uq4yxERaVWHQsHM/tyReS25+yx3H+vuo939mnDele4+M0Xbyem2l9DEzLjmzIMx4Pt/f01jLohIt9XRPYWy5InwHoQjur6c9DWsfwHfO3U8z7y5kftffifuckREUmozFMzsCjPbAUw0s+3hYwewAXhwn1SYRj539EjKR/bnJw8tZsOOtLotQ0TSRJuh4O4/d/ci4Hp37xM+itx9oLtfsY9qTBtZWcYvzprI7roGrpq5KO5yREQ+oKOHjx4ys0IAM/ucmf3azEZGWFfaGl3cm4tPGsOs19bxyOvvxl2OiMj7dDQU/geoNrNDgcuA1cCdkVWV5i444QAm7NeHHz24iG3VdXGXIyLSrKOhUO/BJTNnADe5+00EN59JJ+QksrjurIls3lXLNbMWx12OiEizjobCDjO7Avg88HB49VFOdGWlv4OH9uWCEw7g3opKnn1zY9zliIgAHQ+Fc4Aa4Hx3X0fQsd31kVWVIS4+aQwHDCrk8gcWUl1bH3c5IiIdC4UwCP4K9DWz04A97q5zCnspPyfBtZ+eSOWW3fxytgbkEZH4dfSO5s8ALwJnA58BXjCzs6IsLFMcNWoAnz9mJH94fiUvv70l7nJEJMN19PDRD4Aj3f08d/8CwQA6P4qurMxy2bRx7Ncnn+/dt5Caeo3rLCLx6WgoZLn7hqTpTR/itdKOovwcrjnzEN7csJObn9SAPCISn45+sD9iZrPN7Itm9kXgYWBWdGVlnhPHD+bMw4fy308uZ8m72+MuR0QyVHt9Hx1oZpPc/bvALcBE4FBgLnDrPqgvo/zotAn07ZXD9+5fSL3GdRaRGLS3p3AjsAPA3R9w90vd/RKCvYQboy4u0wwozOWq08tYWLmNPzy3Ku5yRCQDtRcKpe6+sOXMcNyD0kgqynCnTdyPkw8awq8eW8aqjbviLkdEMkx7oZDfxrJeXVmIBMyMn37qYHKysrj8gYUakEdE9qn2QmG+mX215Uwz+zLwUjQlSUnffL7/iYOYt2IzM+avibscEckg2e0s/zbwdzP7LO+FQDmQC5wZZWGZbvqRw5m5YC0/e3gJJ44bTEnftnbaRES6RnuD7Kx39+OA/wJWhY//cvdjw64v2mRm08xsmZktN7PLUyz/upm9ZmYLzOxZM5vQuc1IP2bGz//9EOoaG/nhP17XYSQR2Sc62vfRk+7+2/Dxr468JuxJ9WbgVGACcG6KD/273P0Qdz8MuA749YeoPe2VDirkO6eM4/El63n4NQ3IIyLRi/Ku5KOA5e6+wt1rgRkE4zE0c/fku7QKAX0dbuFLk0o5dFhffvzgIrbsqo27HBFJc1GGwlAg+SxpZTjvfczsW2b2FsGewkWpVmRmF5hZhZlVVFVVRVJsd5WdyOIXZ01k2+46fvKQBuQRkWhFGQqWYt4H9gTc/WZ3Hw18D/hhqhW5+63uXu7u5cXFxV1cZvc3vqQP3zzxQB545R2eXLah/ReIiHRSlKFQCQxPmh4GrG2j/QzgUxHW06N968TRjBncmx888Bo7azQgj4hEI8pQmA+MMbNRZpYLTAdmJjcwszFJk58A3oywnh4tLzvBL86ayLvb93DdI0vjLkdE0lRkoeDu9cCFwGxgCXCvuy8ys6vN7PSw2YVmtsjMFgCXAudFVU86+MiI/nzpuFHcOXc1L67cHHc5IpKGrKdd/15eXu4VFRVxlxGb6tp6pt74NFlmzLroeArz2rv/UEQEzOwldy9vr50GyulhCnKz+eVZh7JmczX/9X+L4i5HRNKMQqEHOvqAgXxz8oHcW1HJLN3UJiJdSKHQQ1188hgOHdaXKx54jXe37Y67HBFJEwqFHionkcVN0w+nrqGRS+95lYbGnnVuSES6J4VCD1Y6qJCrTi9j7opN3PbMirjLEZE0oFDo4c4+YhgfP6SEXz26jNcqt8Vdjoj0cAqFHs7M+NmZhzCodx4Xz3iF6lrd7SwinadQSAP9CnL51WcOZeWmXfzkoSVxlyMiPZhCIU0cN3oQXzthNHe/+DazF7U7/pGISEoKhTRy6SljOWRoXy6/fyHrt++JuxwR6YEUCmkkNzuLG6cfxp66Rr5z76s06jJVEfmQFAppZnRxb6785ASeXb6RO55bGXc5ItLDKBTS0PQjhzNlwhCue2QZi9bqMlUR6TiFQhoyM6799ET6FeRw8YwF7K5tiLskEekhFAppakBhLr/+zGEs37CTn83SZaoi0jEKhTT20TGD+Orxo/jzvNU8sWR93OWISA+gUEhz/zl1HBP268N371vIhh26TFVE2qZQSHN52Ql+c+5h7Kqp5z//tlCXqYpImxQKGeDAwUX88LQJPP1GFX+auyruckSkG4s0FMxsmpktM7PlZnZ5iuWXmtliM1toZk+Y2cgo68lknzt6BCcfNJif/3MpS9dtj7scEemmIgsFM0sANwOnAhOAc81sQotmrwDl7j4RuA+4Lqp6Mp2Z8YtPT6RPfg4X372APXW6TFVEPijKPYWjgOXuvsLda4EZwBnJDdz9SXevDifnAcMirCfjDeydxy/Pnsiy9Tu49p9L4y5HRLqhKENhKLAmaboynNeaLwP/TLXAzC4wswozq6iqqurCEjPP5HGD+dKkUv74/CqeXLYh7nJEpJuJMhQsxbyUl76Y2eeAcuD6VMvd/VZ3L3f38uLi4i4sMTN9b9p4xpcU8d2/LWTjzpq4yxGRbiTKUKgEhidNDwPWtmxkZicDPwBOd3d9Qu0D+TkJbpp+ONv31HHZfQtx12WqIhKIMhTmA2PMbJSZ5QLTgZnJDczscOAWgkDQsYx9aFxJEd8/dTz/WrqBv8xbHXc5ItJNRBYK7l4PXAjMBpYA97r7IjO72sxOD5tdD/QG/mZmC8xsZiurkwicd1wpk8cV89OHl/DG+h1xlyMi3YD1tEMH5eXlXlFREXcZaaNqRw3Tbnya4qI8HrxwEnnZibhLEpEImNlL7l7eXjvd0ZzhiovyuP7siSxdt4PrH1kWdzkiEjOFgvBv44fwhWNHcvuzK3nmTV3yK5LJFAoCwPc/fhBjBvfmO/e+yuZdtXGXIyIxUSgI8N5lqlur6/je/bpMVSRTKRSk2YT9+3DZtHE8tng9d7+4pv0XiEjaUSjI+5w/aRTHjxnE1Q8tYvmGnXGXIyL7mEJB3icry/jV2YfSKyfBRXe/ovMLIhlGoSAfMLhPPr88+1De3LCDKTc8zaOL1sVdkojsIwoFSemkg4Yw88KPMrgojwv+/BKX3rOAbdV1cZclIhFTKEirDtqvDw9eOImLTxrDzFfXcsoNT/GvpevjLktEIqRQkDblJLK45JSx/ONbk+hfkMv5f6zgu397le17tNcgko4UCtIhBw/ty8z/mMS3ThzN/S9XMvWGp3n6Dd39LJJuFArSYXnZCb47dTwPfHMShXnZfOGOF7nigdfYWVMfd2ki0kUUCvKhHTa8Hw/9x0f52gkHMGP+20y94WmeX74x7rJEpAsoFKRT8nMSXPHxg7jv68eSm53F/7v9Ba588HV2aa9BpEdTKMheOWLkAGZddDznTxrFn+et5tSbnuHFlZvjLktEOkmhIHutV26CKz85gRlfPQaAc26dy9X/t5jdtQ0xVyYiH5ZCQbrM0QcM5JFvH8/njxnJHc+t5OO/eYaXVmuvQaQnUShIlyrIzebqMw7mrq8cTW19I2f/71x+PmsJe+q01yDSE0QaCmY2zcyWmdlyM7s8xfITzOxlM6s3s7OirEX2reMOHMQj3z6ec44cwS1Pr+C03z7LgjVb4y5LRNoRWSiYWQK4GTgVmACca2YTWjR7G/gicFdUdUh8ivJz+Pm/H8Kd5x/Frpp6/v2/n+P62Uupqddeg0h3FeWewlHAcndf4e61wAzgjOQG7r7K3RcCjRHWITE7YWwxj3z7BD79kWHc/ORbnP7b53j9nW1xlyUiKUQZCkOB5OG7KsN5H5qZXWBmFWZWUVWlrhV6or69crj+7EP5/XnlbKmu5VM3P8cNj71Bbb2+D4h0J1GGgqWY16mBf939Vncvd/fy4uLivSxL4nTSQUN49JIT+OSh+3PTE2/yqZufY96KTRoTWqSbiDIUKoHhSdPDgLURvp/0EP0KcrnhnMO45fNHsGFHDdNvncfpv3uOBxe8Q12D9hxE4hRlKMwHxpjZKDPLBaYDMyN8P+lhppaV8MxlJ3LNmQezq7aei2cs4ITrnuSWp95i2251zS0SB4tyt93MPg7cCCSAO9z9GjO7Gqhw95lmdiTwd6A/sAdY5+5lba2zvLzcKyoqIqtZ4tHY6Mx5YwO3Pb2SuSs2UZCb4DPlwzl/0ihGDCyIuzyRHs/MXnL38nbb9bRjuQqF9Pf6O9u449mVzHx1LY3uTC0r4SvHj+IjI/pjlupUlYi0R6EgPd66bXu4c+4q/vrC22zbXcdhw/vxleNHMa2shOyEbsYX+TAUCpI2qmvrue+lSu54diWrNlUztF8vvjSplHOOHE5Rfk7c5Yn0CAoFSTsNjc4TS9Zz+zMreXHVZnrnZTP9yOF8cVIpw/rrvINIWxQKktZeXbOV3z+7kodfexeAUw8u4SvHH8Bhw/vFXJlI96RQkIywdutu/vT8Ku568W127KnnyNL+fPmjB3DKhCEksnRSWqSJQkEyys6aeu6dv4Y7nltJ5ZbdjBhQwPmTSjm7fDiFedlxlycSO4WCZKSGRufRReu4/dmVvLR6C33yszn36BF8/piROu8gGU2hIBnv5be38PtnVvLP19+l0WHckCImjyvmY+OKKR85gNxsXdYqmUOhIBJas7maR15fx5w3NvDiys3UNTiFuQmOO3AQk8cVM3ncYIb26xV3mSKRUiiIpLCrpp7n39rEnGUbmLOsine27gZgzODezQFRXtqfvOxEzJWKdC2Fgkg73J23qnYyZ1kVT71RxQsrNlPb0EhBboLjRjftRRTrXISkhY6Ggi7LkIxlZhw4uIgDBxfxleMPoLq2nrlvbWLOsirmvLGBx5esB2B0cSGTxw1m8rhijho1QHsRkta0pyCSgruzYuOuICCWbeCFlZuprW+kV06C40YPbD7UNHyA9iKkZ9DhI5EutLu2gbkrNvLUsirmvFHF6k3VABxQXMjkse/tReTnaC9CuieFgkiEVm7c1Xyyet6KTdTUN5KTMAYU5tK/IJd+BTkMKMylX0Eu/QtywnnB834FuWG7HPrk55ClO69lH9A5BZEIjRpUyKhBo/jSpFHsqWtg7opNvLhyM5t21rCluo6t1bW8sX4nW6tr2VJdR0Nj6i9fWQZ9e+U0B0lTeAwozAlD5L0g6V8YLB9QmEuOug6XiCgURPZSfk6CE8cN5sRxg1Mud3e276lvDogt1bVsra5l8666cF5tc5C8u20Pi9/dzpbqWvbUtT5e9YDCXAYX5VGc/Oidx+A++eEpcsR9AAAL0ElEQVTPYF5RXrYGJpIPRaEgEjEzo2+vHPr2ymHkwI6/bk9dQxAYYXhsDsNj084aNuyooWpH8HNF1S6qdtRQ2/DBEMnPyXovMIryKS7Ke1+YNM0b1DtXAxcJoFAQ6bbycxLs17cX+/Vt/25rd2fb7rrmoKhqDo09zfPeqtrJvJWb2Fpd94HXm8GAgtzmsBhYmEtBXjaFuQkKcrMpyE28b7owLxHMy82mMDebgrwEhbnZ5Odkac+kh4s0FMxsGnATkABud/drWyzPA+4EjgA2Aee4+6ooaxJJR2ZGv/B8xJghRW22ralvYOPOWjZsDwKjamcNG7YHP5sCZPWmaqpr66mubaC6tuFD1AEFOYl2A6VX7nshkp+TID87QV5OFnnZiffm5YTPw2X52cG8vOwsnZyPUGShYGYJ4GbgFKASmG9mM919cVKzLwNb3P1AM5sO/AI4J6qaRATyshMM7derw/09NTY6u+sa2FVbz+7aBnbVNFBdW8+u2gaqa5qC4/3Tu5rmhW23765j3bbdzdPVtQ3U1Ld+zqQ9udlZ5GdnkZcUHM0hEgZHXhg2udlGlhmJrPCR/DyrxbJweVaWkZ0V/ExY0vMsyDIjOyvrveeJYB05iSwSWUZOwkhkZZGdFSzLbvk8YeF0OD983+6yhxXlnsJRwHJ3XwFgZjOAM4DkUDgDuCp8fh/wOzMz72nXyYqksawsozAvu8vHpahvaKSmvpE9dQ3safpZ18CeukZq6hrYU99ATV0je+qDeU3L9iQtq3nfsuD5zpp6Nu4MltXUNVLb0Ehjo9PgTkND+LPRaXSnvtHpLp82TeGQHC5B+DQFjXHxyWM5/dD9o60jwnUPBdYkTVcCR7fWxt3rzWwbMBDYmNzIzC4ALgAYMWJEVPWKyD6UncgiO5EV+yBIHoZEfRgUDY1OYyPUNzbS4MHzlIHS8F77+sbwZ0Mj9Y1OfWMj9Q0ePk+a3+A0NDZS1xC0r0tu19AYzGtq0xi8Z11jY7hup1+vnMh/H1H+NVLtC7XM5I60wd1vBW6F4Oa1vS9NRCRg4SEgdWkViPIatEpgeNL0MGBta23MLBvoC2yOsCYREWlDlKEwHxhjZqPMLBeYDsxs0WYmcF74/CzgXzqfICISn8gOH4XnCC4EZhNcknqHuy8ys6uBCnefCfwe+LOZLSfYQ5geVT0iItK+SM/wuPssYFaLeVcmPd8DnB1lDSIi0nG6r11ERJopFEREpJlCQUREmikURESkWY8bec3MqoDVnXz5IFrcLd2DaVu6n3TZDtC2dFd7sy0j3b24vUY9LhT2hplVdGQ4up5A29L9pMt2gLalu9oX26LDRyIi0kyhICIizTItFG6Nu4AupG3pftJlO0Db0l1Fvi0ZdU5BRETalml7CiIi0gaFgoiINMuYUDCzaWa2zMyWm9nlcdfTWWY23MyeNLMlZrbIzC6Ou6a9YWYJM3vFzB6Ku5a9YWb9zOw+M1sa/m2OjbumzjKzS8J/W6+b2d1mlh93TR1lZneY2QYzez1p3gAze8zM3gx/9o+zxo5oZTuuD/99LTSzv5tZvyjeOyNCwcwSwM3AqcAE4FwzmxBvVZ1WD3zH3Q8CjgG+1YO3BeBiYEncRXSBm4BH3H08cCg9dJvMbChwEVDu7gcTdHvfk7q0/yMwrcW8y4En3H0M8EQ43d39kQ9ux2PAwe4+EXgDuCKKN86IUACOApa7+wp3rwVmAGfEXFOnuPu77v5y+HwHwYfP0Hir6hwzGwZ8Arg97lr2hpn1AU4gGB8Ed691963xVrVXsoFe4WiIBXxwxMRuy92f5oOjN54B/Cl8/ifgU/u0qE5ItR3u/qi714eT8whGs+xymRIKQ4E1SdOV9NAP0mRmVgocDrwQbyWddiNwGdAYdyF76QCgCvhDeCjsdjMrjLuoznD3d4BfAm8D7wLb3P3ReKvaa0Pc/V0IvlQBg2OupyucD/wzihVnSihYink9+lpcM+sN3A982923x13Ph2VmpwEb3P2luGvpAtnAR4D/cffDgV30jEMUHxAebz8DGAXsDxSa2efirUqSmdkPCA4j/zWK9WdKKFQCw5Omh9GDdolbMrMcgkD4q7s/EHc9nTQJON3MVhEczvs3M/tLvCV1WiVQ6e5Ne2z3EYRET3QysNLdq9y9DngAOC7mmvbWejPbDyD8uSHmejrNzM4DTgM+G9V49pkSCvOBMWY2ysxyCU6czYy5pk4xMyM4dr3E3X8ddz2d5e5XuPswdy8l+Hv8y9175DdSd18HrDGzceGsk4DFMZa0N94GjjGzgvDf2kn00JPmSWYC54XPzwMejLGWTjOzacD3gNPdvTqq98mIUAhPzlwIzCb4B36vuy+Kt6pOmwR8nuCb9YLw8fG4ixL+A/irmS0EDgN+FnM9nRLu7dwHvAy8RvAZ0WO6iTCzu4G5wDgzqzSzLwPXAqeY2ZvAKeF0t9bKdvwOKAIeC//f/28k761uLkREpElG7CmIiEjHKBRERKSZQkFERJopFEREpJlCQUREmikUJHZmtjP8WWpm/6+L1/39FtPPd+X6U7zfp8zsyijfo4N1lJrZ7qTLlv83adkRZvZa2GPwb8L7ETCzX5rZv8VXtXQHuiRVYmdmO929t5lNBv7T3U/7EK9NuHtDe+vuijo7WM/zBDcXbYxo/dlJnaK11a4UeCjs6bTlshcJeqedB8wCfuPu/zSzkcBt7j6la6uWnkR7CtKdXAscH36zvSQca+F6M5sf9iH/NQAzmxyOKXEXwQ1WmNk/zOylcByAC8J51xL09rnAzP4azmvaK7Fw3a+H35rPSVr3nKSxEf6a9E36WjNbHNbyy5bFm9lYoKYpEMys2MzuD+ufb2aTzCzLzFYl94UffmMfkqp9uPwqM7vVzB4F7jSzZ8zssKTXP2dmEzvyCw67eejj7nPDbhLuJOw11N1XAwPNrKTDfzFJP+6uhx6xPoCd4c/JBN9um+ZfAPwwfJ4HVBB01DaZoNO5UUltB4Q/ewGvAwOT153ivT5N0D99AhhC0L3DfuG6txH0j5VFcFfpR4EBwDLe27vul2I7vgT8Kmn6LuCj4fMRBF2TQDD2wpfC50cDj7fT/irgJaBXOH0ecGP4fCxQkaKW0vB39ArwFHB8OL+86f3C6eNb/M5vAz4d978JPeJ7ZHckOERiMgWYaGZnhdN9gTFALfCiu69ManuRmZ0ZPh8ettvUxro/CtztwaGn9Wb2FHAksD1cdyWAmS0g+ICdB+wBbjezh4FUI8XtR9CFdpOTgQnhjgZAHzMrAu4BrgT+QNDv0z3ttAeY6e67w+d/A35kZt8l6EL5jylqeRcY4e6bzOwI4B9mVkb7PQZvIOgdVTKUQkG6MwP+w91nv29mcO5hV4vpk4Fj3b3azOYA7Q0hmerDsUlN0vMGINvd683sKIIO4qYT9KXV8qTsboLgapIV1rQ7uZGZzQUONLNigkM3P22nPSRtb7iNjxF0cf0Zgm//7+PuNU3b4e4vmdlbBHsVlbx/cJaWPQbnh9shGUrnFKQ72UHQ4VeT2cA3LOgqHDMba6kHr+kLbAk/LMcTDFPapK7p9S08DZwTnrcoJhg57cXWCrNg/Iq+7j4L+DZBp3ctLQEOTJp+lCA8mtZxGIC7O/B34NcEh4g2tdW+FbcDvwHmu3vLkcaazmckwucHEOw5rfBgkJkdZnZMeK7kC7y/19CxBIffJEMpFKQ7WQjUm9mrZnYJwQffYuBlCwYwv4XUe7ePANkW9FD6E4JDPU1uBRY2nWhO8vfw/V4F/gVc5kEX2K0pAh4K3+Mp4JIUbZ4GDm86MU041nF4Ynox8PWktvcAn+O9Q0fttX8fDwYn2k5wCCqVEwi2+1WCXk+/nhQe3yD43S4H3iIcwSsMzwMJzt1IhtIlqSJdyMxuAv7P3R+P+H32B+YA4929S4YzDc/JfMTdf9QV65OeSXsKIl3rZwSD3UfGzL5AMC73D7oqEELZwK+6cH3SA2lPQUREmmlPQUREmikURESkmUJBRESaKRRERKSZQkFERJr9f3n7NRqeMHmlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy rate:  72.5\n"
     ]
    }
   ],
   "source": [
    "dims = {}\n",
    "dims[\"numberInputs\"] = trainingData.shape[0]\n",
    "dims[\"numberOutputs\"] = trainingLabels.shape[0]\n",
    "dims[\"hiddenLayerSizes\"] = (20, 10)\n",
    "\n",
    "testParams, testCosts, testGraph = model(\n",
    "    data = trainingData, \n",
    "    labels = trainingLabels, \n",
    "    dims = dims,\n",
    "    numIterations = 600,\n",
    "    learningRate = 0.0001,\n",
    "    lamb = 0.0, \n",
    "    batchSize = 50,\n",
    "    dropProb = 1,\n",
    "    beta1 = 0.9, \n",
    "    beta2 = 0.999,  \n",
    "    epsilon = 1e-8,\n",
    "    printCost = True, \n",
    "    showGraph = True\n",
    ")\n",
    "\n",
    "adamPreds = predict(testData, testParams, testLabels)\n",
    "print(\"Test set accuracy rate: \", adamPreds['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "Our best model so far in this series achieved an accuracy rate of 73.5% after quite a bit of hyperparameter tuning.  After just a few manual tweaks to the hyperparameters (not shown here) we have a 72.5% accuracy rate with the optimizations we've implemented.  No doubt with several rounds of tuning we could squeeze out some additional performance.  The following table summarizes what we have so far:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Model Type</th>\n",
    "        <th>Test Set Accuracy</th>\n",
    "        <th>F-Score</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Linear regression</td>\n",
    "        <td>65.5%</td>\n",
    "        <td>[0.64974619 0.66009852]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Shallow neural network</td>\n",
    "        <td>73.0%</td>\n",
    "        <td>[0.73786408 0.72164948]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>N-layer neural network</td>\n",
    "        <td>73.5%</td>\n",
    "        <td>[0.72820513 0.74146341]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>N-layer neural network - This write-up</td>\n",
    "        <td>72.5%</td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "However....\n",
    "\n",
    "In parallel to this write-up I was also working on the same model using TensorFlow and Keras.  I wanted to have something to compare against as a sanity check.  The model was definitely growing and there are a lot of moving parts at this point.\n",
    "\n",
    "Without much effort the TensorFlow model achieved a 71% accuracy rate against our image set with no tuning, and the Keras model--which utilized a convolutional neural network--achieved an 82% (!) accuracy rate on the first try with no tuning.  So, after seeing that the following thoughts occurred:\n",
    "\n",
    "1. I was achieving similar results using frameworks and libraries as I was building models from scratch, which was a great validation for the work I've been doing\n",
    "2. It was probably time to start using frameworks and libraries to take advantage of their speed and ease of use; my learning goals from modeling by hand had been achieved\n",
    "3. I could live with an accuracy rate of 72.5% for the model in this write-up; there didn't seem to be a huge benefit to a lengthy tuning processes when the convolutional neural networks were clearly superior (which frankly isn’t a surprise….)\n",
    "\n",
    "So, having said that I'm going to wrap up work on building models from scratch, and the next write-ups will utilize TensorFlow and/or Keras.  :)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
