{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Vanishing/Exploding-Gradients\" data-toc-modified-id=\"Vanishing/Exploding-Gradients-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Vanishing/Exploding Gradients</a></span><ul class=\"toc-item\"><li><span><a href=\"#Xavier-initializer-(aka-Glorot-uniform-initializer)\" data-toc-modified-id=\"Xavier-initializer-(aka-Glorot-uniform-initializer)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Xavier initializer (aka Glorot uniform initializer)</a></span></li><li><span><a href=\"#He-initializer\" data-toc-modified-id=\"He-initializer-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>He initializer</a></span></li><li><span><a href=\"#Nonsaturating-Activation-Functions\" data-toc-modified-id=\"Nonsaturating-Activation-Functions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Nonsaturating Activation Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exponential-linear-unit-(ELU)\" data-toc-modified-id=\"Exponential-linear-unit-(ELU)-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Exponential linear unit (ELU)</a></span></li><li><span><a href=\"#Leaky-ReLu\" data-toc-modified-id=\"Leaky-ReLu-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Leaky ReLu</a></span></li></ul></li><li><span><a href=\"#Batch-normalization\" data-toc-modified-id=\"Batch-normalization-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Batch normalization</a></span></li><li><span><a href=\"#Gradient-Clipping\" data-toc-modified-id=\"Gradient-Clipping-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Gradient Clipping</a></span></li></ul></li><li><span><a href=\"#Transfer-Learning\" data-toc-modified-id=\"Transfer-Learning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Transfer Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-sample-model-to-learn-from\" data-toc-modified-id=\"Build-sample-model-to-learn-from-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Build sample model to learn from</a></span></li><li><span><a href=\"#Transfer-learning---Method-one\" data-toc-modified-id=\"Transfer-learning---Method-one-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Transfer learning - Method one</a></span></li><li><span><a href=\"#Transfer-learning---Method-two\" data-toc-modified-id=\"Transfer-learning---Method-two-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Transfer learning - Method two</a></span></li><li><span><a href=\"#Freezing-the-lower-layers\" data-toc-modified-id=\"Freezing-the-lower-layers-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Freezing the lower layers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Method-one---tf.get_collection\" data-toc-modified-id=\"Method-one---tf.get_collection-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Method one - tf.get_collection</a></span></li><li><span><a href=\"#Method-two---tf.stop_gradient\" data-toc-modified-id=\"Method-two---tf.stop_gradient-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Method two - tf.stop_gradient</a></span></li><li><span><a href=\"#Method-three---Hybrid\" data-toc-modified-id=\"Method-three---Hybrid-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Method three - Hybrid</a></span></li></ul></li><li><span><a href=\"#Caching-frozen-layers\" data-toc-modified-id=\"Caching-frozen-layers-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Caching frozen layers</a></span></li><li><span><a href=\"#Altering-layers\" data-toc-modified-id=\"Altering-layers-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Altering layers</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:16:47.837591Z",
     "start_time": "2018-09-10T01:16:47.832590Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:16:49.170664Z",
     "start_time": "2018-09-10T01:16:48.004580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Setup vars for the MINST data set\n",
    "inputs = 28 * 28    # image dim in pixels\n",
    "hidden1 = 300\n",
    "hidden2 = 100\n",
    "hidden3 = 50\n",
    "hidden4 = 50\n",
    "hidden5 = 50\n",
    "outputs = 10\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batchSize = 50\n",
    "\n",
    "mninst = input_data.read_data_sets(\"./datasets/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetGraph( seed= 10):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-07T20:47:08.055767Z",
     "start_time": "2018-09-07T20:47:08.051832Z"
    }
   },
   "source": [
    "## Xavier initializer (aka Glorot uniform initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the default init for the `tf.layers.dense` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-07T20:47:13.613722Z",
     "start_time": "2018-09-07T20:47:13.609737Z"
    }
   },
   "source": [
    "## He initializer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:12:57.334476Z",
     "start_time": "2018-09-09T05:12:57.328458Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:12:57.873940Z",
     "start_time": "2018-09-09T05:12:57.662931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.92 Test Acc:  0.9055\n",
      "10 Train Acc:  0.98 Test Acc:  0.9605\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9699\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helps to mitigate the `dying ReLu` problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential linear unit (ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:53.014680Z",
     "start_time": "2018-09-09T05:14:52.987684Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "entropy = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:54.082231Z",
     "start_time": "2018-09-09T05:14:53.792180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # Use ELU\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.elu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.elu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.114258Z",
     "start_time": "2018-09-09T05:14:56.169295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Test Acc:  0.9023\n",
      "10 Train Acc:  0.96 Test Acc:  0.949\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9625\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.132242Z",
     "start_time": "2018-09-09T05:16:23.117241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "entropy = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.213247Z",
     "start_time": "2018-09-09T05:16:23.174245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write the leaky ReLu function\n",
    "def leakyReLu(z, name = None):\n",
    "    return tf.maximum(0.01 * z, z, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.568265Z",
     "start_time": "2018-09-09T05:16:23.216247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # Use leaky ReLu\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = leakyReLu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = leakyReLu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:17:53.829453Z",
     "start_time": "2018-09-09T05:16:23.573285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.94 Test Acc:  0.9041\n",
      "10 Train Acc:  1.0 Test Acc:  0.9595\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9714\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:17:53.852456Z",
     "start_time": "2018-09-09T05:17:53.832453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "entropy = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:17:53.883446Z",
     "start_time": "2018-09-09T05:17:53.855456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write a partial, so we can have cleaner code  :)\n",
    "from functools import partial\n",
    "\n",
    "# We'll use this to control batch normalization during training... \n",
    "# Set it to TRUE when training, and FALSE otherwise\n",
    "training = tf.placeholder_with_default(False, shape = (), name = 'training')\n",
    "\n",
    "batchNormLayer = partial(tf.layers.batch_normalization, training = training, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:17:54.964502Z",
     "start_time": "2018-09-09T05:17:53.889439Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\")\n",
    "    batchNorm1 = batchNormLayer(layer1)\n",
    "    batchNorm1Activation = tf.nn.elu(batchNorm1)\n",
    "    \n",
    "    layer2 = tf.layers.dense(batchNorm1Activation, hidden2, name = \"hLayerTwo\")\n",
    "    batchNorm2 = batchNormLayer(layer2)\n",
    "    batchNorm2Activation = tf.nn.elu(batchNorm2)\n",
    "    \n",
    "    \n",
    "    yHBeforeBatchNorm = tf.layers.dense(batchNorm2Activation, outputs, name = \"yH\")\n",
    "    yH = batchNormLayer(yHBeforeBatchNorm)   \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:19:53.042264Z",
     "start_time": "2018-09-09T05:17:54.975503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  1.0 Test Acc:  0.925\n",
      "10 Train Acc:  1.0 Test Acc:  0.9744\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9779\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "\n",
    "# New operations added from the batch normalization\n",
    "updateOps = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            # Note that training = TRUE\n",
    "            sess.run([opt, updateOps], feed_dict = {training: True, x: xBatch, y: yBatch})\n",
    "        # Note that training = FALSE by default\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:19:53.078283Z",
     "start_time": "2018-09-09T05:19:53.045263Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "entropy = batchNorm1 = batchNorm1Activation = batchNorm2 = batchNorm2Activation = None\n",
    "yHBeforeBatchNorm = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:19:53.430303Z",
     "start_time": "2018-09-09T05:19:53.081283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    # Gradient clipping threshold\n",
    "    threshold = 1.0\n",
    "    \n",
    "    # Setup optimizer to use gradient clipping\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr)\n",
    "    gradVars = opt.compute_gradients(loss)\n",
    "    cappedGradVars = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "                     for grad, var in gradVars]\n",
    "    clippedOpt = opt.apply_gradients(cappedGradVars)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Save the trained model parameters to disk\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:21:29.025776Z",
     "start_time": "2018-09-09T05:19:53.433303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.88 Test Acc:  0.9012\n",
      "10 Train Acc:  0.96 Test Acc:  0.9592\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9708\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([clippedOpt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "    \n",
    "    # Save the trained model\n",
    "    savePath = saver.save(sess, \"./DNN-TensorFlow-Gradient-Clipping.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build sample model to learn from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T22:35:01.589522Z",
     "start_time": "2018-09-09T22:35:01.578454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T22:36:35.145813Z",
     "start_time": "2018-09-09T22:35:01.897473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.98 Test Acc:  0.895\n",
      "10 Train Acc:  1.0 Test Acc:  0.9703\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9748\n"
     ]
    }
   ],
   "source": [
    "# Create and save base model we want to use later on for transfer learning\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    layer3 = tf.layers.dense(layer2, hidden3, name = \"hLayerThree\", activation = tf.nn.relu)\n",
    "    layer4 = tf.layers.dense(layer3, hidden4, name = \"hLayerFour\", activation = tf.nn.relu)\n",
    "    layer5 = tf.layers.dense(layer4, hidden5, name = \"hLayerFive\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer5, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Save our trained model parameters to disk\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Collect all the model operations in one spot to support model transfer learning\n",
    "for op in (x, y, layer1, layer2, layer3, layer4, layer5, yH, loss, opt, correct, accuracy):\n",
    "    tf.add_to_collection(\"modelOps\", op)\n",
    "\n",
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "    \n",
    "    # Save the CG, so we can restore and use it later on in another model\n",
    "    savePath = saver.save(sess, \"./DNN-Base-Pretrained-Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIME TAKEN:  1M 33.2S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T22:36:35.154795Z",
     "start_time": "2018-09-09T22:36:35.148813Z"
    }
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning - Method one\n",
    "\n",
    "Attach to the TF CG components by utilizing the collection created by the orig. model author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:21:30.353130Z",
     "start_time": "2018-09-09T23:21:30.335127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:21:30.956161Z",
     "start_time": "2018-09-09T23:21:30.828154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN-Base-Pretrained-Model\n"
     ]
    }
   ],
   "source": [
    "sess2 = tf.Session() \n",
    "\n",
    "#First let's load meta graph and restore weights\n",
    "saver2 = tf.train.import_meta_graph('./DNN-Base-Pretrained-Model.meta')\n",
    "saver2.restore(sess2,tf.train.latest_checkpoint('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:22:31.226613Z",
     "start_time": "2018-09-09T23:22:31.220596Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attach to the TF CG components by utilizing the collection created by the orig. model author\n",
    "# and pull out the first four\n",
    "tf.get_collection(\"modelOps\")\n",
    "x2, y2 = tf.get_collection(\"modelOps\")[0:2]\n",
    "opt2 = tf.get_collection(\"modelOps\")[9]\n",
    "accuracy2 = tf.get_collection(\"modelOps\")[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:22:51.579779Z",
     "start_time": "2018-09-09T23:22:51.376767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9756\n"
     ]
    }
   ],
   "source": [
    "accTest2 = accuracy2.eval(session = sess2, feed_dict = {x2: mninst.test.images, y2: mninst.test.labels})\n",
    "print(accTest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:23:07.092650Z",
     "start_time": "2018-09-09T23:23:07.084667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'x:0' shape=(?, 784) dtype=float32>,\n",
       " <tf.Tensor 'y:0' shape=(?,) dtype=int64>,\n",
       " <tf.Tensor 'neuralNetEasy/hLayerOne/Relu:0' shape=(?, 300) dtype=float32>,\n",
       " <tf.Tensor 'neuralNetEasy/hLayerTwo/Relu:0' shape=(?, 100) dtype=float32>,\n",
       " <tf.Tensor 'neuralNetEasy/hLayerThree/Relu:0' shape=(?, 50) dtype=float32>,\n",
       " <tf.Tensor 'neuralNetEasy/hLayerFour/Relu:0' shape=(?, 50) dtype=float32>,\n",
       " <tf.Tensor 'neuralNetEasy/hLayerFive/Relu:0' shape=(?, 50) dtype=float32>,\n",
       " <tf.Tensor 'neuralNetEasy/yH/BiasAdd:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'loss/loss:0' shape=() dtype=float32>,\n",
       " <tf.Operation 'optimizer/GradientDescent' type=NoOp>,\n",
       " <tf.Tensor 'eval/in_top_k/InTopKV2:0' shape=(?,) dtype=bool>,\n",
       " <tf.Tensor 'eval/accuracy:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the items in the collection\n",
    "tf.get_collection(\"modelOps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning - Method two\n",
    "\n",
    "Attach to the TF CG components by calling them specificly from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:23:07.948719Z",
     "start_time": "2018-09-09T23:23:07.931697Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:23:08.483748Z",
     "start_time": "2018-09-09T23:23:08.323740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN-Base-Pretrained-Model\n"
     ]
    }
   ],
   "source": [
    "# Create a new session\n",
    "sess = tf.Session()    \n",
    "\n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('./DNN-Base-Pretrained-Model.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:23:09.024779Z",
     "start_time": "2018-09-09T23:23:08.783746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9756\n"
     ]
    }
   ],
   "source": [
    "# Attach to the TF CG components by calling them specifically from the graph\n",
    "graph = tf.get_default_graph()\n",
    "x = graph.get_tensor_by_name(\"x:0\")\n",
    "y = graph.get_tensor_by_name(\"y:0\")\n",
    "\n",
    "feed_dict = {x: mninst.test.images, y: mninst.test.labels}\n",
    "accuracy = graph.get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "accTest = accuracy.eval(session = sess, feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "print(accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:10.590576Z",
     "start_time": "2018-09-09T05:23:10.585594Z"
    }
   },
   "source": [
    "## Freezing the lower layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:34:28.310319Z",
     "start_time": "2018-09-10T01:34:28.306300Z"
    }
   },
   "source": [
    "### Method one - tf.get_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:25:19.436888Z",
     "start_time": "2018-09-10T01:25:19.411868Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:25:20.628946Z",
     "start_time": "2018-09-10T01:25:20.491930Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load previous model meta graph\n",
    "saver = tf.train.import_meta_graph('./DNN-Base-Pretrained-Model.meta')\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:25:22.940070Z",
     "start_time": "2018-09-10T01:25:22.931084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hLayerFour/kernel:0' shape=(50, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'hLayerFour/bias:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'hLayerFive/kernel:0' shape=(50, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'hLayerFive/bias:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'yH/kernel:0' shape=(50, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'yH/bias:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach to hidden layers 4, 5, and the output (yH), \n",
    "# so we can train only these layers and exclude the others (i.e. the \"frozen layers\")\n",
    "trainableVars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = \"hLayerFour|hLayerFive|yH\")\n",
    "trainableVars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:26:25.232659Z",
     "start_time": "2018-09-10T01:25:26.508275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN-Base-Pretrained-Model\n",
      "0 Train Acc:  0.98 Test Acc:  0.977\n",
      "10 Train Acc:  1.0 Test Acc:  0.9761\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9761\n"
     ]
    }
   ],
   "source": [
    "# Attach to the TF CG components by calling them specifically from the graph\n",
    "x = graph.get_tensor_by_name(\"x:0\")\n",
    "y = graph.get_tensor_by_name(\"y:0\")\n",
    "loss = graph.get_tensor_by_name(\"loss/loss:0\")\n",
    "accuracy = graph.get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "# Create a new optimizer, and pass it the unfrozen layers we pulled out in the previous step\n",
    "opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss, var_list = trainableVars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Restore weights from orig. model\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    # Further training using new layers and/or new data\n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIME TAKEN:  58.7s  \n",
    "ORIG MODEL TIME TAKEN: 1M 33.2S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:26:25.240656Z",
     "start_time": "2018-09-10T01:26:25.235636Z"
    }
   },
   "source": [
    "### Method two - tf.stop_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:26:25.316659Z",
     "start_time": "2018-09-10T01:26:25.243636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:26:25.674680Z",
     "start_time": "2018-09-10T01:26:25.319652Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load previous model meta graph\n",
    "saver = tf.train.import_meta_graph('./DNN-Base-Pretrained-Model.meta')\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:27:23.764006Z",
     "start_time": "2018-09-10T01:26:25.677663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN-Base-Pretrained-Model\n",
      "0 Train Acc:  0.96 Test Acc:  0.964\n",
      "10 Train Acc:  1.0 Test Acc:  0.974\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9752\n"
     ]
    }
   ],
   "source": [
    "# Attach to the TF CG components by calling them specifically from the graph\n",
    "x = graph.get_tensor_by_name(\"x:0\")\n",
    "y = graph.get_tensor_by_name(\"y:0\")\n",
    "l3 = graph.get_tensor_by_name(\"neuralNetEasy/hLayerThree/Relu:0\")\n",
    "\n",
    "# Freeze the network below this point\n",
    "l3Stop = tf.stop_gradient(l3)\n",
    "\n",
    "# Add some new, unfrozen layers to the CG\n",
    "newL4 = tf.layers.dense(l3Stop, hidden4, name = \"newHLayerFour\", activation = tf.nn.relu)\n",
    "newL5 = tf.layers.dense(newL4, hidden5, name = \"newHLayerFive\", activation = tf.nn.relu)\n",
    "newYH = tf.layers.dense(newL5, outputs, name = \"newYH\")\n",
    "\n",
    "# Define new loss function\n",
    "with tf.name_scope(\"newLoss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = newYH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "       \n",
    "# New eval function\n",
    "with tf.name_scope(\"newEval\"):\n",
    "    correct = tf.nn.in_top_k(newYH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "    \n",
    "# New optimizer\n",
    "with tf.name_scope(\"newOptimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Init vars\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Restore weights from orig. model\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    # Further training using new layers and/or new data\n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIME TAKEN:  58.1s  \n",
    "ORIG MODEL TIME TAKEN: 1M 33.2S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:10.650597Z",
     "start_time": "2018-09-09T05:23:10.593594Z"
    }
   },
   "source": [
    "## Caching frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "l3 = l3Stop = newL4 = newL5 = newYH = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous model meta graph\n",
    "saver = tf.train.import_meta_graph('./DNN-Base-Pretrained-Model.meta')\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN-Base-Pretrained-Model\n",
      "0 Train Acc:  1.0 Test Acc:  0.9648\n",
      "10 Train Acc:  0.98 Test Acc:  0.9753\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9751\n"
     ]
    }
   ],
   "source": [
    "# Attach to the TF CG components by calling them specifically from the graph\n",
    "x = graph.get_tensor_by_name(\"x:0\")\n",
    "y = graph.get_tensor_by_name(\"y:0\")\n",
    "\n",
    "# Cache this layer\n",
    "l3 = graph.get_tensor_by_name(\"neuralNetEasy/hLayerThree/Relu:0\")\n",
    "\n",
    "# Don't forget this part!  We don't want the model using any of the earlier hidden layers....\n",
    "l3Stop = tf.stop_gradient(l3)\n",
    "\n",
    "# Add some new, unfrozen layers to the CG\n",
    "newL4 = tf.layers.dense(l3Stop, hidden4, name = \"newHLayerFour\", activation = tf.nn.relu)\n",
    "newL5 = tf.layers.dense(newL4, hidden5, name = \"newHLayerFive\", activation = tf.nn.relu)\n",
    "newYH = tf.layers.dense(newL5, outputs, name = \"newYH\")\n",
    "\n",
    "# Define new loss function\n",
    "with tf.name_scope(\"newLoss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = newYH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "       \n",
    "# New eval function\n",
    "with tf.name_scope(\"newEval\"):\n",
    "    correct = tf.nn.in_top_k(newYH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "    \n",
    "# New optimizer\n",
    "with tf.name_scope(\"newOptimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Restore weights from orig. model\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    # Create the cache of hidden layer three\n",
    "    # We run the test and train data through the previously trained model layer\n",
    "    # And cache the result, so we don't have to run through this again\n",
    "    l3CacheTrain = sess.run(l3, feed_dict = {x: mninst.train.images, y: mninst.train.labels})\n",
    "    l3CacheTest = sess.run(l3, feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "    \n",
    "    numCacheGroupings = len(l3CacheTrain) // batchSize\n",
    "    \n",
    "    # Further training using new layers and/or new data with the L3 cache\n",
    "    for e in range(epochs):\n",
    "        # Split the L3 cache up into batches\n",
    "        indices = np.random.permutation(len(mninst.train.images))\n",
    "        cacheBatchesData = np.array_split(l3CacheTrain[indices], numCacheGroupings)\n",
    "        cacheBatchLabels = np.array_split(mninst.train.labels[indices], numCacheGroupings)\n",
    "              \n",
    "        for xBatch, yBatch in zip(cacheBatchesData, cacheBatchLabels):\n",
    "            \n",
    "            # Note we pass in the cached layer to the optimizer instad of the training data\n",
    "            sess.run([opt], feed_dict = {l3: xBatch, y: yBatch})\n",
    "            \n",
    "            accTrain = accuracy.eval(feed_dict = {l3: xBatch, y: yBatch})\n",
    "            accTest = accuracy.eval(feed_dict = {l3: l3CacheTest, y: mninst.test.labels})\n",
    "        \n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:10.701582Z",
     "start_time": "2018-09-09T05:23:10.654598Z"
    }
   },
   "source": [
    "## Altering layers\n",
    "\n",
    "See the above section on `Transfer Learning` for examples of this where we added additional layers, estimators, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
