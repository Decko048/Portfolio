{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Vanishing/Exploding-Gradients\" data-toc-modified-id=\"Vanishing/Exploding-Gradients-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Vanishing/Exploding Gradients</a></span><ul class=\"toc-item\"><li><span><a href=\"#Xavier-initializer-(aka-Glorot-uniform-initializer)\" data-toc-modified-id=\"Xavier-initializer-(aka-Glorot-uniform-initializer)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Xavier initializer (aka Glorot uniform initializer)</a></span></li><li><span><a href=\"#He-initializer\" data-toc-modified-id=\"He-initializer-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>He initializer</a></span></li><li><span><a href=\"#Nonsaturating-Activation-Functions\" data-toc-modified-id=\"Nonsaturating-Activation-Functions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Nonsaturating Activation Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exponential-linear-unit-(ELU)\" data-toc-modified-id=\"Exponential-linear-unit-(ELU)-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Exponential linear unit (ELU)</a></span></li><li><span><a href=\"#Leaky-ReLu\" data-toc-modified-id=\"Leaky-ReLu-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Leaky ReLu</a></span></li></ul></li><li><span><a href=\"#Batch-normalization\" data-toc-modified-id=\"Batch-normalization-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Batch normalization</a></span></li><li><span><a href=\"#Gradient-Clipping\" data-toc-modified-id=\"Gradient-Clipping-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Gradient Clipping</a></span></li></ul></li><li><span><a href=\"#Transfer-Learning\" data-toc-modified-id=\"Transfer-Learning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Transfer Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-sample-model-to-learn-from\" data-toc-modified-id=\"Build-sample-model-to-learn-from-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Build sample model to learn from</a></span></li><li><span><a href=\"#Transfer-learning---Method-one\" data-toc-modified-id=\"Transfer-learning---Method-one-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Transfer learning - Method one</a></span></li><li><span><a href=\"#Transfer-learning---Method-two\" data-toc-modified-id=\"Transfer-learning---Method-two-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Transfer learning - Method two</a></span></li><li><span><a href=\"#Freezing-the-lower-layers\" data-toc-modified-id=\"Freezing-the-lower-layers-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Freezing the lower layers</a></span></li><li><span><a href=\"#Caching-frozen-layers\" data-toc-modified-id=\"Caching-frozen-layers-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Caching frozen layers</a></span></li><li><span><a href=\"#Altering-layers\" data-toc-modified-id=\"Altering-layers-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Altering layers</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:12:52.634088Z",
     "start_time": "2018-09-09T05:12:52.626083Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:12:53.911182Z",
     "start_time": "2018-09-09T05:12:53.046152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Setup vars for the MINST data set\n",
    "inputs = 28 * 28    # image dim in pixels\n",
    "hidden1 = 300\n",
    "hidden2 = 100\n",
    "hidden3 = 50\n",
    "hidden4 = 50\n",
    "hidden5 = 50\n",
    "outputs = 10\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batchSize = 50\n",
    "\n",
    "mninst = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-07T20:47:08.055767Z",
     "start_time": "2018-09-07T20:47:08.051832Z"
    }
   },
   "source": [
    "## Xavier initializer (aka Glorot uniform initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the default init for the `tf.layers.dense` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-07T20:47:13.613722Z",
     "start_time": "2018-09-07T20:47:13.609737Z"
    }
   },
   "source": [
    "## He initializer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:12:57.334476Z",
     "start_time": "2018-09-09T05:12:57.328458Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:12:57.873940Z",
     "start_time": "2018-09-09T05:12:57.662931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.92 Test Acc:  0.9055\n",
      "10 Train Acc:  0.98 Test Acc:  0.9605\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9699\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helps to mitigate the `dying ReLu` problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential linear unit (ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:53.014680Z",
     "start_time": "2018-09-09T05:14:52.987684Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "entropy = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:54.082231Z",
     "start_time": "2018-09-09T05:14:53.792180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # Use ELU\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.elu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.elu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.114258Z",
     "start_time": "2018-09-09T05:14:56.169295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Test Acc:  0.9023\n",
      "10 Train Acc:  0.96 Test Acc:  0.949\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9625\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.132242Z",
     "start_time": "2018-09-09T05:16:23.117241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "entropy = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.166242Z",
     "start_time": "2018-09-09T05:16:23.135245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.213247Z",
     "start_time": "2018-09-09T05:16:23.174245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write the leaky ReLu function\n",
    "def leakyReLu(z, name = None):\n",
    "    return tf.maximum(0.01 * z, z, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.568265Z",
     "start_time": "2018-09-09T05:16:23.216247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # Use leaky ReLu\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = leakyReLu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = leakyReLu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:17:53.829453Z",
     "start_time": "2018-09-09T05:16:23.573285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.94 Test Acc:  0.9041\n",
      "10 Train Acc:  1.0 Test Acc:  0.9595\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9714\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:17:53.852456Z",
     "start_time": "2018-09-09T05:17:53.832453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "entropy = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:17:53.883446Z",
     "start_time": "2018-09-09T05:17:53.855456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write a partial, so we can have cleaner code  :)\n",
    "from functools import partial\n",
    "\n",
    "# We'll use this to control batch normalization during training... \n",
    "# Set it to TRUE when training, and FALSE otherwise\n",
    "training = tf.placeholder_with_default(False, shape = (), name = 'training')\n",
    "\n",
    "batchNormLayer = partial(tf.layers.batch_normalization, training = training, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:17:54.964502Z",
     "start_time": "2018-09-09T05:17:53.889439Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\")\n",
    "    batchNorm1 = batchNormLayer(layer1)\n",
    "    batchNorm1Activation = tf.nn.elu(batchNorm1)\n",
    "    \n",
    "    layer2 = tf.layers.dense(batchNorm1Activation, hidden2, name = \"hLayerTwo\")\n",
    "    batchNorm2 = batchNormLayer(layer2)\n",
    "    batchNorm2Activation = tf.nn.elu(batchNorm2)\n",
    "    \n",
    "    \n",
    "    yHBeforeBatchNorm = tf.layers.dense(batchNorm2Activation, outputs, name = \"yH\")\n",
    "    yH = batchNormLayer(yHBeforeBatchNorm)   \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:19:53.042264Z",
     "start_time": "2018-09-09T05:17:54.975503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  1.0 Test Acc:  0.925\n",
      "10 Train Acc:  1.0 Test Acc:  0.9744\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9779\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "\n",
    "# New operations added from the batch normalization\n",
    "updateOps = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            # Note that training = TRUE\n",
    "            sess.run([opt, updateOps], feed_dict = {training: True, x: xBatch, y: yBatch})\n",
    "        # Note that training = FALSE by default\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:19:53.078283Z",
     "start_time": "2018-09-09T05:19:53.045263Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "entropy = batchNorm1 = batchNorm1Activation = batchNorm2 = batchNorm2Activation = None\n",
    "yHBeforeBatchNorm = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:19:53.430303Z",
     "start_time": "2018-09-09T05:19:53.081283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    # Gradient clipping threshold\n",
    "    threshold = 1.0\n",
    "    \n",
    "    # Setup optimizer to use gradient clipping\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr)\n",
    "    gradVars = opt.compute_gradients(loss)\n",
    "    cappedGradVars = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "                     for grad, var in gradVars]\n",
    "    clippedOpt = opt.apply_gradients(cappedGradVars)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Save the trained model parameters to disk\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:21:29.025776Z",
     "start_time": "2018-09-09T05:19:53.433303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.88 Test Acc:  0.9012\n",
      "10 Train Acc:  0.96 Test Acc:  0.9592\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9708\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([clippedOpt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "    \n",
    "    # Save the trained model\n",
    "    savePath = saver.save(sess, \"./DNN-TensorFlow-Gradient-Clipping.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build sample model to learn from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:21:29.051777Z",
     "start_time": "2018-09-09T05:21:29.029776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:08.776490Z",
     "start_time": "2018-09-09T05:21:29.055762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.94 Test Acc:  0.8969\n",
      "10 Train Acc:  1.0 Test Acc:  0.9715\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9738\n"
     ]
    }
   ],
   "source": [
    "# Create and save base model we want to use later on for transfer learning\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    layer3 = tf.layers.dense(layer2, hidden3, name = \"hLayerThree\", activation = tf.nn.relu)\n",
    "    layer4 = tf.layers.dense(layer3, hidden4, name = \"hLayerFour\", activation = tf.nn.relu)\n",
    "    layer5 = tf.layers.dense(layer4, hidden5, name = \"hLayerFive\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer5, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Save our trained model parameters to disk\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Method One:\n",
    "#   Collect all the model operations in one spot to support model transfer learning\n",
    "for op in (x, y, opt, accuracy):\n",
    "    tf.add_to_collection(\"modelOps\", op)\n",
    "\n",
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "    \n",
    "    # Save the CG, so we can restore and use it later on in another model\n",
    "    savePath = saver.save(sess, \"./DNN-Base-Pretrained-Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:08.794490Z",
     "start_time": "2018-09-09T05:23:08.786472Z"
    }
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning - Method one\n",
    "\n",
    "Attach to the TF CG components by utilizing the collection created by the orig. model author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:08.873495Z",
     "start_time": "2018-09-09T05:23:08.801474Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:09.023503Z",
     "start_time": "2018-09-09T05:23:08.875496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear away all our previous work\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:09.248517Z",
     "start_time": "2018-09-09T05:23:09.026504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN-Base-Pretrained-Model\n"
     ]
    }
   ],
   "source": [
    "sess2 = tf.Session() \n",
    "\n",
    "#First let's load meta graph and restore weights\n",
    "saver2 = tf.train.import_meta_graph('./DNN-Base-Pretrained-Model.meta')\n",
    "saver2.restore(sess2,tf.train.latest_checkpoint('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:09.302519Z",
     "start_time": "2018-09-09T05:23:09.251516Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attach to the TF CG components by utilizing the collection created by the orig. model author\n",
    "# and pull out the first four\n",
    "tf.get_collection(\"modelOps\")\n",
    "x2, y2, opt2, accuracy2 = tf.get_collection(\"modelOps\")[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:09.665522Z",
     "start_time": "2018-09-09T05:23:09.308503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9738\n"
     ]
    }
   ],
   "source": [
    "accTest2 = accuracy2.eval(session = sess2, feed_dict = {x2: mninst.test.images, y2: mninst.test.labels})\n",
    "print(accTest2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:09.685541Z",
     "start_time": "2018-09-09T05:23:09.668541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'x:0' shape=(?, 784) dtype=float32>,\n",
       " <tf.Tensor 'y:0' shape=(?,) dtype=int64>,\n",
       " <tf.Operation 'optimizer/GradientDescent' type=NoOp>,\n",
       " <tf.Tensor 'eval/accuracy:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the items in the collection\n",
    "tf.get_collection(\"modelOps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning - Method two\n",
    "\n",
    "Attach to the TF CG components by calling them specificly from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:09.817549Z",
     "start_time": "2018-09-09T05:23:09.687545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:09.868551Z",
     "start_time": "2018-09-09T05:23:09.820550Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear away all our previous work\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:10.333580Z",
     "start_time": "2018-09-09T05:23:09.871553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN-Base-Pretrained-Model\n"
     ]
    }
   ],
   "source": [
    "# Create a new session\n",
    "sess=tf.Session()    \n",
    "\n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('./DNN-Base-Pretrained-Model.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:10.582593Z",
     "start_time": "2018-09-09T05:23:10.336579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9738\n"
     ]
    }
   ],
   "source": [
    "# Attach to the TF CG components by calling them specificly from the graph\n",
    "graph = tf.get_default_graph()\n",
    "x = graph.get_tensor_by_name(\"x:0\")\n",
    "y = graph.get_tensor_by_name(\"y:0\")\n",
    "\n",
    "feed_dict = {x: mninst.test.images, y: mninst.test.labels}\n",
    "accuracy = graph.get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "accTest = accuracy.eval(session = sess, feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "print(accTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:10.590576Z",
     "start_time": "2018-09-09T05:23:10.585594Z"
    }
   },
   "source": [
    "## Freezing the lower layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:10.650597Z",
     "start_time": "2018-09-09T05:23:10.593594Z"
    }
   },
   "source": [
    "## Caching frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:10.701582Z",
     "start_time": "2018-09-09T05:23:10.654598Z"
    }
   },
   "source": [
    "## Altering layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
