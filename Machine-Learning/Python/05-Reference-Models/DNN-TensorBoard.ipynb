{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#TensorBoard---First-Run\" data-toc-modified-id=\"TensorBoard---First-Run-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TensorBoard - First Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#He-initializer-model\" data-toc-modified-id=\"He-initializer-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>He initializer model</a></span></li><li><span><a href=\"#ELU-model\" data-toc-modified-id=\"ELU-model-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>ELU model</a></span></li><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Second-Run\" data-toc-modified-id=\"TensorBoard---Second-Run-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>TensorBoard - Second Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Third-Run\" data-toc-modified-id=\"TensorBoard---Third-Run-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>TensorBoard - Third Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Fourth-Run\" data-toc-modified-id=\"TensorBoard---Fourth-Run-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>TensorBoard - Fourth Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Fifth-Run\" data-toc-modified-id=\"TensorBoard---Fifth-Run-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>TensorBoard - Fifth Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "<img style=\"float: left; margin-right: 15px;\" height = 40%; width = 40%; src=\"./images/TensorFlow_banner.png\" />\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "This is a strictly technical notebook; little to no narrative has been added other than code comments.\n",
    "\n",
    "The goal of this write-up is to capture model training metrics via TensorBoard in various configurations.  Configs are as follows:\n",
    "\n",
    "* TensorBoard - First Run\n",
    "  * Initial run of the TensorBoard (TB). Execute two models, and capture their loss and accuracy in two seperate plots.\n",
    "* TensorBoard - Second Run\n",
    "  * Execute a single model, and capture loss and accuracy on the same plot.\n",
    "* TensorBoard - Third Run\n",
    "  * Execute a single model; capture loss and accuracy on the same plot; and capture metadata such as memory consumption.\n",
    "* TensorBoard - Fourth Run\n",
    "  *  Execute a single model multiple times with various hyperparameters; capture loss and accuracy on the same plot; and capture metadata such as memory consumption.\n",
    "* TensorBoard - Fifth Run\n",
    "  * Execute a single model; capture loss and accuracy on the same plot; capture metadata such as memory consumption; and capture the each layer's bias, weights, and activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Import libs, define global vars, and import data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T16:14:27.002150Z",
     "start_time": "2018-09-13T16:14:26.999150Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T16:14:27.768150Z",
     "start_time": "2018-09-13T16:14:27.262150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Setup vars for the MINST data set\n",
    "inputs = 28 * 28    # image dim in pixels\n",
    "hidden1 = 300\n",
    "hidden2 = 100\n",
    "hidden3 = 50\n",
    "hidden4 = 50\n",
    "hidden5 = 50\n",
    "outputs = 10\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batchSize = 50\n",
    "\n",
    "mninst = input_data.read_data_sets(\"./datasets/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T16:14:26.712150Z",
     "start_time": "2018-09-13T16:14:26.708150Z"
    }
   },
   "outputs": [],
   "source": [
    "def resetGraph(seed= 10):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T16:14:26.841150Z",
     "start_time": "2018-09-13T16:14:26.714150Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanLogs():\n",
    "    os.system('rm -rf ./logs/dnnTensorBoard/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard - First Run\n",
    "\n",
    "Initial run of the TensorBoard (TB).  Execute two models, and capture their loss and accuracy in two seperate plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-07T20:47:13.613722Z",
     "start_time": "2018-09-07T20:47:13.609737Z"
    }
   },
   "source": [
    "## He initializer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:12:57.873940Z",
     "start_time": "2018-09-09T05:12:57.662931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir\n",
    "logDir = './logs/dnnTensorBoard/heInit/train'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Test Acc:  0.9039\n",
      "10 Train Acc:  0.98 Test Acc:  0.9598\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9707\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDir, sess.graph)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                merge = tf.summary.merge_all()\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:54.082231Z",
     "start_time": "2018-09-09T05:14:53.792180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Set the TB logdir\n",
    "logDir = './logs/dnnTensorBoard/elu/train'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # Use ELU\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.elu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.elu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.114258Z",
     "start_time": "2018-09-09T05:14:56.169295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.94 Test Acc:  0.9004\n",
      "10 Train Acc:  1.0 Test Acc:  0.9471\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9612\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDir, sess.graph)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                merge = tf.summary.merge_all()\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB First Run](./images/tb-1st-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard - Second Run\n",
    "\n",
    "Execute a single model, and capture loss and accuracy on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.96 Test Acc:  0.9021\n",
      "10 Train Acc:  0.96 Test Acc:  0.9593\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.97\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/secondRun/train'\n",
    "logDirTest = './logs/dnnTensorBoard/secondRun/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB First Run](./images/tb-2nd-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard - Third Run\n",
    "\n",
    "Execute a single model; capture loss and accuracy on the same plot; and capture metadata such as memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Test Acc:  0.903\n",
      "10 Train Acc:  1.0 Test Acc:  0.9594\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9701\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/thirdRun/train'\n",
    "logDirTest = './logs/dnnTensorBoard/thirdRun/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Generate and capture metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run(\n",
    "                    [merge, opt], \n",
    "                    feed_dict = {x: xBatch, y: yBatch},\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata\n",
    "                )\n",
    "                \n",
    "                # Write metadata\n",
    "                trainWriter.add_run_metadata(run_metadata, 'step%03d' % counter)\n",
    "                \n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "            else:\n",
    "                # Train as normal\n",
    "                summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB Third Run](./images/tb-3rd-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard - Fourth Run\n",
    "\n",
    "Execute a single model multiple times with various hyperparameters; capture loss and accuracy on the same plot; and capture metadata such as memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.88 Test Acc:  0.905\n",
      "10 Train Acc:  0.96 Test Acc:  0.9598\n",
      " \n",
      "FINAL ::  Train Acc:  0.94 Test Acc:  0.9709\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batchSize = 50\n",
    "\n",
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/fourthRun/1/train'\n",
    "logDirTest = './logs/dnnTensorBoard/fourthRun/1/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Generate and capture metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run(\n",
    "                    [merge, opt], \n",
    "                    feed_dict = {x: xBatch, y: yBatch},\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata\n",
    "                )\n",
    "                \n",
    "                # Write metadata\n",
    "                trainWriter.add_run_metadata(run_metadata, 'step%03d' % counter)\n",
    "                \n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "            else:\n",
    "                # Train as normal\n",
    "                summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.53 Test Acc:  0.552\n",
      "10 Train Acc:  0.91 Test Acc:  0.8856\n",
      "20 Train Acc:  0.94 Test Acc:  0.9062\n",
      " \n",
      "FINAL ::  Train Acc:  0.91 Test Acc:  0.9156\n"
     ]
    }
   ],
   "source": [
    "# Alter hyperparams for second run\n",
    "lr = 0.001\n",
    "epochs = 30\n",
    "batchSize = 100\n",
    "\n",
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/fourthRun/2/train'\n",
    "logDirTest = './logs/dnnTensorBoard/fourthRun/2/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Generate and capture metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run(\n",
    "                    [merge, opt], \n",
    "                    feed_dict = {x: xBatch, y: yBatch},\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata\n",
    "                )\n",
    "                \n",
    "                # Write metadata\n",
    "                trainWriter.add_run_metadata(run_metadata, 'step%03d' % counter)\n",
    "                \n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "            else:\n",
    "                # Train as normal\n",
    "                summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB Fourth Run](./images/tb-4th-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard - Fifth Run\n",
    "\n",
    "Execute a single model; capture loss and accuracy on the same plot; capture metadata such as memory consumption; and capture the each layer's bias, weights, and activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T16:15:15.073150Z",
     "start_time": "2018-09-13T16:14:32.373150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name hLayerOne/kernel:0 is illegal; using hLayerOne/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name hLayerOne/bias:0 is illegal; using hLayerOne/bias_0 instead.\n",
      "INFO:tensorflow:Summary name hLayerTwo/kernel:0 is illegal; using hLayerTwo/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name hLayerTwo/bias:0 is illegal; using hLayerTwo/bias_0 instead.\n",
      "INFO:tensorflow:Summary name yH/kernel:0 is illegal; using yH/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name yH/bias:0 is illegal; using yH/bias_0 instead.\n",
      "0 Train Acc:  0.96 Test Acc:  0.9056\n",
      "10 Train Acc:  0.98 Test Acc:  0.9624\n",
      " \n",
      "FINAL ::  Train Acc:  0.96 Test Acc:  0.9705\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/fifthRun/train'\n",
    "logDirTest = './logs/dnnTensorBoard/fifthRun/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    # Create the first hidden layer\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    \n",
    "    # Record the weights of bias of the layer and feed to the summary\n",
    "    # This is the verbose method....\n",
    "    with tf.variable_scope(\"hLayerOne\", reuse=True):\n",
    "        weights = tf.get_variable(\"kernel\")\n",
    "        bias = tf.get_variable(\"bias\")\n",
    "        tf.summary.histogram(\"layer1Weights\", weights)\n",
    "        tf.summary.histogram(\"layer1Bias\", bias)\n",
    "    \n",
    "    \n",
    "    # Create the second hidden layer\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    \n",
    "    # Record the weights of bias of the layer and feed to the summary\n",
    "    # And this time we'll do it the \"easy\" way\n",
    "    tf.contrib.layers.summarize_activation(layer2)\n",
    "    \n",
    "    \n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")  \n",
    "    # And the easy way again!  ;)\n",
    "    tf.contrib.layers.summarize_activation(yH)\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    # Capture predictions and activation\n",
    "    tf.summary.histogram(\"outputActivations\", entropy)\n",
    "    \n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.name, var)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Generate and capture metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run(\n",
    "                    [merge, opt], \n",
    "                    feed_dict = {x: xBatch, y: yBatch},\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata\n",
    "                )\n",
    "                \n",
    "                # Write metadata\n",
    "                trainWriter.add_run_metadata(run_metadata, 'step%03d' % counter)\n",
    "                \n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "            else:\n",
    "                # Train as normal\n",
    "                summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB First Run](./images/tb-5th-run.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
