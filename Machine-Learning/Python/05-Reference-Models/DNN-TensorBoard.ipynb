{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Vanishing/Exploding-Gradients\" data-toc-modified-id=\"Vanishing/Exploding-Gradients-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Vanishing/Exploding Gradients</a></span><ul class=\"toc-item\"><li><span><a href=\"#Xavier-initializer-(aka-Glorot-uniform-initializer)\" data-toc-modified-id=\"Xavier-initializer-(aka-Glorot-uniform-initializer)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Xavier initializer (aka Glorot uniform initializer)</a></span></li><li><span><a href=\"#He-initializer\" data-toc-modified-id=\"He-initializer-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>He initializer</a></span></li><li><span><a href=\"#Nonsaturating-Activation-Functions\" data-toc-modified-id=\"Nonsaturating-Activation-Functions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Nonsaturating Activation Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exponential-linear-unit-(ELU)\" data-toc-modified-id=\"Exponential-linear-unit-(ELU)-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Exponential linear unit (ELU)</a></span></li><li><span><a href=\"#Leaky-ReLu\" data-toc-modified-id=\"Leaky-ReLu-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Leaky ReLu</a></span></li></ul></li><li><span><a href=\"#Batch-normalization\" data-toc-modified-id=\"Batch-normalization-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Batch normalization</a></span></li><li><span><a href=\"#Gradient-Clipping\" data-toc-modified-id=\"Gradient-Clipping-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Gradient Clipping</a></span></li></ul></li><li><span><a href=\"#Transfer-Learning\" data-toc-modified-id=\"Transfer-Learning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Transfer Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-sample-model-to-learn-from\" data-toc-modified-id=\"Build-sample-model-to-learn-from-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Build sample model to learn from</a></span></li><li><span><a href=\"#Transfer-learning---Method-one\" data-toc-modified-id=\"Transfer-learning---Method-one-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Transfer learning - Method one</a></span></li><li><span><a href=\"#Transfer-learning---Method-two\" data-toc-modified-id=\"Transfer-learning---Method-two-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Transfer learning - Method two</a></span></li><li><span><a href=\"#Freezing-the-lower-layers\" data-toc-modified-id=\"Freezing-the-lower-layers-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Freezing the lower layers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Method-one---tf.get_collection\" data-toc-modified-id=\"Method-one---tf.get_collection-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Method one - tf.get_collection</a></span></li><li><span><a href=\"#Method-two---tf.stop_gradient\" data-toc-modified-id=\"Method-two---tf.stop_gradient-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Method two - tf.stop_gradient</a></span></li></ul></li><li><span><a href=\"#Caching-frozen-layers\" data-toc-modified-id=\"Caching-frozen-layers-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Caching frozen layers</a></span></li><li><span><a href=\"#Altering-layers\" data-toc-modified-id=\"Altering-layers-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Altering layers</a></span></li></ul></li><li><span><a href=\"#Optimizers\" data-toc-modified-id=\"Optimizers-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Optimizers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Momentum\" data-toc-modified-id=\"Momentum-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Momentum</a></span></li><li><span><a href=\"#Momentum-with-exponential-learning-rate-scheduling\" data-toc-modified-id=\"Momentum-with-exponential-learning-rate-scheduling-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Momentum with exponential learning rate scheduling</a></span></li><li><span><a href=\"#Nesterov-Accelerated-Gradient\" data-toc-modified-id=\"Nesterov-Accelerated-Gradient-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Nesterov Accelerated Gradient</a></span></li><li><span><a href=\"#RMSProp\" data-toc-modified-id=\"RMSProp-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>RMSProp</a></span></li><li><span><a href=\"#Adam\" data-toc-modified-id=\"Adam-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Adam</a></span></li></ul></li><li><span><a href=\"#Regularization\" data-toc-modified-id=\"Regularization-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Regularization</a></span><ul class=\"toc-item\"><li><span><a href=\"#$\\ell_1$-and-$\\ell_2$-regularization\" data-toc-modified-id=\"$\\ell_1$-and-$\\ell_2$-regularization-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>$\\ell_1$ and $\\ell_2$ regularization</a></span></li><li><span><a href=\"#Dropout\" data-toc-modified-id=\"Dropout-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Dropout</a></span></li><li><span><a href=\"#Max-Norm-Regularization\" data-toc-modified-id=\"Max-Norm-Regularization-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Max-Norm Regularization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:16:47.837591Z",
     "start_time": "2018-09-10T01:16:47.832590Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:16:49.170664Z",
     "start_time": "2018-09-10T01:16:48.004580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Setup vars for the MINST data set\n",
    "inputs = 28 * 28    # image dim in pixels\n",
    "hidden1 = 300\n",
    "hidden2 = 100\n",
    "hidden3 = 50\n",
    "hidden4 = 50\n",
    "hidden5 = 50\n",
    "outputs = 10\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batchSize = 50\n",
    "\n",
    "mninst = input_data.read_data_sets(\"./datasets/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetGraph(seed= 10):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLogs():\n",
    "    os.system('rm -rf ./logs/dnnTensorBoard/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard - First Run\n",
    "\n",
    "Initial run of the TensorBoard (TB).  Execute two models, and capture their loss and accuracy in two seperate plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-07T20:47:13.613722Z",
     "start_time": "2018-09-07T20:47:13.609737Z"
    }
   },
   "source": [
    "## He initializer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:12:57.873940Z",
     "start_time": "2018-09-09T05:12:57.662931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir\n",
    "logDir = './logs/dnnTensorBoard/heInit/train'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Test Acc:  0.9039\n",
      "10 Train Acc:  0.98 Test Acc:  0.9598\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9707\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDir, sess.graph)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                merge = tf.summary.merge_all()\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:54.082231Z",
     "start_time": "2018-09-09T05:14:53.792180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Set the TB logdir\n",
    "logDir = './logs/dnnTensorBoard/elu/train'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # Use ELU\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.elu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.elu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.114258Z",
     "start_time": "2018-09-09T05:14:56.169295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.94 Test Acc:  0.9004\n",
      "10 Train Acc:  1.0 Test Acc:  0.9471\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9612\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDir, sess.graph)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                merge = tf.summary.merge_all()\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB First Run](./images/tb-1st-run.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard - Second Run\n",
    "\n",
    "Execute a single model, and capture loss and accuracy on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.96 Test Acc:  0.9021\n",
      "10 Train Acc:  0.96 Test Acc:  0.9593\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.97\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/secondRun/train'\n",
    "logDirTest = './logs/dnnTensorBoard/secondRun/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB First Run](./images/tb-2nd-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard - Third Run\n",
    "\n",
    "Execute a single model; capture loss and accuracy on the same plot; and capture metadata such as memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Test Acc:  0.903\n",
      "10 Train Acc:  1.0 Test Acc:  0.9594\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9701\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/thirdRun/train'\n",
    "logDirTest = './logs/dnnTensorBoard/thirdRun/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Generate and capture metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run(\n",
    "                    [merge, opt], \n",
    "                    feed_dict = {x: xBatch, y: yBatch},\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata\n",
    "                )\n",
    "                \n",
    "                # Write metadata\n",
    "                trainWriter.add_run_metadata(run_metadata, 'step%03d' % counter)\n",
    "                \n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "            else:\n",
    "                # Train as normal\n",
    "                summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB Third Run](./images/tb-3rd-run.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard - Fourth Run\n",
    "\n",
    "Execute a single model multiple times with various hyperparameters; capture loss and accuracy on the same plot; and capture metadata such as memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.88 Test Acc:  0.905\n",
      "10 Train Acc:  0.96 Test Acc:  0.9598\n",
      " \n",
      "FINAL ::  Train Acc:  0.94 Test Acc:  0.9709\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batchSize = 50\n",
    "\n",
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/fourthRun/1/train'\n",
    "logDirTest = './logs/dnnTensorBoard/fourthRun/1/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Generate and capture metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run(\n",
    "                    [merge, opt], \n",
    "                    feed_dict = {x: xBatch, y: yBatch},\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata\n",
    "                )\n",
    "                \n",
    "                # Write metadata\n",
    "                trainWriter.add_run_metadata(run_metadata, 'step%03d' % counter)\n",
    "                \n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "            else:\n",
    "                # Train as normal\n",
    "                summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.53 Test Acc:  0.552\n",
      "10 Train Acc:  0.91 Test Acc:  0.8856\n",
      "20 Train Acc:  0.94 Test Acc:  0.9062\n",
      " \n",
      "FINAL ::  Train Acc:  0.91 Test Acc:  0.9156\n"
     ]
    }
   ],
   "source": [
    "# Alter hyperparams for second run\n",
    "lr = 0.001\n",
    "epochs = 30\n",
    "batchSize = 100\n",
    "\n",
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/dnnTensorBoard/fourthRun/2/train'\n",
    "logDirTest = './logs/dnnTensorBoard/fourthRun/2/test'\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"NN\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Oddly enough if we want to write to values into the same plot we need TWO summary writers\n",
    "    # One for training and one for test\n",
    "    # Also notice we don't pass the 'sess.graph' arg to the 2nd summary writer instance\n",
    "    # Also notice we don't call 'merge = tf.summary.merge_all()' other than this initial time\n",
    "    # We add the summary values manually via calls to 'train_writer.add_summary(summaryObj, counterValue)'\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):      \n",
    "            \n",
    "            counter += 1       \n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 500 == 0:\n",
    "                # Generate and capture metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run(\n",
    "                    [merge, opt], \n",
    "                    feed_dict = {x: xBatch, y: yBatch},\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata\n",
    "                )\n",
    "                \n",
    "                # Write metadata\n",
    "                trainWriter.add_run_metadata(run_metadata, 'step%03d' % counter)\n",
    "                \n",
    "                # Manually add the test accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "                # Manually add the train accuracy summary value\n",
    "                summary, acc = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter)\n",
    "            else:\n",
    "                # Train as normal\n",
    "                summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "                \n",
    "        \n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "![TB Fourth Run](./images/tb-4th-run.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
