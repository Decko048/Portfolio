---
title: "Linear Regression - Advertising Data"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
  word_document: default
---

# Set defaults for notebook
```{r}
setHook("plot.new", function() par(col = "blue", pch = '+'))
```

---
# Load the data
```{r}
library(ISLR)
```

```{r}
attach(Auto)
names(Auto)
```

```{r}
dim(Auto)
```

---
# Examine the data
```{r}
head(Auto, 5)
```

```{r}
summary(Auto)
```

```{r}
plot(Auto)
```


---
# 8

## a
Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results.  Comment on the output.

```{r}
fit = lm(mpg ~ horsepower, data = Auto)
summary(fit)
```


* Is there a relationship between the predictor and the response?

Yes.  The $\mid{t}\mid$-statistic value is large (24.5) while the p-value is small (2.2e-16 < 0.05).  This implies there is a relationship between the dependent and independent variable.  Or to put it another way, we can reject the null hypothesis that $H_0$ : $\beta_1 = 0$ and declare a relationship to exist between miles per gallon and horsepower.

* How strong is the relationship between the predictor and the response?

A small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response. Since the p-value for horsepower is very close to zero there is almost no chance the probability of seeing such a value if $H_0$ were true.  This leads us to conclude the relationship is very strong.

We can also consider the F-statistic.  When there is no relationship between the response and predictors, we expect the F-statistic to take on a value close to 1.  However if $H_a$ is true we expect F to be greater than 1.  Above we can see that the F-statistic of 599.7 much greater than one.  This provides compelling evidence against the null hypothesis $H_0$.  Next we examine the `p-value` for the F-statistic to determine if the F-statistic is large enough to reject the null hypothesis, $H_0$.  The F-statistic's p-value is 2.2e-16, so we have extremely strong evidence that horsepower is associated with mpg.

*  Is the relationship between the predictor and the response positive or negative?

Since the coefficient for horsepower is -0.16 we conclude the relationship is negative; for every unit increase in horsepower we expect mpg to decrease by -0.16.

* Other observations

The residual standard error (RSE) provides an absolute measure of lack of fit of the model to the data.  The smaller the RSE value is then the better the model fits the data.  The RSE value of 4.906 implies that actual mpg values deviate from the true regression line by approximately 4.906 units on average.  This probably isn't the greatest value, and would likely concern someone buying a new car if they were gas consumption conscious.  Saying the car achieves 30 mpg but could be off by 5 mpg probably wouldn't sit well with most consumers.

The higher RSE is likely explained somewhat by the adjusted R-squared value of 0.6049.  This implies the model explains 60% of the variability in the response, and that inclusion of other independent variables would likely help the model's accuracy.  For example, a visual inspection of the `plot(Auto)` outputs above indicates that weight would also have an impact on mpg, and better help the regression explain more of the variability in the response.

* What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?

If we consider the formula $Y = \beta_0 + \beta_1 * mpg$ then we'd expect a value of `39.94 - 0.16*98 = 24.26 mpg`.  

We can check our work in R by calculating the 95% confidence and prediction intervals:

```{r}
newdata = data.frame(horsepower=98)
predict(fit, newdata, interval="confidence")
```

```{r}
newdata = data.frame(horsepower=98)
predict(fit, newdata, interval="prediction")
```

We use a `confidence interval` to quantify the uncertainty surrounding the average mpg over a large number of automobiles. For example in general automobiles with 98 horsepower have a 95% confidence interval of [23.97308, 24.96108].  We interpret this to mean that 95% of intervals of this form will contain the true value of f(X) (i.e. mpg).  

On the other hand, a `prediction interval` can be used to quantify the uncertainty surrounding mpg for a `particular` automobile. Given a specific car with 98 horsepower the 95% prediction interval is [14.8094, 34.12476].

## b

Plot the response and the predictor. Use the abline() function to display the least squares regression line.

```{r}
{
  # Actual values
  plot(Auto$horsepower, Auto$mpg, pch = 19, col = 'red')
  # Least squares regression line
  abline(fit, lwd = 2)
}
```

## c

Use the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.

```{r}
plot(fit)
```

### Non-linearity of the Data

*Plot graph:*  `Residuals vs Fitted` 

Ideally, the `Residuals vs Fitted` plot won't have any discernible patterns.  If there is a pattern--such as a 'U' shape for example--its presence may indicate the data is non-linear.  In the example above there is indeed a 'U' shape present, so we can conclude the dat is non-linear.

### Normally Distributed Data

*Plot graph:*  `Normal Q-Q`

The points seem to mostly line up along the straight dashed reference line in the plot.  There doesn't appear to be a concave shape or "heavy tails" in the plot.  This indicates the data is mostly normally distributed.

###  Non-constant Variance of Error Terms

*Plot graph:*  `Scale-Location`

Non-constant variances in the errors, or heteroscedasticity, can be identified by the presence of a funnel shape in the `Scale-Location` plot.  Ideally we'd like to see a horizontal fitted line with equally (randomly) spread points.  In the example above the points on the graph don't 'funnel' as the fitted values increase along the X-axis, so we can assume we don't have a heteroscedasticity issue.

### Outliers

*Plot graph:*  `Residuals vs Leverage`

This plot helps to find influential observations that might skew the results of the linear regression.  Even though the data might have extreme values, these values might not be influential when determining the regression line (i.e. the results wouldn't be much different if we either include or exclude them from analysis). They follow the trend in the majority of cases and they don't really matter; they are not influential. On the other hand, some cases could be very influential even if they look to be within a reasonable range of the values. They could be extreme cases against a regression line and can alter the results if we exclude them from analysis.

Points to watch for are those at the upper right corner or at the lower right corner of the `Residuals vs Leverage` plot. We look for cases outside of a dashed line, Cook's distance. When cases are outside of the Cook's distance (meaning they have high Cook's distance scores), the cases are influential to the regression results. The regression results will be altered if we exclude those cases.

In the `Residuals vs Leverage` graph above we don't observe any data points outside of the Cook's distance dashed line.

--
# 9

This question involves the use of multiple linear regression on the Auto data set.

## a

Produce a scatter plot matrix which includes all of the variables in the data set.

See the 'Examine the data' section above.

## b

Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.






