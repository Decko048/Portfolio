---
title: "Linear Model Selection and Regularization - Full"
output:
  html_document:
    df_print: paged
---

# 8

In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.

## a

Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector e of length n = 100.

```{r}
set.seed(1)
x = rnorm(n = 100)
e = rnorm(n = 100, mean = 0, sd = 5)
```


## b

Generate a response vector Y of length n = 100 according to the model $Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon$, where B0 through B3 are constants of your choice.

```{r}
y = 2 + 4*x + 6*(x^2) + 8*(x^3) + e
plot(x, y)
```


## c

Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X, X2,...,X10. What is the best model obtained according to Cp, BIC, and adjusted R-squared? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained.

```{r}
library(leaps)

data = data.frame(y = y, x = x)
head(data)
```

```{r}
fit = regsubsets(y ~ poly(x, degree = 10, raw = T), data = data, nvmax = 10)
sfit = summary(fit)

par(mfrow = c(1,3))
{
  plot(sfit$adjr2, type = 'b', ylab = "Adjusted R-squared Score", xlab = "Subset Size", main = "R-squared")
  abline(v = which.max(sfit$adjr2), col = "blue")
  plot(sfit$cp, type = 'b', ylab = "Cp Score", xlab = "Subset Size", main = "Cp")
  abline(v = which.min(sfit$cp), col = "blue")
  plot(sfit$bic, type = 'b', ylab = "BIC Score", xlab = "Subset Size", main = "BIC")
  abline(v = which.min(sfit$bic), col = "blue")
}
```

```{r}
# Difference in BIC Scores when P = 4 and P = 3:
print(sfit$bic[4] - sfit$bic[3])
```

The adjusted R-squared and Cp statistics have the best scores when the number of predictors was four, and the BIC had the best score when the number of predictors was three.  

Does this matter?  An article on the subject:  https://www.methodology.psu.edu/resources/aic-vs-bic/.  Also note that the Cp (i.e. Mallows Cp) is a a variant of AIC developed by Colin Mallows, so we can assume the comments in the PSU article apply equally to the Cp statistic.

From the article:

> But despite various subtle theoretical differences, their [i.e. AIC/Cp and BIC] only difference in practice is the size of the penalty; BIC penalizes model complexity more heavily. The only way they should disagree is when AIC chooses a larger model than BIC.

And this is exactly what we see above where the Cp model optimizes at four parameters and the BIC model optimizes on three.  For all intents and purposes then we will say the best model using best subset selection is P = 3 (i.e. we'll favor the simpler model over the more complex one).  

The coefficients for P = 3 are:

```{r}
coefficients(fit, id = 3)
```

These are very close to the true beta values of (2,4,6,8).  The coefficients for P = 4 for comparison:

```{r}
coefficients(fit, id = 4)
```

So, although the number of predictors is similar between the models, _which_ predictors were selected by the models differs greatly.


## d

### Forward stepwise selection

```{r}
# Forward stepwise selection
fit = regsubsets(y ~ poly(x, degree = 10, raw = T), data = data, method = 'forward')
sfit = summary(fit)

par(mfrow = c(1,3))
{
  plot(sfit$adjr2, type = 'b', ylab = "Adjusted R-squared Score", xlab = "Subset Size", main = "R-squared")
  abline(v = which.max(sfit$adjr2), col = "blue")
  plot(sfit$cp, type = 'b', ylab = "Cp Score", xlab = "Subset Size", main = "Cp")
  abline(v = which.min(sfit$cp), col = "blue")
  plot(sfit$bic, type = 'b', ylab = "BIC Score", xlab = "Subset Size", main = "BIC")
  abline(v = which.min(sfit$bic), col = "blue")
}
```

### Backward stepwise selection

```{r}
# Forward stepwise selection
fit = regsubsets(y ~ poly(x, degree = 10, raw = T), data = data, method = 'backward')
sfit = summary(fit)

par(mfrow = c(1,3))
{
  plot(sfit$adjr2, type = 'b', ylab = "Adjusted R-squared Score", xlab = "Subset Size", main = "R-squared")
  abline(v = which.max(sfit$adjr2), col = "blue")
  plot(sfit$cp, type = 'b', ylab = "Cp Score", xlab = "Subset Size", main = "Cp")
  abline(v = which.min(sfit$cp), col = "blue")
  plot(sfit$bic, type = 'b', ylab = "BIC Score", xlab = "Subset Size", main = "BIC")
  abline(v = which.min(sfit$bic), col = "blue")
}
```

Again the Cp and BIC differ in model size, but as we discussed earlier it is to be expected and makes sense.  We'll settle on the best subset selection where P = 6 (again favoring the simpler model) and examine the coefficients:

```{r}
coefficients(fit, id = 6)
```

## e

Now fit a lasso model to the simulated data, again using X, X2^,...,X^10 as predictors. Use cross-validation to select the optimal value of lambda. Create plots of the cross-validation error as a function of lambda. Report the resulting coefficient estimates, and discuss the results obtained.

```{r}
library(glmnet)
```

The `glmnet` function supports both Ridge Regression and Lasso.  

* If `alpha = 1` is the lasso penalty is applied, and if `alpha = 0` the ridge penalty is applied.  
* By default `glmnet` computes its own lambda sequence based on nlambda and lambda.min.ratio; supplying a value of lambda overrides this. 
* Also by default `glmnet()` standardizes the variables so that they are on the same scale, which is critical for Ridge Regression
* The `glmnet` function also expects an `x` matrix as well as a `y` vector

```{r}
# Create the X matrix; exclude the intercept
xm = model.matrix(y ~ poly(x, degree = 10, raw = T), data = data)[, -1]
# Collect the Y values
y = y
```

And finally, we'll want a test and train set:

```{r}
set.seed(1)

# Half the data is training data
train = sample(1:nrow(xm), nrow(xm)/2)

# The other half is test data
test = (-train)

x.train = xm[train,]
x.test = xm[test,]
y.train = y[train]
y.test = y[test]
```

Fit the Lasso model (note alpha = 1) using cross-validation and plot the error rate:

```{r}
set.seed(1)
cv = cv.glmnet(x.train, y.train, alpha = 1)
plot(cv)
```

We can now made predictions on the test data and compute the error rate:

```{r}
l = cv$lambda.min
p = predict(cv, s = l, newx = x.test)
err = mean((p - y.test)^2)

{
  print(err)
  print(l)
}
```

So the error rate is 22.7 with an optimal lambda value of 0.6.  If we examine the coefficients we can see that they are sparse (i.e. a number of them have been set to zero) since The Lasso is a variable selection model:

```{r}
fit = glmnet(xm, y, alpha = 1)
predict(fit, type = "coefficients", s = l)
```

We can also note that the Lasso model does a fairly good job of picking coefficient values that are close to the true values of Beta.  For example we have true vs. model coefficients as intercept 2 => 3.09, first term 4 => 4.4, second term 6 => 3.93, and third term 8 => 7.12. 

## f

Now generate a response vector Y according to the model $Y = \beta_0 + \beta_7{X^7} + \epsilon$, and perform best subset selection and the lasso.  Discuss the results obtained.

```{r}
y2 = 2 + 1.5 * (x^7) + e
data2 = data.frame(y2 = y2, x = x)
```

__best subset selection__

```{r}
fit = regsubsets(y2 ~ poly(x, degree = 10, raw = T), data = data2, nvmax = 10)
sfit = summary(fit)

par(mfrow = c(1,3))
{
  plot(sfit$adjr2, type = 'b', ylab = "Adjusted R-squared Score", xlab = "Subset Size", main = "R-squared")
  abline(v = which.max(sfit$adjr2), col = "blue")
  plot(sfit$cp, type = 'b', ylab = "Cp Score", xlab = "Subset Size", main = "Cp")
  abline(v = which.min(sfit$cp), col = "blue")
  plot(sfit$bic, type = 'b', ylab = "BIC Score", xlab = "Subset Size", main = "BIC")
  abline(v = which.min(sfit$bic), col = "blue")
}
```

```{r}
coefficients(fit, id = 1)
```

```{r}
coefficients(fit, id = 2)
```

```{r}
coefficients(fit, id = 4)
```



__Lasso__

```{r}
xmat = model.matrix(y2 ~ poly(x, 10 , raw = T))[,-1]
cv = cv.glmnet(xmat, y2, alpha = 1)
l = cv$lambda.min

par(mfrow=c(1,1))

plot(cv)
```

```{r}
predict(cv, s=l, type="coefficients")
```

__discussion__

The best subset selection model picks predictor variable counts of 4, 2, and 1 for the R-squared, Cp, and BIC scores respectively.  If we examine the coefficient estimates we see the BIC model's beta values are almost equal to the true beta values we created the data set with.  

The Lasso's coefficient estimates are also almost equal to the true beta values we created the data set with.

Based on the experiments we've conducted above I would tend to favor either best subset selection using BIC or the Lasso to predict the best model and model parameters.


# 9

First, create a dataframe to hold model results:

```{r}
results =  data.frame(
  Model = character(),
  MSPE = integer(),
  stringsAsFactors=FALSE
)
```


In this exercise, we will predict the number of applications received using the other variables in the College data set.

```{r}
library(ISLR)
attach(College)
head(College)
```

```{r}
names(College)
```


```{r}
dim(College)
```

```{r}
summary(College)
```


## a

Split the data set into a training set and a test set.

```{r}
{
  library(dplyr)
  set.seed(1)
  
  # 80/20 split of the data into train/test
  index = sample(seq_len(nrow(College)), size = floor(0.8 * nrow(College)))
  
  train = College[index, ]
  test = College[-index, ]
  
  train.matrix = model.matrix(Apps ~ ., data = College[index,])[,-1]
  test.matrix = model.matrix(Apps ~ ., data = College[-index,])[,-1]
}

{
  print(dim(train))
  print(dim(test))
  
  print(dim(train.matrix))
  print(dim(test.matrix))
}
```

```{r}
?seq_len
```


## b

Fit a linear model using least squares on the training set, and report the test error obtained.

```{r}
{
  fit = lm(Apps ~ ., data = College, subset = index)
  p = predict(fit, newdata = College[-index, ])
  err = mean( (p - College[-index, ]$Apps)^2 )
  results[nrow(results)+1,] = list('Least Squares', err)
  results
}
```

## c

Fit a ridge regression model on the training set, with lamba chosen by cross-validation. Report the test error obtained.

```{r}
{
set.seed(1)
cv = cv.glmnet(train.matrix, train$Apps, alpha = 0)

l = cv$lambda.min
p = predict(cv, s = l, newx = test.matrix)
e = mean((p - test$Apps)^2)

results[nrow(results)+1,] = list('Ridge Regression', e)
results
}
```

## d

Fit a lasso model on the training set, with lambda chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.

```{r}
{
set.seed(1)
cv = cv.glmnet(train.matrix, train$Apps, alpha = 1)

l = cv$lambda.min
p = predict(cv, s = l, newx = test.matrix)
e = mean((p - test$Apps)^2)

results[nrow(results)+1,] = list('The Lasso', e)
results
}
```

```{r}
fit = glmnet(model.matrix(Apps ~ ., data = College)[, -1], College$Apps, alpha = 1)
round(predict(fit, s = l, type = "coefficients"))
```

## e

Fit a PCR model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.

```{r}
{
library(pls)
set.seed(1)

fit = pcr(Apps ~ ., data = College[index, ], scale = T, validation = 'CV')
summary(fit)
}
```

```{r}
validationplot(fit, val.type = "MSEP")
```



The lowest adjusted CV error rate is 1155 with M = 17 (i.e. no reduction of dimensionality provided better results than utilizing the full set of predictor variables -- at least with this sample set of training data).  We can now calculate the error rate on the test data which results in exactly the same error value as the linear regression model:


```{r}
{
pred = predict(fit, College[-index, ], ncomp = 17)
e = mean( (pred - College[-index, ]$Apps)^2)

results[nrow(results)+1,] = list('PRC', e)
results
}
```

## f

Fit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.

```{r}
{
set.seed(1)

fit = plsr(Apps ~ ., data = College[index, ], scale = T, validation = 'CV')
summary(fit)
}
```

```{r}
validationplot(fit, val.type = "MSEP")
```

Once M = 10 the adjusted cross validation score reaches its minimal value of 1156.  We can now calculate the error rate:

```{r}
{
pred = predict(fit, College[-index, ], ncomp = 10)
e = mean( (pred - College[-index, ]$Apps)^2)

results[nrow(results)+1,] = list('PLS', e)
results
}
```


## g

Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?


The test error rates for each model we considered from smallest to largest in tabular and graphical format:

```{r}
{
results$MSPE = sapply(as.integer(results[, 2]), round)
results[order(results$MSPE),]
}
```

```{r}
{
  plot(results[order(results$MSPE),]$MSPE, type = 'l', ylab = "MSPE", xlab = "Model", xaxt="n")
  axis(1, at = 1:5, labels = results[order(results$MSPE),]$Model)
}
```

There is little difference in the models except for Ridge Regression which had the largest error rate, and the PRC which failed to reduce the number of predictors (and likely should be thrown out of the pool of possible models to be considered for this data set).  Next, we can plot the actual application figures for the test data set against the predicted number of applications given by the linear model:

```{r}
{
  fit = lm(Apps ~ ., data = College[index, ])
  p = predict(fit, newdata = College[-index, ])
  

  plot(test$Apps, col = 'blue', ylab = 'College Applications')
  points(p, col = 'red')
  legend('topleft', legend=c("Actual", "Predicted"), col=c("blue", "red"), lty = c(0,0), pch=c(1,1))
}
```

For colleges where the number of applications was less than around 7,500 the model's predictions fit the actual values closely.  However, for colleges with applications > 7,500 the model became less accurate.  We can examine the diagnostic plots of the least squares model to look for insights into this behavior:

```{r}
plot(fit)
```

Observing the diagnostic plots for the least squares model we see three issues that would likely contribute to the model's lack of fit for all College application values:

* The `Normal Q-Q` plot indicates the College data comes from a non-normal distribution.  (The tails in the `Normal Q-Q` that curve off in the extremities usually mean that the data has more extreme values than would be expected if it truly came from a Normal distribution.) 
* The `Scale-Location` graph indicates the data homoscedasticity issues
* The `Residuals vs Leverage` graph identifies at least one outlier with high leverage and several other values approaching outlier status



# 10

We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not.  We will now explore this in a simulated data set.

## a

Generate a data set with p = 20 features, n = 1,000 observations, and an associated quantitative response vector generated according to the model `Y = xB + e`, where B has some elements that are exactly equal to zero.

```{r}
{
set.seed(1)

e = rnorm(1000)
x = matrix(rnorm(1000*20), nrow = 1000, ncol=20)
b = sample(-5:5, 20, replace=TRUE)
b[c(3,6,7,10,13,17)] = 0

y = x %*% b + e
hist(y, breaks = 100)
}
```

## b

Split your data set into a training set containing 100 observations and a test set containing 900 observations.

```{r}
{
  set.seed(1)
  
  # 90/10 split of the data into train/test
  index = sample(seq_len(nrow(y)), size = floor(0.9 * nrow(y)))
  
  data = as.data.frame(cbind(y, x))
  colnames(data)[1] = 'y'
  colnames(data)[2:21] = paste('B', seq(1:(ncol(data)-1)), sep = '')
  
  train = data[index, ]
  test = data[-index, ]
  
  head(train)
}
```

## c

Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.

* Mean squared error (MSE) measures the expected squared distance between an estimator and the true underlying parameter.
* The mean squared prediction error (MSPE) measures the expected squared distance between what your predictor predicts for a specific value and what the true value is.


```{r}
library(leaps)

fit = regsubsets(y ~ ., data = train, nvmax = 20)
# summary(fit)$outmat
```

```{r}
# Container for each best subset selection model's MSE error rate
errs = rep(20, 0)

# Convert the dataset to a matrix - This keeps the values of B and adds an intercept term equal to one
m = model.matrix(y ~ ., data = train)

# For each model
for (i in 1:20) {
  # Pull model's coefficents
  c = coef(fit, id = i)
  
  # Obtain values from the training data matrix lining up w/ the columns selected by the model
  v = m[, names(c)]
  
  # Predict the Y values
  p = v %*% c
  
  # Calculate the error of the Y value predictions vs. the actual training Y values
  errs[i] = mean( (train$y - p)^2 )
}

{
  print(which.min(errs))
  print(errs[which.min(errs)])
}

```

```{r}
{
  plot(errs, type = 'l', ylab = "Best Subset Selection Model MSE", xlab = "Number of Variables Included in the Model")
  points( which.min(errs), errs[which.min(errs)], pch = 19, col = 'red')
  text(which.min(errs)-1, errs[which.min(errs)]+6, labels = c("Best MSE"))
}
```

So, as per the main idea behind this exercise, we observe that as the number of features used in a model increases the training error will necessarily decrease.

## d

Plot the test set MSE associated with the best model of each size.

```{r}
# Container for each best subset selection model's MSE error rate
errs = rep(20, 0)

# Convert the dataset to a matrix - This keeps the values of B and adds an intercept term equal to one
m = model.matrix(y ~ ., data = test)

# For each model
for (i in 1:20) {
  # Pull model's coefficents
  c = coef(fit, id = i)
  
  # Obtain values from the test data matrix lining up w/ the columns selected by the model
  v = m[, names(c)]
  
  # Predict the Y values
  p = v %*% c
  
  # Calculate the error of the Y value predictions vs. the actual test Y values
  errs[i] = mean( (test$y - p)^2 )
}

{
  print(which.min(errs))
  print(errs[which.min(errs)])
}
```

```{r}
{
  plot(errs, type = 'l', ylab = "Best Subset Selection Model MSE", xlab = "Number of Variables Included in the Model")
  points( which.min(errs), errs[which.min(errs)], pch = 19, col = 'red')
  text(which.min(errs), errs[which.min(errs)]+6, labels = c("Best MSE"))
}
```

## e

For which model size does the test set MSE take on its minimum value? Comment on your results.

The lowest MSE achieved on the test data set was when P = 13 with an MSPE rate of 1.108762.


